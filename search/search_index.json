{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"A high level framework and library for running, training, and deploying state-of-the-art Natural Language Processing (NLP) models for end to end tasks. AdaptNLP allows users ranging from beginner python coders to experienced machine learning engineers to leverage state-of-the-art NLP models and training techniques in one easy-to-use python package. Built atop Zalando Research's Flair and Hugging Face's Transformers library, AdaptNLP provides Machine Learning Researchers and Scientists a modular and adaptive approach to a variety of NLP tasks with an Easy API for training, inference, and deploying NLP-based microservices. Key Features Full Guides and API Documentation Tutorial Jupyter/Google Colab Notebooks Unified API for NLP Tasks with SOTA Pretrained Models (Adaptable with Flair and Transformer's Models) Token Tagging Sequence Classification Embeddings Question Answering Summarization Translation Text Generation More in development Training and Fine-tuning Interface Integration with Transformer's Trainer Module for fast and easy transfer learning with custom datasets Jeremy's ULM-FIT approach for transfer learning in NLP Fine-tuning Transformer's language models and task-specific predictive heads like Flair's SequenceClassifier Rapid NLP Model Deployment with Sebasti\u00e1n's FastAPI Framework Containerized FastAPI app Immediately deploy any custom trained Flair or AdaptNLP model Dockerizing AdaptNLP with GPUs Easily build and run AdaptNLP containers leveraging NVIDIA GPUs with Docker Quick Start \u00b6 Requirements and Installation for Linux/Mac \u00b6 Note: AdaptNLP will be using the latest stable torch version (v1.7 as of 11/2/20) which requires Python 3.7+. Please downgrade torch<=1.6 if using Python 3.6 Virtual Environment \u00b6 To avoid dependency clustering and issues, it would be wise to install AdaptNLP in a virtual environment. To create a new python 3.7+ virtual environment, run this command and then activate it however your operating system specifies: python -m venv venv-adaptnlp AdaptNLP Install \u00b6 Install using pip in your virtual environment: pip install adaptnlp If you want to work on AdaptNLP, pip install adaptnlp[dev] will install its development tools. Requirements and Installation for Windows \u00b6 PyTorch Install \u00b6 PyTorch needs to manually installed on Windows environments. If it's not already installed, proceed to http://pytorch.org/get-started/locally to select your preferences and then run the given install command. Note that the current version of PyTorch we use relies on cuda 10.1. AdaptNLP Install \u00b6 Install using pip: pip install adaptnlp If you want to work on AdaptNLP, pip install adaptnlp[dev] will install its development tools. Examples and General Use \u00b6 Once you have installed AdaptNLP, here are a few examples of what you can run with AdaptNLP modules: Named Entity Recognition with EasyTokenTagger \u00b6 from adaptnlp import EasyTokenTagger ## Example Text example_text = \"Novetta's headquarters is located in Mclean, Virginia.\" ## Load the token tagger module and tag text with the NER model tagger = EasyTokenTagger () sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner\" ) ## Output tagged token span results in Flair's Sentence object model for sentence in sentences : for entity in sentence . get_spans ( \"ner\" ): print ( entity ) Output Span [ 1 ]: \"Novetta\" [ \u2212 Labels : ORG ( 0.9925 )] Span [ 7 ]: \"Mclean\" [ \u2212 Labels : LOC ( 0.9993 )] Span [ 9 ]: \"Virginia\" [ \u2212 Labels : LOC ( 1.0 )] English Sentiment Classifier EasySequenceClassifier \u00b6 from adaptnlp import EasySequenceClassifier from pprint import pprint ## Example Text example_text = \"This didn't work at all\" ## Load the sequence classifier module and classify sequence of text with the multi-lingual sentiment model classifier = EasySequenceClassifier () sentences = classifier . tag_text ( text = example_text , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , mini_batch_size = 1 , ) ## Output labeled text results in Flair's Sentence object model print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output 2020 - 08 - 31 02 : 21 : 25 , 011 loading file nlptown / bert - base - multilingual - uncased - sentiment Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 / 1 [ 00 : 00 < 00 : 00 , 98.67 it / s ] Tag Score Outputs : { \"This didn't work at all\" : [ 1 star ( 0.8421 ), 2 stars ( 0.1379 ), 3 stars ( 0.018 ), 4 stars ( 0.0012 ), 5 stars ( 0.0007 )]} Span-based Question Answering EasyQuestionAnswering \u00b6 from adaptnlp import EasyQuestionAnswering from pprint import pprint ## Example Query and Context query = \"What is the meaning of life?\" context = \"Machine Learning is the meaning of life.\" top_n = 5 ## Load the QA module and run inference on results qa = EasyQuestionAnswering () best_answer , best_n_answers = qa . predict_qa ( query = query , context = context , n_best_size = top_n , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) ## Output top answer as well as top 5 answers print ( best_answer ) pprint ( best_n_answers ) Output [ OrderedDict ([( 'text' , 'Machine Learning' ), ( 'probability' , 0.9978840517692185 ), ( 'start_logit' , 8.442423820495605 ), ( 'end_logit' , 8.063008308410645 ), ( 'start_index' , 0 ), ( 'end_index' , 1 )]), OrderedDict ([( 'text' , 'Machine' ), ( 'probability' , 0.0010938238341776795 ), ( 'start_logit' , 8.442423820495605 ), ( 'end_logit' , 1.2470508813858032 ), ( 'start_index' , 0 ), ( 'end_index' , 0 )]), OrderedDict ([( 'text' , 'Machine Learning is the meaning of life.' ), ( 'probability' , 0.0007310516091540702 ), ( 'start_logit' , 8.442423820495605 ), ( 'end_logit' , 0.8440999984741211 ), ( 'start_index' , 0 ), ( 'end_index' , 6 )]), OrderedDict ([( 'text' , 'Machine Learning is the meaning' ), ( 'probability' , 0.00020125385987753433 ), ( 'start_logit' , 8.442423820495605 ), ( 'end_logit' , - 0.4458169639110565 ), ( 'start_index' , 0 ), ( 'end_index' , 4 )]), OrderedDict ([( 'text' , 'Learning' ), ( 'probability' , 8.981892757199805e-05 ), ( 'start_logit' , - 0.8731728196144104 ), ( 'end_logit' , 8.063008308410645 ), ( 'start_index' , 1 ), ( 'end_index' , 1 )])] Summarization EasySummarizer \u00b6 from adaptnlp import EasySummarizer # Text from encyclopedia Britannica on Einstein text = \"\"\"Einstein would write that two \u201cwonders\u201d deeply affected his early years. The first was his encounter with a compass at age five. He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his 'sacred little geometry book'. Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would never amount to anything.\"\"\" summarizer = EasySummarizer () # Summarize summaries = summarizer . summarize ( text = text , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , num_beams = 4 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Summaries: \\n \" ) for s in summaries : print ( s , \" \\n \" ) Output Einstein was mystified invisible forces could deflect the needle . the second wonder came at age 12 when he discovered a book of geometry . Einstein became deeply religious at age 12 . Translation EasyTranslator \u00b6 from adaptnlp import EasyTranslator text = [ \"Machine learning will take over the world very soon.\" , \"Machines can speak in many languages.\" ,] translator = EasyTranslator () # Translate translations = translator . translate ( text = text , t5_prefix = \"translate English to German\" , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Translations: \\n \" ) for t in translations : print ( t , \" \\n \" ) Output Das Maschinelle Lernen wird die Welt in K\u00fcrze \u00fcbernehmen. Maschinen k\u00f6nnen in vielen Sprachen sprechen. Tutorials \u00b6 Look in the Tutorials directory for a quick introduction to the library and its very simple and straight forward use cases: NLP Tasks Token Classification: NER, POS, Chunk, and Frame Tagging Sequence Classification: Sentiment Embeddings: Transformer Embeddings e.g. BERT, XLM, GPT2, XLNet, roBERTa, ALBERT Question Answering: Span-based Question Answering Model Summarization: Abstractive and Extractive Translation: Seq2Seq Custom Fine-Tuning and Training with Transformer Models Fine-tuning a Transformers Language Model Checkout the documentation for more information. REST Service \u00b6 We use FastAPI for standing up endpoints for serving state-of-the-art NLP models with AdaptNLP. The REST directory contains more detail on deploying a REST API locally or with docker in a very easy and fast way. Docker \u00b6 AdaptNLP official docker images are up on Docker Hub . Images have AdaptNLP installed from source in developer mode with tutorial notebooks available, and will default to launching a jupyter server from where you can start running the tutorial and workshop notebooks. Images can build with GPU support if NVIDA-Docker is correctly installed. Pull and Run AdaptNLP Immediately \u00b6 Simply run an image with AdaptNLP installed from source in developer mode by running: docker run -itp 8888:8888 achangnovetta/adaptnlp:latest Run an image with AdaptNLP running on GPUs if you have nvidia drivers and nvidia-docker 19.03+ installed: docker run -itp 8888:8888 --gpus all achangnovetta/adaptnlp:latest Check localhost:8888 or localhost:8888/lab to access the container notebook servers. Build \u00b6 Refer to the docker/ directory and run the following to build and run adaptnlp from the available images. Note: A container with GPUs enabled requires Docker version 19.03+ and nvida-docker installed # From the repo directory docker build -t achangnovetta/adaptnlp:latest -f docker/runtime/Dockerfile.cuda11.0-runtime-ubuntu18.04-py3.8 . docker run -itp 8888:8888 achangnovetta/adaptnlp:latest If you want to use CUDA compatible GPUs docker run -itp 8888:8888 --gpus all achangnovetta/adaptnlp:latest Check localhost:8888 or localhost:8888/lab to access the container notebook servers. Contact \u00b6 Please contact the author Andrew Chang at achang@novetta.com with questions or comments regarding AdaptNLP. Follow us on Twitter at @achang1618 and @AdaptNLP for updates and NLP dialogue. License \u00b6 This project is licensed under the terms of the Apache 2.0 license.","title":"AdaptNLP Overview"},{"location":"index.html#quick-start","text":"","title":"Quick Start"},{"location":"index.html#requirements-and-installation-for-linuxmac","text":"Note: AdaptNLP will be using the latest stable torch version (v1.7 as of 11/2/20) which requires Python 3.7+. Please downgrade torch<=1.6 if using Python 3.6","title":"Requirements and Installation for Linux/Mac"},{"location":"index.html#virtual-environment","text":"To avoid dependency clustering and issues, it would be wise to install AdaptNLP in a virtual environment. To create a new python 3.7+ virtual environment, run this command and then activate it however your operating system specifies: python -m venv venv-adaptnlp","title":"Virtual Environment"},{"location":"index.html#adaptnlp-install","text":"Install using pip in your virtual environment: pip install adaptnlp If you want to work on AdaptNLP, pip install adaptnlp[dev] will install its development tools.","title":"AdaptNLP Install"},{"location":"index.html#requirements-and-installation-for-windows","text":"","title":"Requirements and Installation for Windows"},{"location":"index.html#pytorch-install","text":"PyTorch needs to manually installed on Windows environments. If it's not already installed, proceed to http://pytorch.org/get-started/locally to select your preferences and then run the given install command. Note that the current version of PyTorch we use relies on cuda 10.1.","title":"PyTorch Install"},{"location":"index.html#adaptnlp-install_1","text":"Install using pip: pip install adaptnlp If you want to work on AdaptNLP, pip install adaptnlp[dev] will install its development tools.","title":"AdaptNLP Install"},{"location":"index.html#examples-and-general-use","text":"Once you have installed AdaptNLP, here are a few examples of what you can run with AdaptNLP modules:","title":"Examples and General Use"},{"location":"index.html#named-entity-recognition-with-easytokentagger","text":"from adaptnlp import EasyTokenTagger ## Example Text example_text = \"Novetta's headquarters is located in Mclean, Virginia.\" ## Load the token tagger module and tag text with the NER model tagger = EasyTokenTagger () sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner\" ) ## Output tagged token span results in Flair's Sentence object model for sentence in sentences : for entity in sentence . get_spans ( \"ner\" ): print ( entity ) Output Span [ 1 ]: \"Novetta\" [ \u2212 Labels : ORG ( 0.9925 )] Span [ 7 ]: \"Mclean\" [ \u2212 Labels : LOC ( 0.9993 )] Span [ 9 ]: \"Virginia\" [ \u2212 Labels : LOC ( 1.0 )]","title":"Named Entity Recognition with EasyTokenTagger"},{"location":"index.html#english-sentiment-classifier-easysequenceclassifier","text":"from adaptnlp import EasySequenceClassifier from pprint import pprint ## Example Text example_text = \"This didn't work at all\" ## Load the sequence classifier module and classify sequence of text with the multi-lingual sentiment model classifier = EasySequenceClassifier () sentences = classifier . tag_text ( text = example_text , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , mini_batch_size = 1 , ) ## Output labeled text results in Flair's Sentence object model print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output 2020 - 08 - 31 02 : 21 : 25 , 011 loading file nlptown / bert - base - multilingual - uncased - sentiment Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 / 1 [ 00 : 00 < 00 : 00 , 98.67 it / s ] Tag Score Outputs : { \"This didn't work at all\" : [ 1 star ( 0.8421 ), 2 stars ( 0.1379 ), 3 stars ( 0.018 ), 4 stars ( 0.0012 ), 5 stars ( 0.0007 )]}","title":"English Sentiment Classifier EasySequenceClassifier"},{"location":"index.html#span-based-question-answering-easyquestionanswering","text":"from adaptnlp import EasyQuestionAnswering from pprint import pprint ## Example Query and Context query = \"What is the meaning of life?\" context = \"Machine Learning is the meaning of life.\" top_n = 5 ## Load the QA module and run inference on results qa = EasyQuestionAnswering () best_answer , best_n_answers = qa . predict_qa ( query = query , context = context , n_best_size = top_n , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) ## Output top answer as well as top 5 answers print ( best_answer ) pprint ( best_n_answers ) Output [ OrderedDict ([( 'text' , 'Machine Learning' ), ( 'probability' , 0.9978840517692185 ), ( 'start_logit' , 8.442423820495605 ), ( 'end_logit' , 8.063008308410645 ), ( 'start_index' , 0 ), ( 'end_index' , 1 )]), OrderedDict ([( 'text' , 'Machine' ), ( 'probability' , 0.0010938238341776795 ), ( 'start_logit' , 8.442423820495605 ), ( 'end_logit' , 1.2470508813858032 ), ( 'start_index' , 0 ), ( 'end_index' , 0 )]), OrderedDict ([( 'text' , 'Machine Learning is the meaning of life.' ), ( 'probability' , 0.0007310516091540702 ), ( 'start_logit' , 8.442423820495605 ), ( 'end_logit' , 0.8440999984741211 ), ( 'start_index' , 0 ), ( 'end_index' , 6 )]), OrderedDict ([( 'text' , 'Machine Learning is the meaning' ), ( 'probability' , 0.00020125385987753433 ), ( 'start_logit' , 8.442423820495605 ), ( 'end_logit' , - 0.4458169639110565 ), ( 'start_index' , 0 ), ( 'end_index' , 4 )]), OrderedDict ([( 'text' , 'Learning' ), ( 'probability' , 8.981892757199805e-05 ), ( 'start_logit' , - 0.8731728196144104 ), ( 'end_logit' , 8.063008308410645 ), ( 'start_index' , 1 ), ( 'end_index' , 1 )])]","title":"Span-based Question Answering EasyQuestionAnswering"},{"location":"index.html#summarization-easysummarizer","text":"from adaptnlp import EasySummarizer # Text from encyclopedia Britannica on Einstein text = \"\"\"Einstein would write that two \u201cwonders\u201d deeply affected his early years. The first was his encounter with a compass at age five. He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his 'sacred little geometry book'. Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would never amount to anything.\"\"\" summarizer = EasySummarizer () # Summarize summaries = summarizer . summarize ( text = text , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , num_beams = 4 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Summaries: \\n \" ) for s in summaries : print ( s , \" \\n \" ) Output Einstein was mystified invisible forces could deflect the needle . the second wonder came at age 12 when he discovered a book of geometry . Einstein became deeply religious at age 12 .","title":"Summarization EasySummarizer"},{"location":"index.html#translation-easytranslator","text":"from adaptnlp import EasyTranslator text = [ \"Machine learning will take over the world very soon.\" , \"Machines can speak in many languages.\" ,] translator = EasyTranslator () # Translate translations = translator . translate ( text = text , t5_prefix = \"translate English to German\" , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Translations: \\n \" ) for t in translations : print ( t , \" \\n \" ) Output Das Maschinelle Lernen wird die Welt in K\u00fcrze \u00fcbernehmen. Maschinen k\u00f6nnen in vielen Sprachen sprechen.","title":"Translation EasyTranslator"},{"location":"index.html#tutorials","text":"Look in the Tutorials directory for a quick introduction to the library and its very simple and straight forward use cases: NLP Tasks Token Classification: NER, POS, Chunk, and Frame Tagging Sequence Classification: Sentiment Embeddings: Transformer Embeddings e.g. BERT, XLM, GPT2, XLNet, roBERTa, ALBERT Question Answering: Span-based Question Answering Model Summarization: Abstractive and Extractive Translation: Seq2Seq Custom Fine-Tuning and Training with Transformer Models Fine-tuning a Transformers Language Model Checkout the documentation for more information.","title":"Tutorials"},{"location":"index.html#rest-service","text":"We use FastAPI for standing up endpoints for serving state-of-the-art NLP models with AdaptNLP. The REST directory contains more detail on deploying a REST API locally or with docker in a very easy and fast way.","title":"REST Service"},{"location":"index.html#docker","text":"AdaptNLP official docker images are up on Docker Hub . Images have AdaptNLP installed from source in developer mode with tutorial notebooks available, and will default to launching a jupyter server from where you can start running the tutorial and workshop notebooks. Images can build with GPU support if NVIDA-Docker is correctly installed.","title":"Docker"},{"location":"index.html#pull-and-run-adaptnlp-immediately","text":"Simply run an image with AdaptNLP installed from source in developer mode by running: docker run -itp 8888:8888 achangnovetta/adaptnlp:latest Run an image with AdaptNLP running on GPUs if you have nvidia drivers and nvidia-docker 19.03+ installed: docker run -itp 8888:8888 --gpus all achangnovetta/adaptnlp:latest Check localhost:8888 or localhost:8888/lab to access the container notebook servers.","title":"Pull and Run AdaptNLP Immediately"},{"location":"index.html#build","text":"Refer to the docker/ directory and run the following to build and run adaptnlp from the available images. Note: A container with GPUs enabled requires Docker version 19.03+ and nvida-docker installed # From the repo directory docker build -t achangnovetta/adaptnlp:latest -f docker/runtime/Dockerfile.cuda11.0-runtime-ubuntu18.04-py3.8 . docker run -itp 8888:8888 achangnovetta/adaptnlp:latest If you want to use CUDA compatible GPUs docker run -itp 8888:8888 --gpus all achangnovetta/adaptnlp:latest Check localhost:8888 or localhost:8888/lab to access the container notebook servers.","title":"Build"},{"location":"index.html#contact","text":"Please contact the author Andrew Chang at achang@novetta.com with questions or comments regarding AdaptNLP. Follow us on Twitter at @achang1618 and @AdaptNLP for updates and NLP dialogue.","title":"Contact"},{"location":"index.html#license","text":"This project is licensed under the terms of the Apache 2.0 license.","title":"License"},{"location":"contributing.html","text":"Contributing for AdaptNLP \u00b6 Contributions are welcome for AdaptNLP via. PRs. Make sure all features, bugs, etc. changes are addressed with an issue number. If no issue posts regarding the PR are found, please create one before submitting the PR. Early contributions for v0.1.* may be limited due to rapid development and potential design changes.","title":"Contributing"},{"location":"contributing.html#contributing-for-adaptnlp","text":"Contributions are welcome for AdaptNLP via. PRs. Make sure all features, bugs, etc. changes are addressed with an issue number. If no issue posts regarding the PR are found, please create one before submitting the PR. Early contributions for v0.1.* may be limited due to rapid development and potential design changes.","title":"Contributing for AdaptNLP"},{"location":"rest.html","text":"AdaptNLP-Rest API \u00b6 Quick Start \u00b6 Docker Hub \u00b6 The docker image of AdaptNLP is built with the achangnovetta/adaptnlp:latest image. The docker image of AdaptNLP's NLP task rest services are hosted on Docker Hub as well here . Every image can be pulled and run by running the following: Token Tagging docker run -itp 5000:5000 -e TOKEN_TAGGING_MODE=ner -e TOKEN_TAGGING_MODEe=ner-ontonotes-fast achangnovetta/token-tagging:latest bash Sequence Classification docker run -itp 5000:5000 -e SEQUENCE_CLASSIFICATION_MODEL=nlptown/bert-base-multilingual-uncased-sentiment achangnovetta/sequence-classification:latest bash Question Answering docker run -itp 5000:5000 -e QUESTION_ANSWERING_MODEL=distilbert-base-uncased-distilled-squad achangnovetta/question-answering:latest bash Translation docker run -itp 5000:5000 -e TRANSLATION_MODEL=Helsinki-NLP/opus-mt-ar-e achangnovetta/translation:latest bash Summarization docker run -itp 5000:5000 -e SUMMARIZATION_MODEL=facebook/bart-large-cnn achangnovetta/summarization:latest bash Text Generation docker run -itp 5000:5000 -e TEXT_GENERATION_MODEL=gpt2 achangnovetta/text-generation:latest bash Note: Add the --gpus arg parameter if you'd like the images and endpoints to run with GPUs. You need to have NVIDIA Docker installed with a CUDA-compatible GPU. Build and Run \u00b6 Manually build and run your own docker images and deploy endpoints by following one of the below methods: 1. Docker Build Env Arg Entries \u00b6 Specify the pretrained models you want to use for the endpoints. These can be Transformers pre-trained models, Flair's pre-trained models, or your own custom trained models with a path pointing to the model. (The model must be in the directory you are building your image from for your respective NLP task) Token Tagger docker build -t token-tagging:latest --build-arg TOKEN_TAGGING_MODE=ner \\ --build-arg TOKEN_TAGGING_MODEL=ner-ontonotes-fast . docker run -itp 5000:5000 token-tagging:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all token-tagging:latest bash Sequence Classifier docker build -t sequence-classification:latest --build-arg SEQUENCE_CLASSIFICATION_MODEL=nlptown/bert-base-multilingual-uncased-sentiment . docker run -itp 5000:5000 sequence-classification:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all sequence-classification:latest bash Question Answering docker build -t question-answering:latest --build-arg QUESTION_ANSWERING_MODEL=distilbert-base-uncased-distilled-squad . docker run -itp 5000:5000 question-answering:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all question-answering:latest bash Translation docker build -t translation:latest --build-arg TRANSLATION_MODEL=Helsinki-NLP/opus-mt-ar-e . docker run -itp 5000:5000 translation:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all translation:latest bash Summarization docker build -t summarization:latest --build-arg SUMMARIZATION_MODEL=facebook/bart-large-cnn . docker run -itp 5000:5000 summarization:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all summarization:latest bash Text Generation docker build -t text-generation:latest --build-arg TEXT_GENERATION_MODEL=gpt2 . docker run -itp 5000:5000 text-generation:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all text-generation:latest bash 2. Docker Run Env Arg Entries \u00b6 Sometimes you may wont to specify the models as environment variables in docker post-build for convience or other reasons. To do so use the below commands to deploy any of the above NLP task services. The example below runs the token classification service. docker build -t token-tagging:latest . docker run -itp 5000:5000 -e TOKEN_TAGGING_MODE='ner' \\ -e TOKEN_TAGGING_MODEL='ner-ontonotes-fast' \\ token-tagging:latest \\ bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all -e TOKEN_TAGGING_MODE='ner' \\ -e TOKEN_TAGGING_MODEL='ner-ontonotes-fast' \\ token-tagging:latest \\ bash 3. Local \u00b6 If you just want to run the rest services locally in an environment that has AdaptNLP installed, you can run the following in whichever NLP task directory you would like. pip install - r requirements export TOKEN_TAGGING_MODE = ner export TOKEN_TAGGING_MODEL = ner - ontonotes - fast export SEQUENCE_CLASSIFICATION_MODEL = en - sentiment export QUESTION_ANSWERING_MODEL = distilbert - base - uncased - distilled - squad uvicorn app . main : app -- host 0.0 . 0.0 -- port 5000 SwaggerUI \u00b6 Access SwaggerUI console by going to localhost:5000/docs after deploying","title":"NLP Services with FastAPI"},{"location":"rest.html#adaptnlp-rest-api","text":"","title":"AdaptNLP-Rest API"},{"location":"rest.html#quick-start","text":"","title":"Quick Start"},{"location":"rest.html#docker-hub","text":"The docker image of AdaptNLP is built with the achangnovetta/adaptnlp:latest image. The docker image of AdaptNLP's NLP task rest services are hosted on Docker Hub as well here . Every image can be pulled and run by running the following: Token Tagging docker run -itp 5000:5000 -e TOKEN_TAGGING_MODE=ner -e TOKEN_TAGGING_MODEe=ner-ontonotes-fast achangnovetta/token-tagging:latest bash Sequence Classification docker run -itp 5000:5000 -e SEQUENCE_CLASSIFICATION_MODEL=nlptown/bert-base-multilingual-uncased-sentiment achangnovetta/sequence-classification:latest bash Question Answering docker run -itp 5000:5000 -e QUESTION_ANSWERING_MODEL=distilbert-base-uncased-distilled-squad achangnovetta/question-answering:latest bash Translation docker run -itp 5000:5000 -e TRANSLATION_MODEL=Helsinki-NLP/opus-mt-ar-e achangnovetta/translation:latest bash Summarization docker run -itp 5000:5000 -e SUMMARIZATION_MODEL=facebook/bart-large-cnn achangnovetta/summarization:latest bash Text Generation docker run -itp 5000:5000 -e TEXT_GENERATION_MODEL=gpt2 achangnovetta/text-generation:latest bash Note: Add the --gpus arg parameter if you'd like the images and endpoints to run with GPUs. You need to have NVIDIA Docker installed with a CUDA-compatible GPU.","title":"Docker Hub"},{"location":"rest.html#build-and-run","text":"Manually build and run your own docker images and deploy endpoints by following one of the below methods:","title":"Build and Run"},{"location":"rest.html#1-docker-build-env-arg-entries","text":"Specify the pretrained models you want to use for the endpoints. These can be Transformers pre-trained models, Flair's pre-trained models, or your own custom trained models with a path pointing to the model. (The model must be in the directory you are building your image from for your respective NLP task) Token Tagger docker build -t token-tagging:latest --build-arg TOKEN_TAGGING_MODE=ner \\ --build-arg TOKEN_TAGGING_MODEL=ner-ontonotes-fast . docker run -itp 5000:5000 token-tagging:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all token-tagging:latest bash Sequence Classifier docker build -t sequence-classification:latest --build-arg SEQUENCE_CLASSIFICATION_MODEL=nlptown/bert-base-multilingual-uncased-sentiment . docker run -itp 5000:5000 sequence-classification:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all sequence-classification:latest bash Question Answering docker build -t question-answering:latest --build-arg QUESTION_ANSWERING_MODEL=distilbert-base-uncased-distilled-squad . docker run -itp 5000:5000 question-answering:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all question-answering:latest bash Translation docker build -t translation:latest --build-arg TRANSLATION_MODEL=Helsinki-NLP/opus-mt-ar-e . docker run -itp 5000:5000 translation:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all translation:latest bash Summarization docker build -t summarization:latest --build-arg SUMMARIZATION_MODEL=facebook/bart-large-cnn . docker run -itp 5000:5000 summarization:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all summarization:latest bash Text Generation docker build -t text-generation:latest --build-arg TEXT_GENERATION_MODEL=gpt2 . docker run -itp 5000:5000 text-generation:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all text-generation:latest bash","title":"1. Docker Build Env Arg Entries"},{"location":"rest.html#2-docker-run-env-arg-entries","text":"Sometimes you may wont to specify the models as environment variables in docker post-build for convience or other reasons. To do so use the below commands to deploy any of the above NLP task services. The example below runs the token classification service. docker build -t token-tagging:latest . docker run -itp 5000:5000 -e TOKEN_TAGGING_MODE='ner' \\ -e TOKEN_TAGGING_MODEL='ner-ontonotes-fast' \\ token-tagging:latest \\ bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all -e TOKEN_TAGGING_MODE='ner' \\ -e TOKEN_TAGGING_MODEL='ner-ontonotes-fast' \\ token-tagging:latest \\ bash","title":"2. Docker Run Env Arg Entries"},{"location":"rest.html#3-local","text":"If you just want to run the rest services locally in an environment that has AdaptNLP installed, you can run the following in whichever NLP task directory you would like. pip install - r requirements export TOKEN_TAGGING_MODE = ner export TOKEN_TAGGING_MODEL = ner - ontonotes - fast export SEQUENCE_CLASSIFICATION_MODEL = en - sentiment export QUESTION_ANSWERING_MODEL = distilbert - base - uncased - distilled - squad uvicorn app . main : app -- host 0.0 . 0.0 -- port 5000","title":"3. Local"},{"location":"rest.html#swaggerui","text":"Access SwaggerUI console by going to localhost:5000/docs after deploying","title":"SwaggerUI"},{"location":"class-api/embeddings-module.html","text":"EasyWordEmbeddings \u00b6 class adaptnlp. EasyWordEmbeddings ( ) Word embeddings from the latest language models Usage: >>> embeddings = adaptnlp . EasyWordEmbeddings () >>> embeddings . embed_text ( \"text you want embeddings for\" , model_name_or_path = \"bert-base-cased\" ) embed_all ( self , text , *model_names_or_paths ) Embeds text with all embedding models loaded Parameters : * text - Text input, it can be a string or any of Flair's Sentence input formats * model_names_or_paths - A variable input of model names or paths to embed Return : * A list of Flair's Sentence s embed_text ( self , text , model_name_or_path='bert-base-cased' ) Produces embeddings for text Parameters : * text - Text input, it can be a string or any of Flair's Sentence input formats * model_name_or_path - The hosted model name key or model path Return : * A list of Flair's Sentence s EasyStackedEmbeddings \u00b6 class adaptnlp. EasyStackedEmbeddings ( *embeddings ) Word Embeddings that have been concatenated and \"stacked\" as specified by flair Usage: >>> embeddings = adaptnlp . EasyStackedEmbeddings ( \"bert-base-cased\" , \"gpt2\" , \"xlnet-base-cased\" ) Parameters: &ast;embeddings - Non-keyword variable number of strings specifying the embeddings you want to stack embed_text ( self , text ) Stacked embeddings Parameters : * text - Text input, it can be a string or any of Flair's Sentence input formats Return : * A list of Flair's Sentence s EasyDocumentEmbeddings \u00b6 class adaptnlp. EasyDocumentEmbeddings ( *embeddings , methods=['rnn', 'pool'] , configs={'pool_configs': {'fine_tune_mode': 'linear', 'pooling': 'mean'}, 'rnn_configs': {'hidden_size': 512, 'rnn_layers': 1, 'reproject_words': True, 'reproject_words_dimension': 256, 'bidirectional': False, 'dropout': 0.5, 'word_dropout': 0.0, 'locked_dropout': 0.0, 'rnn_type': 'GRU', 'fine_tune': True}} ) Document Embeddings generated by pool and rnn methods applied to the word embeddings of text Usage: >>> embeddings = adaptnlp . EasyDocumentEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" , methods [ \"rnn\" ]) Parameters: &ast;embeddings - Non-keyword variable number of strings referring to model names or paths methods - A list of strings to specify which document embeddings to use i.e. [\"rnn\", \"pool\"] (avoids unncessary loading of models if only using one) configs - A dictionary of configurations for flair's rnn and pool document embeddings >>> example_configs = { \"pool_configs\" : { \"fine_tune_mode\" : \"linear\" , \"pooling\" : \"mean\" , }, ... \"rnn_configs\" : { \"hidden_size\" : 512 , ... \"rnn_layers\" : 1 , ... \"reproject_words\" : True , ... \"reproject_words_dimension\" : 256 , ... \"bidirectional\" : False , ... \"dropout\" : 0.5 , ... \"word_dropout\" : 0.0 , ... \"locked_dropout\" : 0.0 , ... \"rnn_type\" : \"GRU\" , ... \"fine_tune\" : True , }, ... } embed_pool ( self , text ) Generate stacked embeddings with DocumentPoolEmbeddings Parameters : * text - Text input, it can be a string or any of Flair's Sentence input formats Return : * A list of Flair's Sentence s embed_rnn ( self , text ) Generate stacked embeddings with DocumentRNNEmbeddings Parameters : * text - Text input, it can be a string or any of Flair's Sentence input formats Return : * A list of Flair's Sentence s","title":"Embeddings"},{"location":"class-api/embeddings-module.html#easywordembeddings","text":"class adaptnlp. EasyWordEmbeddings ( ) Word embeddings from the latest language models Usage: >>> embeddings = adaptnlp . EasyWordEmbeddings () >>> embeddings . embed_text ( \"text you want embeddings for\" , model_name_or_path = \"bert-base-cased\" ) embed_all ( self , text , *model_names_or_paths ) Embeds text with all embedding models loaded Parameters : * text - Text input, it can be a string or any of Flair's Sentence input formats * model_names_or_paths - A variable input of model names or paths to embed Return : * A list of Flair's Sentence s embed_text ( self , text , model_name_or_path='bert-base-cased' ) Produces embeddings for text Parameters : * text - Text input, it can be a string or any of Flair's Sentence input formats * model_name_or_path - The hosted model name key or model path Return : * A list of Flair's Sentence s","title":"EasyWordEmbeddings"},{"location":"class-api/embeddings-module.html#easystackedembeddings","text":"class adaptnlp. EasyStackedEmbeddings ( *embeddings ) Word Embeddings that have been concatenated and \"stacked\" as specified by flair Usage: >>> embeddings = adaptnlp . EasyStackedEmbeddings ( \"bert-base-cased\" , \"gpt2\" , \"xlnet-base-cased\" ) Parameters: &ast;embeddings - Non-keyword variable number of strings specifying the embeddings you want to stack embed_text ( self , text ) Stacked embeddings Parameters : * text - Text input, it can be a string or any of Flair's Sentence input formats Return : * A list of Flair's Sentence s","title":"EasyStackedEmbeddings"},{"location":"class-api/embeddings-module.html#easydocumentembeddings","text":"class adaptnlp. EasyDocumentEmbeddings ( *embeddings , methods=['rnn', 'pool'] , configs={'pool_configs': {'fine_tune_mode': 'linear', 'pooling': 'mean'}, 'rnn_configs': {'hidden_size': 512, 'rnn_layers': 1, 'reproject_words': True, 'reproject_words_dimension': 256, 'bidirectional': False, 'dropout': 0.5, 'word_dropout': 0.0, 'locked_dropout': 0.0, 'rnn_type': 'GRU', 'fine_tune': True}} ) Document Embeddings generated by pool and rnn methods applied to the word embeddings of text Usage: >>> embeddings = adaptnlp . EasyDocumentEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" , methods [ \"rnn\" ]) Parameters: &ast;embeddings - Non-keyword variable number of strings referring to model names or paths methods - A list of strings to specify which document embeddings to use i.e. [\"rnn\", \"pool\"] (avoids unncessary loading of models if only using one) configs - A dictionary of configurations for flair's rnn and pool document embeddings >>> example_configs = { \"pool_configs\" : { \"fine_tune_mode\" : \"linear\" , \"pooling\" : \"mean\" , }, ... \"rnn_configs\" : { \"hidden_size\" : 512 , ... \"rnn_layers\" : 1 , ... \"reproject_words\" : True , ... \"reproject_words_dimension\" : 256 , ... \"bidirectional\" : False , ... \"dropout\" : 0.5 , ... \"word_dropout\" : 0.0 , ... \"locked_dropout\" : 0.0 , ... \"rnn_type\" : \"GRU\" , ... \"fine_tune\" : True , }, ... } embed_pool ( self , text ) Generate stacked embeddings with DocumentPoolEmbeddings Parameters : * text - Text input, it can be a string or any of Flair's Sentence input formats Return : * A list of Flair's Sentence s embed_rnn ( self , text ) Generate stacked embeddings with DocumentRNNEmbeddings Parameters : * text - Text input, it can be a string or any of Flair's Sentence input formats Return : * A list of Flair's Sentence s","title":"EasyDocumentEmbeddings"},{"location":"class-api/language-model-finetuner-module.html","text":"LMFineTuner \u00b6 class adaptnlp. LMFineTunerManual ( train_data_file , eval_data_file=None , model_type='bert' , model_name_or_path=None , mlm=True , mlm_probability=0.15 , config_name=None , tokenizer_name=None , cache_dir=None , block_size=-1 , no_cuda=False , overwrite_cache=False , seed=42 , fp16=False , fp16_opt_level='O1' , local_rank=-1 ) A Language Model Fine Tuner object you can set language model configurations and then train and evaluate Usage: >>> finetuner = adaptnlp . LMFineTuner () >>> finetuner . train () Parameters: train_data_file - The input training data file (a text file). eval_data_file - An optional input evaluation data file to evaluate the perplexity on (a text file). model_type - The model architecture to be trained or fine-tuned. model_name_or_path - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch. mlm - Train with masked-language modeling loss instead of language modeling. mlm_probability - Ratio of tokens to mask for masked language modeling loss config_name - Optional Transformers pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config. tokenizer_name - Optional Transformers pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer. cache_dir - Optional directory to store the pre-trained models downloaded from s3 (If None, will go to default dir) block_size - Optional input sequence length after tokenization. The training dataset will be truncated in block of this size for training.\" -1 will default to the model max input length for single sentence inputs (take into account special tokens). no_cuda - Avoid using CUDA when available overwrite_cache - Overwrite the cached training and evaluation sets seed - random seed for initialization fp16 - Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit fp16_opt_level - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. local_rank - For distributed training: local_rank find_learning_rate ( self , output_dir , file_name='learning_rate.tsv' , start_learning_rate=1e-07 , end_learning_rate=10 , iterations=100 , mini_batch_size=8 , stop_early=True , smoothing_factor=0.7 , adam_epsilon=1e-08 , weight_decay=0.0 , **kwargs ) Uses Leslie's cyclical learning rate finding method to generate and save the loss x learning rate plot This method returns a suggested learning rate using the static method LMFineTuner.suggest_learning_rate() which is implicitly run in this method. output_dir - Path to dir for learning rate file to be saved file_name - Name of learning rate .tsv file start_learning_rate - Initial learning rate to start cyclical learning rate finder method end_learning_rate - End learning rate to stop exponential increase of the learning rate iterations - Number of optimizer iterations for the ExpAnnealLR scheduler mini_batch_size - Batch size for dataloader stop_early - Bool for stopping early once loss diverges smoothing_factor - Smoothing factor on moving average of losses adam_epsilon - Epsilon for Adam optimizer. weight_decay - Weight decay if we apply some. kwargs - Additional keyword arguments for the Adam optimizer return - Learning rate as a float suggest_learning_rate ( losses , lrs , lr_diff=30 , loss_threshold=0.05 , adjust_value=1 ) Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method losses - Numpy array of losses lrs - Numpy array of exponentially increasing learning rates (must match dim of losses ) lr_diff - Learning rate Interval of slide ruler loss_threshold - Threshold of loss difference on interval where the sliding stops adjust_value - Coefficient for adjustment return - the optimal learning rate as a float train_one_cycle ( self , output_dir , should_continue=False , overwrite_output_dir=False , evaluate_during_training=False , per_gpu_train_batch_size=4 , gradient_accumulation_steps=1 , learning_rate=5e-05 , weight_decay=0.0 , adam_epsilon=1e-08 , max_grad_norm=1.0 , num_train_epochs=1.0 , max_steps=-1 , warmup_steps=0 , logging_steps=50 , save_steps=50 , save_total_limit=3 , use_tensorboard=False ) output_dir - The output directory where the model predictions and checkpoints will be written. should_continue - Whether to continue training from latest checkpoint in output_dir overwrite_output_dir - Overwrite the content of output directory output_dir evaluate_during_training - Run evaluation during training at each logging_step . per_gpu_train_batch_size - Batch size per GPU/CPU for training. (If evaluate_during_training is True, this is also the eval batch size gradient_accumulation_steps - Number of updates steps to accumulate before performing a backward/update pass learning_rate - The initial learning rate for Adam optimizer. weight_decay - Weight decay if we apply some. adam_epsilon - Epsilon for Adam optimizer. max_grad_norm - Max gradient norm. Duh num_train_epochs - Total number of training epochs to perform. max_steps - If > 0: set total number of training steps to perform. Override num_train_epochs . warmup_steps - Linear warmup over warmup_steps. logging_steps - Number of steps until logging occurs. save_steps - Number of steps until checkpoint is saved in output_dir save_total_limit - Limit the total amount of checkpoints, delete the older checkpoints in the output_dir , does not delete by default use_tensorboard - Only useable if tensorboard is installed return - None train ( self , output_dir , should_continue=False , overwrite_output_dir=False , evaluate_during_training=False , per_gpu_train_batch_size=4 , gradient_accumulation_steps=1 , learning_rate=5e-05 , weight_decay=0.0 , adam_epsilon=1e-08 , max_grad_norm=1.0 , num_train_epochs=1.0 , max_steps=-1 , warmup_steps=0 , logging_steps=50 , save_steps=50 , save_total_limit=3 , use_tensorboard=False ) output_dir - The output directory where the model predictions and checkpoints will be written. should_continue - Whether to continue training from latest checkpoint in output_dir overwrite_output_dir - Overwrite the content of output directory output_dir evaluate_during_training - Run evaluation during training at each logging_step . per_gpu_train_batch_size - Batch size per GPU/CPU for training. (If evaluate_during_training is True, this is also the eval batch size gradient_accumulation_steps - Number of updates steps to accumulate before performing a backward/update pass learning_rate - The initial learning rate for Adam optimizer. weight_decay - Weight decay if we apply some. adam_epsilon - Epsilon for Adam optimizer. max_grad_norm - Max gradient norm. Duh num_train_epochs - Total number of training epochs to perform. max_steps - If > 0: set total number of training steps to perform. Override num_train_epochs . warmup_steps - Linear warmup over warmup_steps. logging_steps - Number of steps until logging occurs. save_steps - Number of steps until checkpoint is saved in output_dir save_total_limit - Limit the total amount of checkpoints, delete the older checkpoints in the output_dir , does not delete by default use_tensorboard - Only useable if tensorboard is installed return - None evaluate ( self , output_dir , per_gpu_eval_batch_size=4 , prefix='' ) output_dir - The output directory where the model predictions and checkpoints will be written. per_gpu_eval_batch_size - Batch size per GPU/CPU for evaluation. prefix - Prefix of checkpoint i.e. \"checkpoint-50\" return - Results in a dictionary evaluate_all_checkpoints ( self , output_dir , per_gpu_eval_batch_size=4 ) output_dir - The output directory where the model predictions and checkpoints will be written. per_gpu_eval_batch_size - Batch size per GPU/CPU for evaluation. return - Results in a dictionary freeze ( self ) Freeze last classification layer group only unfreeze ( self ) Unfreeze all layers freeze_to ( self , n ) Freeze first n layers of model n - Starting from initial layer, freeze all layers up to nth layer inclusively","title":"Language Model Fine-tuning Manual"},{"location":"class-api/language-model-finetuner-module.html#lmfinetuner","text":"class adaptnlp. LMFineTunerManual ( train_data_file , eval_data_file=None , model_type='bert' , model_name_or_path=None , mlm=True , mlm_probability=0.15 , config_name=None , tokenizer_name=None , cache_dir=None , block_size=-1 , no_cuda=False , overwrite_cache=False , seed=42 , fp16=False , fp16_opt_level='O1' , local_rank=-1 ) A Language Model Fine Tuner object you can set language model configurations and then train and evaluate Usage: >>> finetuner = adaptnlp . LMFineTuner () >>> finetuner . train () Parameters: train_data_file - The input training data file (a text file). eval_data_file - An optional input evaluation data file to evaluate the perplexity on (a text file). model_type - The model architecture to be trained or fine-tuned. model_name_or_path - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch. mlm - Train with masked-language modeling loss instead of language modeling. mlm_probability - Ratio of tokens to mask for masked language modeling loss config_name - Optional Transformers pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config. tokenizer_name - Optional Transformers pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer. cache_dir - Optional directory to store the pre-trained models downloaded from s3 (If None, will go to default dir) block_size - Optional input sequence length after tokenization. The training dataset will be truncated in block of this size for training.\" -1 will default to the model max input length for single sentence inputs (take into account special tokens). no_cuda - Avoid using CUDA when available overwrite_cache - Overwrite the cached training and evaluation sets seed - random seed for initialization fp16 - Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit fp16_opt_level - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. local_rank - For distributed training: local_rank find_learning_rate ( self , output_dir , file_name='learning_rate.tsv' , start_learning_rate=1e-07 , end_learning_rate=10 , iterations=100 , mini_batch_size=8 , stop_early=True , smoothing_factor=0.7 , adam_epsilon=1e-08 , weight_decay=0.0 , **kwargs ) Uses Leslie's cyclical learning rate finding method to generate and save the loss x learning rate plot This method returns a suggested learning rate using the static method LMFineTuner.suggest_learning_rate() which is implicitly run in this method. output_dir - Path to dir for learning rate file to be saved file_name - Name of learning rate .tsv file start_learning_rate - Initial learning rate to start cyclical learning rate finder method end_learning_rate - End learning rate to stop exponential increase of the learning rate iterations - Number of optimizer iterations for the ExpAnnealLR scheduler mini_batch_size - Batch size for dataloader stop_early - Bool for stopping early once loss diverges smoothing_factor - Smoothing factor on moving average of losses adam_epsilon - Epsilon for Adam optimizer. weight_decay - Weight decay if we apply some. kwargs - Additional keyword arguments for the Adam optimizer return - Learning rate as a float suggest_learning_rate ( losses , lrs , lr_diff=30 , loss_threshold=0.05 , adjust_value=1 ) Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method losses - Numpy array of losses lrs - Numpy array of exponentially increasing learning rates (must match dim of losses ) lr_diff - Learning rate Interval of slide ruler loss_threshold - Threshold of loss difference on interval where the sliding stops adjust_value - Coefficient for adjustment return - the optimal learning rate as a float train_one_cycle ( self , output_dir , should_continue=False , overwrite_output_dir=False , evaluate_during_training=False , per_gpu_train_batch_size=4 , gradient_accumulation_steps=1 , learning_rate=5e-05 , weight_decay=0.0 , adam_epsilon=1e-08 , max_grad_norm=1.0 , num_train_epochs=1.0 , max_steps=-1 , warmup_steps=0 , logging_steps=50 , save_steps=50 , save_total_limit=3 , use_tensorboard=False ) output_dir - The output directory where the model predictions and checkpoints will be written. should_continue - Whether to continue training from latest checkpoint in output_dir overwrite_output_dir - Overwrite the content of output directory output_dir evaluate_during_training - Run evaluation during training at each logging_step . per_gpu_train_batch_size - Batch size per GPU/CPU for training. (If evaluate_during_training is True, this is also the eval batch size gradient_accumulation_steps - Number of updates steps to accumulate before performing a backward/update pass learning_rate - The initial learning rate for Adam optimizer. weight_decay - Weight decay if we apply some. adam_epsilon - Epsilon for Adam optimizer. max_grad_norm - Max gradient norm. Duh num_train_epochs - Total number of training epochs to perform. max_steps - If > 0: set total number of training steps to perform. Override num_train_epochs . warmup_steps - Linear warmup over warmup_steps. logging_steps - Number of steps until logging occurs. save_steps - Number of steps until checkpoint is saved in output_dir save_total_limit - Limit the total amount of checkpoints, delete the older checkpoints in the output_dir , does not delete by default use_tensorboard - Only useable if tensorboard is installed return - None train ( self , output_dir , should_continue=False , overwrite_output_dir=False , evaluate_during_training=False , per_gpu_train_batch_size=4 , gradient_accumulation_steps=1 , learning_rate=5e-05 , weight_decay=0.0 , adam_epsilon=1e-08 , max_grad_norm=1.0 , num_train_epochs=1.0 , max_steps=-1 , warmup_steps=0 , logging_steps=50 , save_steps=50 , save_total_limit=3 , use_tensorboard=False ) output_dir - The output directory where the model predictions and checkpoints will be written. should_continue - Whether to continue training from latest checkpoint in output_dir overwrite_output_dir - Overwrite the content of output directory output_dir evaluate_during_training - Run evaluation during training at each logging_step . per_gpu_train_batch_size - Batch size per GPU/CPU for training. (If evaluate_during_training is True, this is also the eval batch size gradient_accumulation_steps - Number of updates steps to accumulate before performing a backward/update pass learning_rate - The initial learning rate for Adam optimizer. weight_decay - Weight decay if we apply some. adam_epsilon - Epsilon for Adam optimizer. max_grad_norm - Max gradient norm. Duh num_train_epochs - Total number of training epochs to perform. max_steps - If > 0: set total number of training steps to perform. Override num_train_epochs . warmup_steps - Linear warmup over warmup_steps. logging_steps - Number of steps until logging occurs. save_steps - Number of steps until checkpoint is saved in output_dir save_total_limit - Limit the total amount of checkpoints, delete the older checkpoints in the output_dir , does not delete by default use_tensorboard - Only useable if tensorboard is installed return - None evaluate ( self , output_dir , per_gpu_eval_batch_size=4 , prefix='' ) output_dir - The output directory where the model predictions and checkpoints will be written. per_gpu_eval_batch_size - Batch size per GPU/CPU for evaluation. prefix - Prefix of checkpoint i.e. \"checkpoint-50\" return - Results in a dictionary evaluate_all_checkpoints ( self , output_dir , per_gpu_eval_batch_size=4 ) output_dir - The output directory where the model predictions and checkpoints will be written. per_gpu_eval_batch_size - Batch size per GPU/CPU for evaluation. return - Results in a dictionary freeze ( self ) Freeze last classification layer group only unfreeze ( self ) Unfreeze all layers freeze_to ( self , n ) Freeze first n layers of model n - Starting from initial layer, freeze all layers up to nth layer inclusively","title":"LMFineTuner"},{"location":"class-api/language-model-module.html","text":"LMFineTuner \u00b6 class adaptnlp. LMFineTuner ( model_name_or_path='bert-base-cased' ) A Language Model Fine Tuner object you can set language model configurations and then train and evaluate Usage: >>> finetuner = adaptnlp . LMFineTuner () >>> finetuner . train () Parameters: model_name_or_path - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch. train ( self , training_args , train_file , eval_file , line_by_line=False , mlm=False , mlm_probability=0.15 , plm_probability=0.16666666666666666 , max_span_length=5 , block_size=-1 , overwrite_cache=False ) Train and fine-tune the loaded language model train_file - The input training data file (a text file). eval_file - An optional input evaluation data file to evaluate the perplexity on (a text file). line_by_line - Whether distinct lines of text in the dataset are to be handled as distinct sequences. mlm - Train with masked-language modeling loss instead of language modeling. mlm_probability - Ratio of tokens to mask for masked language modeling loss plm_probability - Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling. max_span_length - Maximum length of a span of masked tokens for permutation language modeling. block_size - Optional input sequence length after tokenization. The training dataset will be truncated in block of this size for training.\" -1 will default to the model max input length for single sentence inputs (take into account special tokens). overwrite_cache - Overwrite the cached training and evaluation sets evaluate ( self )","title":"Language Model Fine-tuning"},{"location":"class-api/language-model-module.html#lmfinetuner","text":"class adaptnlp. LMFineTuner ( model_name_or_path='bert-base-cased' ) A Language Model Fine Tuner object you can set language model configurations and then train and evaluate Usage: >>> finetuner = adaptnlp . LMFineTuner () >>> finetuner . train () Parameters: model_name_or_path - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch. train ( self , training_args , train_file , eval_file , line_by_line=False , mlm=False , mlm_probability=0.15 , plm_probability=0.16666666666666666 , max_span_length=5 , block_size=-1 , overwrite_cache=False ) Train and fine-tune the loaded language model train_file - The input training data file (a text file). eval_file - An optional input evaluation data file to evaluate the perplexity on (a text file). line_by_line - Whether distinct lines of text in the dataset are to be handled as distinct sequences. mlm - Train with masked-language modeling loss instead of language modeling. mlm_probability - Ratio of tokens to mask for masked language modeling loss plm_probability - Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling. max_span_length - Maximum length of a span of masked tokens for permutation language modeling. block_size - Optional input sequence length after tokenization. The training dataset will be truncated in block of this size for training.\" -1 will default to the model max input length for single sentence inputs (take into account special tokens). overwrite_cache - Overwrite the cached training and evaluation sets evaluate ( self )","title":"LMFineTuner"},{"location":"class-api/question-answering-module.html","text":"EasyQuestionAnswering \u00b6 class adaptnlp. EasyQuestionAnswering ( ) Question Answering Module Usage: >>> qa = adaptnlp . EasyQuestionAnswering () >>> qa . predict_qa ( query = 'What is life?' , context = 'Life is NLP.' , n_best_size = 5 , mini_batch_size = 1 ) predict_qa ( self , query , context , n_best_size=5 , mini_batch_size=32 , model_name_or_path='bert-large-uncased-whole-word-masking-finetuned-squad' , **kwargs ) Predicts top_n answer spans of query in regards to context query - String or list of strings that specify the ordered questions corresponding to context context - String or list of strings that specify the ordered contexts corresponding to query n_best_size - The top n answers returned mini_batch_size - Mini batch size for inference model_name_or_path - Path to QA model or name of QA model at huggingface.co/models kwargs (Optional) - Keyword arguments for AdaptiveModel s like TransformersQuestionAnswering return - Either a list of string answers or a dict of the results TransformersQuestionAnswering \u00b6 class adaptnlp. TransformersQuestionAnswering ( tokenizer , model ) Adaptive Model for Transformers Question Answering Model Parameters tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers * model - A transformer Question Answering model load ( model_name_or_path ) Class method for loading and constructing this model model_name_or_path - A key string of one of Transformer's pre-trained Question Answering (SQUAD) models predict ( self , query , context , n_best_size=5 , mini_batch_size=32 , max_answer_length=10 , do_lower_case=False , version_2_with_negative=False , verbose_logging=False , null_score_diff_threshold=0.0 , max_seq_length=512 , doc_stride=128 , max_query_length=64 , **kwargs ) Predict method for running inference using the pre-trained question answering model query - String or list of strings that specify the ordered questions corresponding to context context - String or list of strings that specify the ordered contexts corresponding to query n_best_size - Number of top n results you want mini_batch_size - Mini batch size max_answer_length - Maximum token length for answers that are returned do_lower_case - Set as True if using uncased QA models version_2_with_negative - Set as True if using QA model with SQUAD2.0 verbose_logging - Set True if you want prediction verbose loggings null_score_diff_threshold - Threshold for predicting null(no answer) in Squad 2.0 Model. Default is 0.0. Raise this if you want fewer null answers max_seq_length - Maximum context token length. Check model configs to see max sequence length the model was trained with doc_stride - Number of token strides to take when splitting up conext into chunks of size max_seq_length max_query_length - Maximum token length for queries **kwargs (Optional) - Optional arguments for the Transformers model (mostly for saving evaluations)","title":"Question Answering"},{"location":"class-api/question-answering-module.html#easyquestionanswering","text":"class adaptnlp. EasyQuestionAnswering ( ) Question Answering Module Usage: >>> qa = adaptnlp . EasyQuestionAnswering () >>> qa . predict_qa ( query = 'What is life?' , context = 'Life is NLP.' , n_best_size = 5 , mini_batch_size = 1 ) predict_qa ( self , query , context , n_best_size=5 , mini_batch_size=32 , model_name_or_path='bert-large-uncased-whole-word-masking-finetuned-squad' , **kwargs ) Predicts top_n answer spans of query in regards to context query - String or list of strings that specify the ordered questions corresponding to context context - String or list of strings that specify the ordered contexts corresponding to query n_best_size - The top n answers returned mini_batch_size - Mini batch size for inference model_name_or_path - Path to QA model or name of QA model at huggingface.co/models kwargs (Optional) - Keyword arguments for AdaptiveModel s like TransformersQuestionAnswering return - Either a list of string answers or a dict of the results","title":"EasyQuestionAnswering"},{"location":"class-api/question-answering-module.html#transformersquestionanswering","text":"class adaptnlp. TransformersQuestionAnswering ( tokenizer , model ) Adaptive Model for Transformers Question Answering Model Parameters tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers * model - A transformer Question Answering model load ( model_name_or_path ) Class method for loading and constructing this model model_name_or_path - A key string of one of Transformer's pre-trained Question Answering (SQUAD) models predict ( self , query , context , n_best_size=5 , mini_batch_size=32 , max_answer_length=10 , do_lower_case=False , version_2_with_negative=False , verbose_logging=False , null_score_diff_threshold=0.0 , max_seq_length=512 , doc_stride=128 , max_query_length=64 , **kwargs ) Predict method for running inference using the pre-trained question answering model query - String or list of strings that specify the ordered questions corresponding to context context - String or list of strings that specify the ordered contexts corresponding to query n_best_size - Number of top n results you want mini_batch_size - Mini batch size max_answer_length - Maximum token length for answers that are returned do_lower_case - Set as True if using uncased QA models version_2_with_negative - Set as True if using QA model with SQUAD2.0 verbose_logging - Set True if you want prediction verbose loggings null_score_diff_threshold - Threshold for predicting null(no answer) in Squad 2.0 Model. Default is 0.0. Raise this if you want fewer null answers max_seq_length - Maximum context token length. Check model configs to see max sequence length the model was trained with doc_stride - Number of token strides to take when splitting up conext into chunks of size max_seq_length max_query_length - Maximum token length for queries **kwargs (Optional) - Optional arguments for the Transformers model (mostly for saving evaluations)","title":"TransformersQuestionAnswering"},{"location":"class-api/sequence-classifier-module.html","text":"EasySequenceClassifier \u00b6 class adaptnlp. EasySequenceClassifier ( ) Sequence classification models Usage: >>> classifier = EasySequenceClassifier () >>> classifier . tag_text ( text = 'text you want to label' , model_name_or_path = 'en-sentiment' ) tag_text ( self , text , model_name_or_path='en-sentiment' , mini_batch_size=32 , **kwargs ) Tags a text sequence with labels the sequence classification models have been trained on text - String, list of strings, Sentence , or list of Sentence s to be classified model_name_or_path - The model name key or model path mini_batch_size - The mini batch size for running inference **kwargs - (Optional) Keyword Arguments for Flair's TextClassifier.predict() method params return A list of Flair's Sentence 's release_model ( self , model_name_or_path ) Unload model from classifier and empty cuda mem cache (may leave residual cache per pytorch documentation on torch.cuda.empty_cache()) model_name_or_path - The model name or key path that you want to unload and release memory from train ( self , training_args , train_dataset , eval_dataset , model_name_or_path='bert-base-uncased' , text_col_nm='text' , label_col_nm='label' , label_names=None ) Trains and/or finetunes the sequence classification model model_name_or_path - The model name key or model path training_args - Transformers TrainingArguments object model train_dataset - Training Dataset class object from the datasets library or path to CSV file (labels must be int values) eval_dataset - Eval Dataset class object from the datasets library or path to CSV file (labels must be int values) text_col_nm - Name of the text feature column used as training data (Default 'text') label_col_nm - Name of the label feature column (Default 'label') label_names - (Only when loading CSV) An ordered list of label strings with int label mapped to string via. index value return - None evaluate ( self , model_name_or_path='bert-base-uncased' ) Evaluates model specified model_name_or_path - The model name key or model path FlairSequenceClassifier \u00b6 class adaptnlp. FlairSequenceClassifier ( model_name_or_path ) Adaptive Model for Flair's Sequence Classifier...very basic Usage: >>> classifier = FlairSequenceClassifier . load ( 'en-sentiment' ) >>> classifier . predict ( text = 'Example text' , mini_batch_size = 32 ) Parameters: model_name_or_path - A key string of one of Flair's pre-trained Sequence Classifier Model evaluate ( self ) get_preds ( self , dl=None , cbs=[] ) Get raw predictions based on dl with cbs . For basic inference, cbs should include any Callbacks needed to do general inference load ( model_name_or_path ) Class method for loading a constructing this classifier model_name_or_path - A key string of one of Flair's pre-trained Sequence Classifier Model predict ( self , text , mini_batch_size=32 , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size **kwargs (Optional) - Optional arguments for the Flair classifier set_as_dict ( self , as_dict=False ) Sets as_dict in _learn set_model ( self , model ) Sets model in _learn train ( self ) TransformersSequenceClassifier \u00b6 class adaptnlp. TransformersSequenceClassifier ( tokenizer , model ) Adaptive model for Transformer's Sequence Classification Model Usage: >>> classifier = TransformersSequenceClassifier . load ( 'transformers-sc-model' ) >>> classifier . predict ( text = 'Example text' , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Sequence Classsifciation model evaluate ( self ) Evaluates model specified model_name_or_path - The model name key or model path get_preds ( self , dl=None , cbs=[] ) Get raw predictions based on dl with cbs . For basic inference, cbs should include any Callbacks needed to do general inference load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained Sequence Classifier Model predict ( self , text , mini_batch_size=32 , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size **kwargs (Optional) - Optional arguments for the Transformers classifier set_as_dict ( self , as_dict=False ) Sets as_dict in _learn set_model ( self , model ) Sets model in _learn train ( self , training_args , train_dataset , eval_dataset , text_col_nm='text' , label_col_nm='label' , compute_metrics=None ) Trains and/or finetunes the sequence classification model training_args - Transformers TrainingArguments object model train_dataset - Training Dataset class object from the datasets library eval_dataset - Eval Dataset class object from the datasets library text_col_nm - Name of the text feature column used as training data (Default 'text') label_col_nm - Name of the label feature column (Default 'label') compute_metrics - Custom metrics function callable for transformers.Trainer 's compute metrics return - None","title":"Sequence Classifier"},{"location":"class-api/sequence-classifier-module.html#easysequenceclassifier","text":"class adaptnlp. EasySequenceClassifier ( ) Sequence classification models Usage: >>> classifier = EasySequenceClassifier () >>> classifier . tag_text ( text = 'text you want to label' , model_name_or_path = 'en-sentiment' ) tag_text ( self , text , model_name_or_path='en-sentiment' , mini_batch_size=32 , **kwargs ) Tags a text sequence with labels the sequence classification models have been trained on text - String, list of strings, Sentence , or list of Sentence s to be classified model_name_or_path - The model name key or model path mini_batch_size - The mini batch size for running inference **kwargs - (Optional) Keyword Arguments for Flair's TextClassifier.predict() method params return A list of Flair's Sentence 's release_model ( self , model_name_or_path ) Unload model from classifier and empty cuda mem cache (may leave residual cache per pytorch documentation on torch.cuda.empty_cache()) model_name_or_path - The model name or key path that you want to unload and release memory from train ( self , training_args , train_dataset , eval_dataset , model_name_or_path='bert-base-uncased' , text_col_nm='text' , label_col_nm='label' , label_names=None ) Trains and/or finetunes the sequence classification model model_name_or_path - The model name key or model path training_args - Transformers TrainingArguments object model train_dataset - Training Dataset class object from the datasets library or path to CSV file (labels must be int values) eval_dataset - Eval Dataset class object from the datasets library or path to CSV file (labels must be int values) text_col_nm - Name of the text feature column used as training data (Default 'text') label_col_nm - Name of the label feature column (Default 'label') label_names - (Only when loading CSV) An ordered list of label strings with int label mapped to string via. index value return - None evaluate ( self , model_name_or_path='bert-base-uncased' ) Evaluates model specified model_name_or_path - The model name key or model path","title":"EasySequenceClassifier"},{"location":"class-api/sequence-classifier-module.html#flairsequenceclassifier","text":"class adaptnlp. FlairSequenceClassifier ( model_name_or_path ) Adaptive Model for Flair's Sequence Classifier...very basic Usage: >>> classifier = FlairSequenceClassifier . load ( 'en-sentiment' ) >>> classifier . predict ( text = 'Example text' , mini_batch_size = 32 ) Parameters: model_name_or_path - A key string of one of Flair's pre-trained Sequence Classifier Model evaluate ( self ) get_preds ( self , dl=None , cbs=[] ) Get raw predictions based on dl with cbs . For basic inference, cbs should include any Callbacks needed to do general inference load ( model_name_or_path ) Class method for loading a constructing this classifier model_name_or_path - A key string of one of Flair's pre-trained Sequence Classifier Model predict ( self , text , mini_batch_size=32 , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size **kwargs (Optional) - Optional arguments for the Flair classifier set_as_dict ( self , as_dict=False ) Sets as_dict in _learn set_model ( self , model ) Sets model in _learn train ( self )","title":"FlairSequenceClassifier"},{"location":"class-api/sequence-classifier-module.html#transformerssequenceclassifier","text":"class adaptnlp. TransformersSequenceClassifier ( tokenizer , model ) Adaptive model for Transformer's Sequence Classification Model Usage: >>> classifier = TransformersSequenceClassifier . load ( 'transformers-sc-model' ) >>> classifier . predict ( text = 'Example text' , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Sequence Classsifciation model evaluate ( self ) Evaluates model specified model_name_or_path - The model name key or model path get_preds ( self , dl=None , cbs=[] ) Get raw predictions based on dl with cbs . For basic inference, cbs should include any Callbacks needed to do general inference load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained Sequence Classifier Model predict ( self , text , mini_batch_size=32 , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size **kwargs (Optional) - Optional arguments for the Transformers classifier set_as_dict ( self , as_dict=False ) Sets as_dict in _learn set_model ( self , model ) Sets model in _learn train ( self , training_args , train_dataset , eval_dataset , text_col_nm='text' , label_col_nm='label' , compute_metrics=None ) Trains and/or finetunes the sequence classification model training_args - Transformers TrainingArguments object model train_dataset - Training Dataset class object from the datasets library eval_dataset - Eval Dataset class object from the datasets library text_col_nm - Name of the text feature column used as training data (Default 'text') label_col_nm - Name of the label feature column (Default 'label') compute_metrics - Custom metrics function callable for transformers.Trainer 's compute metrics return - None","title":"TransformersSequenceClassifier"},{"location":"class-api/sequence-classifier-trainer-module.html","text":"SequenceClassifierTrainer \u00b6 class adaptnlp. SequenceClassifierTrainer ( corpus , encoder , column_name_map , corpus_in_memory=True , predictive_head='flair' , **kwargs ) Sequence Classifier Trainer Usage: >>> sc_trainer = SequenceClassifierTrainer ( corpus = \"/Path/to/data/dir\" ) Parameters: corpus - A flair corpus data model or Path /string to a directory with train.csv/test.csv/dev.csv encoder - A EasyDocumentEmbeddings object if training with a flair prediction head or Path /string if training with Transformer's prediction models column_name_map - Required if corpus is not a Corpus object, it's a dictionary specifying the indices of the text and label columns of the csv i.e. {1:\"text\",2:\"label\"} corpus_in_memory - Boolean for whether to store corpus embeddings in memory predictive_head - For now either \"flair\" or \"transformers\" for the prediction head **kwargs - Keyword arguments for Flair's TextClassifier model class find_learning_rate ( self , output_dir , file_name='learning_rate.tsv' , start_learning_rate=1e-08 , end_learning_rate=10 , iterations=100 , mini_batch_size=32 , stop_early=True , smoothing_factor=0.7 , plot_learning_rate=True , **kwargs ) Uses Leslie's cyclical learning rate finding method to generate and save the loss x learning rate plot This method returns a suggested learning rate using the static method LMFineTuner.suggest_learning_rate() which is implicitly run in this method. output_dir - Path to dir for learning rate file to be saved file_name - Name of learning rate .tsv file start_learning_rate - Initial learning rate to start cyclical learning rate finder method end_learning_rate - End learning rate to stop exponential increase of the learning rate iterations - Number of optimizer iterations for the ExpAnnealLR scheduler mini_batch_size - Batch size for dataloader stop_early - Bool for stopping early once loss diverges smoothing_factor - Smoothing factor on moving average of losses adam_epsilon - Epsilon for Adam optimizer. weight_decay - Weight decay if we apply some. kwargs - Additional keyword arguments for the Adam optimizer return - Learning rate as a float suggested_learning_rate ( losses , lrs , lr_diff=15 , loss_threshold=0.2 , adjust_value=1 ) Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method losses - Numpy array of losses lrs - Numpy array of exponentially increasing learning rates (must match dim of losses ) lr_diff - Learning rate Interval of slide ruler loss_threshold - Threshold of loss difference on interval where the sliding stops adjust_value - Coefficient for adjustment return - the optimal learning rate as a float train ( self , output_dir , learning_rate=0.07 , mini_batch_size=32 , anneal_factor=0.5 , patience=5 , max_epochs=150 , plot_weights=False , **kwargs ) Train the Sequence Classifier output_dir - The output directory where the model predictions and checkpoints will be written. learning_rate - The initial learning rate mini_batch_size - Batch size for the dataloader anneal_factor - The factor by which the learning rate is annealed patience - Patience is the number of epochs with no improvement the Trainer waits until annealing the learning rate max_epochs - Maximum number of epochs to train. Terminates training if this number is surpassed. plot_weights - Bool to plot weights or not kwargs - Keyword arguments for the rest of Flair's Trainer.train() hyperparameters","title":"Sequence Classifier Trainer"},{"location":"class-api/sequence-classifier-trainer-module.html#sequenceclassifiertrainer","text":"class adaptnlp. SequenceClassifierTrainer ( corpus , encoder , column_name_map , corpus_in_memory=True , predictive_head='flair' , **kwargs ) Sequence Classifier Trainer Usage: >>> sc_trainer = SequenceClassifierTrainer ( corpus = \"/Path/to/data/dir\" ) Parameters: corpus - A flair corpus data model or Path /string to a directory with train.csv/test.csv/dev.csv encoder - A EasyDocumentEmbeddings object if training with a flair prediction head or Path /string if training with Transformer's prediction models column_name_map - Required if corpus is not a Corpus object, it's a dictionary specifying the indices of the text and label columns of the csv i.e. {1:\"text\",2:\"label\"} corpus_in_memory - Boolean for whether to store corpus embeddings in memory predictive_head - For now either \"flair\" or \"transformers\" for the prediction head **kwargs - Keyword arguments for Flair's TextClassifier model class find_learning_rate ( self , output_dir , file_name='learning_rate.tsv' , start_learning_rate=1e-08 , end_learning_rate=10 , iterations=100 , mini_batch_size=32 , stop_early=True , smoothing_factor=0.7 , plot_learning_rate=True , **kwargs ) Uses Leslie's cyclical learning rate finding method to generate and save the loss x learning rate plot This method returns a suggested learning rate using the static method LMFineTuner.suggest_learning_rate() which is implicitly run in this method. output_dir - Path to dir for learning rate file to be saved file_name - Name of learning rate .tsv file start_learning_rate - Initial learning rate to start cyclical learning rate finder method end_learning_rate - End learning rate to stop exponential increase of the learning rate iterations - Number of optimizer iterations for the ExpAnnealLR scheduler mini_batch_size - Batch size for dataloader stop_early - Bool for stopping early once loss diverges smoothing_factor - Smoothing factor on moving average of losses adam_epsilon - Epsilon for Adam optimizer. weight_decay - Weight decay if we apply some. kwargs - Additional keyword arguments for the Adam optimizer return - Learning rate as a float suggested_learning_rate ( losses , lrs , lr_diff=15 , loss_threshold=0.2 , adjust_value=1 ) Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method losses - Numpy array of losses lrs - Numpy array of exponentially increasing learning rates (must match dim of losses ) lr_diff - Learning rate Interval of slide ruler loss_threshold - Threshold of loss difference on interval where the sliding stops adjust_value - Coefficient for adjustment return - the optimal learning rate as a float train ( self , output_dir , learning_rate=0.07 , mini_batch_size=32 , anneal_factor=0.5 , patience=5 , max_epochs=150 , plot_weights=False , **kwargs ) Train the Sequence Classifier output_dir - The output directory where the model predictions and checkpoints will be written. learning_rate - The initial learning rate mini_batch_size - Batch size for the dataloader anneal_factor - The factor by which the learning rate is annealed patience - Patience is the number of epochs with no improvement the Trainer waits until annealing the learning rate max_epochs - Maximum number of epochs to train. Terminates training if this number is surpassed. plot_weights - Bool to plot weights or not kwargs - Keyword arguments for the rest of Flair's Trainer.train() hyperparameters","title":"SequenceClassifierTrainer"},{"location":"class-api/summarizer-module.html","text":"EasySummarizer \u00b6 class adaptnlp. EasySummarizer ( ) Summarization Module Usage: >>> summarizer = EasySummarizer () >>> summarizer . summarize ( text = \"Summarize this text\" , model_name_or_path = \"t5-small\" ) summarize ( self , text , model_name_or_path='t5-small' , mini_batch_size=32 , num_beams=4 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on model_name_or_path - A String model id or path to a pre-trained model repository or custom trained model directory mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 4. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method TransformersSummarizer \u00b6 class adaptnlp. TransformersSummarizer ( tokenizer , model ) Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart conditiional generation models have a language modeling head) Usage: >>> summarizer = TransformersSummarizer . load ( \"transformers-summarizer-model\" ) >>> summarizer . predict ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Conditional Generation (Bart or T5) or Language model evaluate ( self ) get_preds ( self , dl=None , cbs=[] ) Get raw predictions based on dl with cbs . For basic inference, cbs should include any Callbacks needed to do general inference load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained Summarizer Model predict ( self , text , mini_batch_size=32 , num_beams=4 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 4. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method set_as_dict ( self , as_dict=False ) Sets as_dict in _learn set_model ( self , model ) Sets model in _learn train ( self )","title":"Summarizer"},{"location":"class-api/summarizer-module.html#easysummarizer","text":"class adaptnlp. EasySummarizer ( ) Summarization Module Usage: >>> summarizer = EasySummarizer () >>> summarizer . summarize ( text = \"Summarize this text\" , model_name_or_path = \"t5-small\" ) summarize ( self , text , model_name_or_path='t5-small' , mini_batch_size=32 , num_beams=4 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on model_name_or_path - A String model id or path to a pre-trained model repository or custom trained model directory mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 4. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method","title":"EasySummarizer"},{"location":"class-api/summarizer-module.html#transformerssummarizer","text":"class adaptnlp. TransformersSummarizer ( tokenizer , model ) Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart conditiional generation models have a language modeling head) Usage: >>> summarizer = TransformersSummarizer . load ( \"transformers-summarizer-model\" ) >>> summarizer . predict ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Conditional Generation (Bart or T5) or Language model evaluate ( self ) get_preds ( self , dl=None , cbs=[] ) Get raw predictions based on dl with cbs . For basic inference, cbs should include any Callbacks needed to do general inference load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained Summarizer Model predict ( self , text , mini_batch_size=32 , num_beams=4 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 4. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method set_as_dict ( self , as_dict=False ) Sets as_dict in _learn set_model ( self , model ) Sets model in _learn train ( self )","title":"TransformersSummarizer"},{"location":"class-api/text-generator-module.html","text":"EasyTextGenerator \u00b6 class adaptnlp. EasyTextGenerator ( ) Text Generation Module Usage: >>> generator = EasyGenerator () >>> generator . generate ( text = \"generate from this text\" , num_tokens_to_produce = 50 ) generate ( self , text , model_name_or_path='gpt2' , mini_batch_size=32 , num_tokens_to_produce=50 , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on model_name_or_path - A String model id or path to a pre-trained model repository or custom trained model directory mini_batch_size - Mini batch size num_tokens_to_produce - Number of tokens you want to generate **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method TransformersTextGenerator \u00b6 class adaptnlp. TransformersTextGenerator ( tokenizer , model ) Adaptive model for Transformer's Language Models Usage: >>> generator = TransformersTextGenerator . load ( \"gpt2\" ) >>> generator . generate ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Language model evaluate ( self ) get_preds ( self , dl=None , cbs=[] ) Get raw predictions based on dl with cbs . For basic inference, cbs should include any Callbacks needed to do general inference load ( model_name_or_path ) Class method for loading and constructing this Model model_name_or_path - A key string of one of Transformer's pre-trained Language Model predict ( self , text , mini_batch_size=32 , num_tokens_to_produce=50 , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size num_tokens_to_produce - Number of tokens you want to generate **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method set_as_dict ( self , as_dict=False ) Sets as_dict in _learn set_model ( self , model ) Sets model in _learn train ( self )","title":"Text Generator"},{"location":"class-api/text-generator-module.html#easytextgenerator","text":"class adaptnlp. EasyTextGenerator ( ) Text Generation Module Usage: >>> generator = EasyGenerator () >>> generator . generate ( text = \"generate from this text\" , num_tokens_to_produce = 50 ) generate ( self , text , model_name_or_path='gpt2' , mini_batch_size=32 , num_tokens_to_produce=50 , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on model_name_or_path - A String model id or path to a pre-trained model repository or custom trained model directory mini_batch_size - Mini batch size num_tokens_to_produce - Number of tokens you want to generate **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method","title":"EasyTextGenerator"},{"location":"class-api/text-generator-module.html#transformerstextgenerator","text":"class adaptnlp. TransformersTextGenerator ( tokenizer , model ) Adaptive model for Transformer's Language Models Usage: >>> generator = TransformersTextGenerator . load ( \"gpt2\" ) >>> generator . generate ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Language model evaluate ( self ) get_preds ( self , dl=None , cbs=[] ) Get raw predictions based on dl with cbs . For basic inference, cbs should include any Callbacks needed to do general inference load ( model_name_or_path ) Class method for loading and constructing this Model model_name_or_path - A key string of one of Transformer's pre-trained Language Model predict ( self , text , mini_batch_size=32 , num_tokens_to_produce=50 , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size num_tokens_to_produce - Number of tokens you want to generate **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method set_as_dict ( self , as_dict=False ) Sets as_dict in _learn set_model ( self , model ) Sets model in _learn train ( self )","title":"TransformersTextGenerator"},{"location":"class-api/token-tagger-module.html","text":"EasyTokenTagger \u00b6 class adaptnlp. EasyTokenTagger ( ) Token level classification models Usage: >>> tagger = adaptnlp . EasyTokenTagger () >>> tagger . tag_text ( text = \"text you want to tag\" , model_name_or_path = \"ner-ontonotes\" ) tag_text ( self , text , model_name_or_path='ner-ontonotes' , mini_batch_size=32 , **kwargs ) Tags tokens with labels the token classification models have been trained on text - Text input, it can be a string or any of Flair's Sentence input formats model_name_or_path - The hosted model name key or model path mini_batch_size - The mini batch size for running inference **kwargs - Keyword arguments for Flair's SequenceTagger.predict() method return - A list of Flair's Sentence 's tag_all ( self , text , mini_batch_size=32 , **kwargs ) Tags tokens with all labels from all token classification models text - Text input, it can be a string or any of Flair's Sentence input formats mini_batch_size - The mini batch size for running inference **kwargs - Keyword arguments for Flair's SequenceTagger.predict() method return A list of Flair's Sentence 's","title":"Token Tagger"},{"location":"class-api/token-tagger-module.html#easytokentagger","text":"class adaptnlp. EasyTokenTagger ( ) Token level classification models Usage: >>> tagger = adaptnlp . EasyTokenTagger () >>> tagger . tag_text ( text = \"text you want to tag\" , model_name_or_path = \"ner-ontonotes\" ) tag_text ( self , text , model_name_or_path='ner-ontonotes' , mini_batch_size=32 , **kwargs ) Tags tokens with labels the token classification models have been trained on text - Text input, it can be a string or any of Flair's Sentence input formats model_name_or_path - The hosted model name key or model path mini_batch_size - The mini batch size for running inference **kwargs - Keyword arguments for Flair's SequenceTagger.predict() method return - A list of Flair's Sentence 's tag_all ( self , text , mini_batch_size=32 , **kwargs ) Tags tokens with all labels from all token classification models text - Text input, it can be a string or any of Flair's Sentence input formats mini_batch_size - The mini batch size for running inference **kwargs - Keyword arguments for Flair's SequenceTagger.predict() method return A list of Flair's Sentence 's","title":"EasyTokenTagger"},{"location":"class-api/translator-module.html","text":"EasyTranslator \u00b6 class adaptnlp. EasyTranslator ( ) Translation Module Usage: >>> translator = EasyTranslator () >>> translator . translate ( text = \"translate this text\" , model_name_or_path = \"t5-small\" ) translate ( self , text , model_name_or_path='t5-small' , t5_prefix='translate English to German' , mini_batch_size=32 , num_beams=1 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on model_name_or_path - A String model id or path to a pre-trained model repository or custom trained model directory t5_prefix (Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models. mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method TransformersTranslator \u00b6 class adaptnlp. TransformersTranslator ( tokenizer , model ) Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart conditional generation models have a language modeling head) Usage: >>> translator = TransformersTranslator . load ( 'transformers-translator-model' ) >>> translator . predict ( text = 'Example text' , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Conditional Generation (Bart or T5) or Language model evaluate ( self ) get_preds ( self , dl=None , cbs=[] ) Get raw predictions based on dl with cbs . For basic inference, cbs should include any Callbacks needed to do general inference load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained translator Model predict ( self , text , t5_prefix='translate English to German' , mini_batch_size=32 , num_beams=1 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on t5_prefix (Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models. mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method set_as_dict ( self , as_dict=False ) Sets as_dict in _learn set_model ( self , model ) Sets model in _learn train ( self )","title":"Translator"},{"location":"class-api/translator-module.html#easytranslator","text":"class adaptnlp. EasyTranslator ( ) Translation Module Usage: >>> translator = EasyTranslator () >>> translator . translate ( text = \"translate this text\" , model_name_or_path = \"t5-small\" ) translate ( self , text , model_name_or_path='t5-small' , t5_prefix='translate English to German' , mini_batch_size=32 , num_beams=1 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on model_name_or_path - A String model id or path to a pre-trained model repository or custom trained model directory t5_prefix (Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models. mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method","title":"EasyTranslator"},{"location":"class-api/translator-module.html#transformerstranslator","text":"class adaptnlp. TransformersTranslator ( tokenizer , model ) Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart conditional generation models have a language modeling head) Usage: >>> translator = TransformersTranslator . load ( 'transformers-translator-model' ) >>> translator . predict ( text = 'Example text' , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Conditional Generation (Bart or T5) or Language model evaluate ( self ) get_preds ( self , dl=None , cbs=[] ) Get raw predictions based on dl with cbs . For basic inference, cbs should include any Callbacks needed to do general inference load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained translator Model predict ( self , text , t5_prefix='translate English to German' , mini_batch_size=32 , num_beams=1 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on t5_prefix (Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models. mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method set_as_dict ( self , as_dict=False ) Sets as_dict in _learn set_model ( self , model ) Sets model in _learn train ( self )","title":"TransformersTranslator"},{"location":"tutorial/index.html","text":"This tutorial section goes over the NLP capabilities available through AdaptNLP and how to use them. You should ideally follow the tutorials along with the provided notebooks in the tutorials directory at the top level of the AdaptNLP library. You could also run the code snippets in these tutorials straight through the python interpreter as well. Install and Setup \u00b6 You will first need to install AdaptNLP with Python 3.6+ using the following command: pip install adaptnlp AdaptNLP is largely built on top of Flair, Transformers, and PyTorch, and dependencies will be handled on install. AdaptNLP can be used with or without GPUs. AdaptNLP will automatically make use of GPU VRAM in environment with CUAD-compatible NVIDIA GPUs and NVIDIA drivers installed. GPU-less environments will run AdaptNLP modules fine as well. Overview of NLP Capabilities \u00b6 An overview of some of the AdaptNLP capabilities via. code snippets. A good way to make sure AdaptNLP is installed correctly is by running each of these code snippets. Note this may take a while if models have not already been downloaded. Named Entity Recognition with EasyTokenTagger \u00b6 from adaptnlp import EasyTokenTagger ## Example Text example_text = \"Novetta's headquarters is located in Mclean, Virginia.\" ## Load the token tagger module and tag text with the NER model tagger = EasyTokenTagger () sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner\" ) ## Output tagged token span results in Flair's Sentence object model for sentence in sentences : for entity in sentence . get_spans ( \"ner\" ): print ( entity ) Output Span [ 1 ]: \"Novetta\" [ \u2212 Labels : ORG ( 0.9925 )] Span [ 7 ]: \"Mclean\" [ \u2212 Labels : LOC ( 0.9993 )] Span [ 9 ]: \"Virginia\" [ \u2212 Labels : LOC ( 1.0 )] English Sentiment Classifier EasySequenceClassifier \u00b6 from adaptnlp import EasySequenceClassifier ## Example Text example_text = \"Novetta is a great company that was chosen as one of top 50 great places to work!\" ## Load the sequence classifier module and classify sequence of text with the english sentiment model classifier = EasySequenceClassifier () sentences = classifier . tag_text ( text = example_text , model_name_or_path = \"en-sentiment\" ) ## Output labeled text results in Flair's Sentence object model for sentence in sentences : print ( sentence . labels ) Output [ POSITIVE ( 0.9977 )] Language Model Embeddings EasyWordEmbeddings \u00b6 from adaptnlp import EasyWordEmbeddings ## Example Text example_text = \"Albert Einstein used to work at Novetta.\" ## Load the embeddings module and embed the tokens within the text embeddings = EasyWordEmbeddings () sentences = embeddings . embed_text ( example_text , model_name_or_path = \"gpt2\" ) # Iterate through tokens in the sentence to access embeddings for sentence in sentences : for token in sentence : print ( token . get_embedding ()) Output tensor ([ - 1.8757 , 0.6195 , - 1.3108 , ... , - 1.3787 , - 0.6885 , 1.6934 ]) tensor ([ - 0.0617 , - 2.3885 , 2.2028 , ... , 0.2774 , 0.8424 , - 1.5328 ]) tensor ([ - 0.0480 , - 0.7461 , - 0.5282 , ... , 0.1554 , 0.2542 , 0.8199 ]) tensor ([ 1.0621 , - 0.3834 , 1.5259 , ... , - 0.0937 , - 0.0337 , 1.0316 ]) tensor ([ - 0.0027 , - 1.6549 , - 1.6274 , ... , 0.3001 , 0.0146 , - 0.1931 ]) tensor ([ 0.6624 , - 0.9889 , 0.6716 , ... , - 0.4907 , 0.5692 , 0.9456 ]) tensor ([ 0.5633 , 0.4789 , - 0.2232 , ... , - 0.1454 , 0.2486 , 0.5163 ]) Span-based Question Answering EasyQuestionAnswering \u00b6 from adaptnlp import EasyQuestionAnswering from pprint import pprint ## Example Query and Context query = \"What is the meaning of life?\" context = \"Machine Learning is the meaning of life.\" top_n = 5 ## Load the QA module and run inference on results qa = EasyQuestionAnswering () best_answer , best_n_answers = qa . predict_qa ( query = query , context = context , n_best_size = top_n ) ## Output top answer as well as top 5 answers print ( best_answer ) pprint ( best_n_answers ) Output [ OrderedDict ([( 'text' , 'Machine Learning' ), ( 'probability' , 0.9924118248851219 ), ( 'start_logit' , 8.646799087524414 ), ( 'end_logit' , 8.419432640075684 ), ( 'start_index' , 0 ), ( 'end_index' , 1 )]), OrderedDict ([( 'text' , 'Learning' ), ( 'probability' , 0.004796293656050888 ), ( 'start_logit' , 3.314504384994507 ), ( 'end_logit' , 8.419432640075684 ), ( 'start_index' , 1 ), ( 'end_index' , 1 )]), OrderedDict ([( 'text' , 'Machine Learning is the meaning of life.' ), ( 'probability' , 0.0018383556202966893 ), ( 'start_logit' , 8.646799087524414 ), ( 'end_logit' , 2.1281659603118896 ), ( 'start_index' , 0 ), ( 'end_index' , 6 )]), OrderedDict ([( 'text' , 'Machine' ), ( 'probability' , 0.0009446411263795704 ), ( 'start_logit' , 8.646799087524414 ), ( 'end_logit' , 1.4623442888259888 ), ( 'start_index' , 0 ), ( 'end_index' , 0 )]), OrderedDict ([( 'text' , 'Learning is the meaning of life.' ), ( 'probability' , 8.884712150840367e-06 ), ( 'start_logit' , 3.314504384994507 ), ( 'end_logit' , 2.1281659603118896 ), ( 'start_index' , 1 ), ( 'end_index' , 6 )])]","title":"Tutorial - Intro"},{"location":"tutorial/index.html#install-and-setup","text":"You will first need to install AdaptNLP with Python 3.6+ using the following command: pip install adaptnlp AdaptNLP is largely built on top of Flair, Transformers, and PyTorch, and dependencies will be handled on install. AdaptNLP can be used with or without GPUs. AdaptNLP will automatically make use of GPU VRAM in environment with CUAD-compatible NVIDIA GPUs and NVIDIA drivers installed. GPU-less environments will run AdaptNLP modules fine as well.","title":"Install and Setup"},{"location":"tutorial/index.html#overview-of-nlp-capabilities","text":"An overview of some of the AdaptNLP capabilities via. code snippets. A good way to make sure AdaptNLP is installed correctly is by running each of these code snippets. Note this may take a while if models have not already been downloaded.","title":"Overview of NLP Capabilities"},{"location":"tutorial/index.html#named-entity-recognition-with-easytokentagger","text":"from adaptnlp import EasyTokenTagger ## Example Text example_text = \"Novetta's headquarters is located in Mclean, Virginia.\" ## Load the token tagger module and tag text with the NER model tagger = EasyTokenTagger () sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner\" ) ## Output tagged token span results in Flair's Sentence object model for sentence in sentences : for entity in sentence . get_spans ( \"ner\" ): print ( entity ) Output Span [ 1 ]: \"Novetta\" [ \u2212 Labels : ORG ( 0.9925 )] Span [ 7 ]: \"Mclean\" [ \u2212 Labels : LOC ( 0.9993 )] Span [ 9 ]: \"Virginia\" [ \u2212 Labels : LOC ( 1.0 )]","title":"Named Entity Recognition with EasyTokenTagger"},{"location":"tutorial/index.html#english-sentiment-classifier-easysequenceclassifier","text":"from adaptnlp import EasySequenceClassifier ## Example Text example_text = \"Novetta is a great company that was chosen as one of top 50 great places to work!\" ## Load the sequence classifier module and classify sequence of text with the english sentiment model classifier = EasySequenceClassifier () sentences = classifier . tag_text ( text = example_text , model_name_or_path = \"en-sentiment\" ) ## Output labeled text results in Flair's Sentence object model for sentence in sentences : print ( sentence . labels ) Output [ POSITIVE ( 0.9977 )]","title":"English Sentiment Classifier EasySequenceClassifier"},{"location":"tutorial/index.html#language-model-embeddings-easywordembeddings","text":"from adaptnlp import EasyWordEmbeddings ## Example Text example_text = \"Albert Einstein used to work at Novetta.\" ## Load the embeddings module and embed the tokens within the text embeddings = EasyWordEmbeddings () sentences = embeddings . embed_text ( example_text , model_name_or_path = \"gpt2\" ) # Iterate through tokens in the sentence to access embeddings for sentence in sentences : for token in sentence : print ( token . get_embedding ()) Output tensor ([ - 1.8757 , 0.6195 , - 1.3108 , ... , - 1.3787 , - 0.6885 , 1.6934 ]) tensor ([ - 0.0617 , - 2.3885 , 2.2028 , ... , 0.2774 , 0.8424 , - 1.5328 ]) tensor ([ - 0.0480 , - 0.7461 , - 0.5282 , ... , 0.1554 , 0.2542 , 0.8199 ]) tensor ([ 1.0621 , - 0.3834 , 1.5259 , ... , - 0.0937 , - 0.0337 , 1.0316 ]) tensor ([ - 0.0027 , - 1.6549 , - 1.6274 , ... , 0.3001 , 0.0146 , - 0.1931 ]) tensor ([ 0.6624 , - 0.9889 , 0.6716 , ... , - 0.4907 , 0.5692 , 0.9456 ]) tensor ([ 0.5633 , 0.4789 , - 0.2232 , ... , - 0.1454 , 0.2486 , 0.5163 ])","title":"Language Model Embeddings EasyWordEmbeddings"},{"location":"tutorial/index.html#span-based-question-answering-easyquestionanswering","text":"from adaptnlp import EasyQuestionAnswering from pprint import pprint ## Example Query and Context query = \"What is the meaning of life?\" context = \"Machine Learning is the meaning of life.\" top_n = 5 ## Load the QA module and run inference on results qa = EasyQuestionAnswering () best_answer , best_n_answers = qa . predict_qa ( query = query , context = context , n_best_size = top_n ) ## Output top answer as well as top 5 answers print ( best_answer ) pprint ( best_n_answers ) Output [ OrderedDict ([( 'text' , 'Machine Learning' ), ( 'probability' , 0.9924118248851219 ), ( 'start_logit' , 8.646799087524414 ), ( 'end_logit' , 8.419432640075684 ), ( 'start_index' , 0 ), ( 'end_index' , 1 )]), OrderedDict ([( 'text' , 'Learning' ), ( 'probability' , 0.004796293656050888 ), ( 'start_logit' , 3.314504384994507 ), ( 'end_logit' , 8.419432640075684 ), ( 'start_index' , 1 ), ( 'end_index' , 1 )]), OrderedDict ([( 'text' , 'Machine Learning is the meaning of life.' ), ( 'probability' , 0.0018383556202966893 ), ( 'start_logit' , 8.646799087524414 ), ( 'end_logit' , 2.1281659603118896 ), ( 'start_index' , 0 ), ( 'end_index' , 6 )]), OrderedDict ([( 'text' , 'Machine' ), ( 'probability' , 0.0009446411263795704 ), ( 'start_logit' , 8.646799087524414 ), ( 'end_logit' , 1.4623442888259888 ), ( 'start_index' , 0 ), ( 'end_index' , 0 )]), OrderedDict ([( 'text' , 'Learning is the meaning of life.' ), ( 'probability' , 8.884712150840367e-06 ), ( 'start_logit' , 3.314504384994507 ), ( 'end_logit' , 2.1281659603118896 ), ( 'start_index' , 1 ), ( 'end_index' , 6 )])]","title":"Span-based Question Answering EasyQuestionAnswering"},{"location":"tutorial/advanced-index.html","text":"This advanced tutorial section goes over using AdaptNLP for training and fine-tuning your own custom NLP models to get State-of-the-Art results. You should ideally follow the tutorials along with the provided notebooks in the tutorials directory at the top level of the AdaptNLP library. You could also run the code snippets in these tutorials straight through the python interpreter as well. Install and Setup \u00b6 AdaptNLP can be used with or without GPUs. AdaptNLP will automatically make use of GPU VRAM in environment with CUDA-compatible NVIDIA GPUs and NVIDIA drivers installed. GPU-less environments will run AdaptNLP modules fine as well. You will almost always want to utilize GPUs for training and fine-tuning useful NLP models, so a CUDA-compatible NVIDIA GPU is a must. Multi-GPU environments with Apex installed can allow for distributed and/or mixed precision training. Overview of Training and Finetuning Capabilities \u00b6 Downstream NLP-task realted models can be trained with encoders providing accurate word representations via. pre-trained language models like ALBERT, GPT2, and other transformer models. With the concepts of ULMFiT in mind, AdaptNLP's approach in training downstream predictive NLP models like sequence classification takes a step further than just utilizing pre-trained contextualized embeddings. We are able to effectively fine-tune state-of-the-art language models for useful NLP tasks on various domain specific data with the help of the adaptnlp.LMFineTuner class and the trainers that are provided in our \"Easy\" modules like adaptnlp.EasySequenceClassifier .","title":"Advanced - Intro"},{"location":"tutorial/advanced-index.html#install-and-setup","text":"AdaptNLP can be used with or without GPUs. AdaptNLP will automatically make use of GPU VRAM in environment with CUDA-compatible NVIDIA GPUs and NVIDIA drivers installed. GPU-less environments will run AdaptNLP modules fine as well. You will almost always want to utilize GPUs for training and fine-tuning useful NLP models, so a CUDA-compatible NVIDIA GPU is a must. Multi-GPU environments with Apex installed can allow for distributed and/or mixed precision training.","title":"Install and Setup"},{"location":"tutorial/advanced-index.html#overview-of-training-and-finetuning-capabilities","text":"Downstream NLP-task realted models can be trained with encoders providing accurate word representations via. pre-trained language models like ALBERT, GPT2, and other transformer models. With the concepts of ULMFiT in mind, AdaptNLP's approach in training downstream predictive NLP models like sequence classification takes a step further than just utilizing pre-trained contextualized embeddings. We are able to effectively fine-tune state-of-the-art language models for useful NLP tasks on various domain specific data with the help of the adaptnlp.LMFineTuner class and the trainers that are provided in our \"Easy\" modules like adaptnlp.EasySequenceClassifier .","title":"Overview of Training and Finetuning Capabilities"},{"location":"tutorial/embeddings.html","text":"Embeddings for NLP are the vector representations of unstructured text. Examples of applications of Word Embeddings are downstream NLP task model training and similarity search. Below, we'll walk through how we can use AdaptNLP's EasyWordEmbeddings , EasyStackedEmbeddings , and EasyDocumentEmbeddings classes. Available Language Models \u00b6 Huggingface's Transformer's model key shortcut names can be found here . The key shortcut names for their public model-sharing repository are available here as of v2.2.2 of the Transformers library. Below are the available transformers model architectures for use as an encoder: Transformer Model ALBERT DistilBERT BERT CamemBERT RoBERTa GPT GPT2 XLNet TransformerXL XLM XLMRoBERTa You can also use Flair's FlairEmbeddings who's model key shortcut names are located here You can also use AllenNLP's ELMOEmbeddings who's model key shortcut names are located here Getting Started with EasyWordEmbeddings \u00b6 With EasyWordEmbeddings , you can load in a language model and produce contextual embeddings with text input. You can look at each word's embeddings which have been contextualized by their surrounding text, meaning embedding outputs will change for the same word depending on the text as a whole. Below is an example of producing word embeddings from OpenAI's GPT2 language model. from adaptnlp import EasyWordEmbeddings example_text = \"This is Albert. My last name is Einstein. I like physics and atoms.\" # Instantiate embeddings tagger embeddings = EasyWordEmbeddings () # Get GPT2 embeddings of example text... A list of flair Sentence objects are generated sentences = embeddings . embed_text ( example_text , model_name_or_path = \"gpt2\" ) # Iterate through to access the embeddings for token in sentences [ 0 ]: print ( token . get_embedding ()) break Output tensor ([ - 0.1524 , - 0.0703 , 0.5778 , ... , - 0.3797 , - 0.3565 , 2.4139 ]) Getting Started with EasyStackedEmbeddings \u00b6 Stacked embeddings are a simple yet important concept pointed out by Flair that can help produce state-of-the-art results in downstream NLP models. It produces contextualized word embeddings like EasyWordEmbeddings , except the embeddings are the concatenation of tensors from multiple language model. Below is an example of producing stacked word embeddings from the BERT base cased and XLNet base cased language models. EasyStackedEmbeddings take in a variable number of key shortcut names to pre-trained language models. from adaptnlp import EasyStackedEmbeddings # Instantiate stacked embeddings tagger embeddings = EasyStackedEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" ) # Run the `embed_stack` method to get the stacked embeddings outlined above sentences = embeddings . embed_text ( example_text ) # Iterate through to access the embeddings for token in sentences [ 0 ]: print ( token . get_embedding ()) break Output tensor ([ 0.5918 , - 0.4142 , 1.0203 , ... , - 0.1045 , - 1.2841 , 0.0192 ]) Getting Started with EasyDocumentEmbeddings \u00b6 Document embeddings you can load in a variable number of language models, just like in stacked embeddings, and produce and embedding for an entire text. Unlike EasyWordEmbeddings and EasyStackedEmbeddings , EasyDocumentEmbeddings will produce one contextualized embedding for a sequence of words using the pool or RNN method provided by Flair. If you are familiar with using Flair's RNN document embeddings, you can pass in hyperparameters through the config parameter when instantiating an EasyDocumentEmbeddings object. Below is an example of producing an embedding from the entire text using the BERT base cased and XLNet base cased language models. We also show the embeddings you get using the pool or RNN method. from adaptnlp import EasyDocumentEmbeddings # Instantiate document embedder with stacked embeddings embeddings = EasyDocumentEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" ) # Document Pool embedding...Instead of a list of flair Sentence objects, we get one Sentence object: the document text = embeddings . embed_pool ( example_text ) #get the text/document embedding text [ 0 ] . get_embedding () Output tensor ([ 0.4216 , 0.0123 , 0.3136 , ... , - 0.0683 , - 0.3761 , - 0.0974 ], grad_fn =< CatBackward > ) # Now again but with Document RNN embedding text = embeddings . embed_rnn ( example_text ) #get the text/document embedding text [ 0 ] . get_embedding () Output tensor ([ 4.0643e-02 , 4.7823e-01 , 3.5992e-01 , - 6.5744e-01 , 2.5690e-01 , - 2.2250e-02 , 6.6651e-01 , - 1.4607e-01 , 4.8427e-01 , 6.3852e-01 , 9.8436e-02 , - 1.4234e-01 , - 6.1204e-01 , 4.4708e-01 , 2.4172e-01 , 2.4852e-01 , - 1.5021e-01 , 5.1846e-01 , - 1.2435e-01 , 1.1078e-01 , 3.6920e-01 , 2.3225e-01 , - 2.2924e-01 , - 4.9226e-02 , 4.7070e-01 , - 1.3099e-01 , 7.9573e-01 , 2.7918e-01 , - 6.8034e-01 , - 5.7282e-01 , 2.8865e-01 , - 5.9626e-01 , 5.1510e-01 , 2.0294e-01 , 3.4929e-01 , - 5.5842e-02 , - 4.6091e-01 , - 3.9273e-01 , - 4.6477e-01 , 7.3891e-02 , 3.1949e-01 , - 3.3215e-01 , 1.3878e-01 , 2.8379e-01 , - 4.9557e-02 , - 4.5319e-01 , 1.1646e-02 , - 6.0409e-02 , - 5.8763e-01 , 8.0155e-01 , - 2.2879e-02 , 2.3967e-01 , 6.0385e-01 , - 4.1895e-01 , - 1.6761e-01 , 6.4883e-01 , 6.1100e-01 , - 7.7293e-01 , 1.7982e-01 , 8.7999e-02 , 4.7579e-01 , - 2.4647e-01 , 2.9902e-01 , - 4.4531e-01 , 3.4841e-01 , - 7.9070e-01 , 5.7861e-02 , - 1.3308e-01 , - 1.0392e-01 , 4.7919e-01 , - 6.1978e-01 , - 1.7192e-01 , - 4.7946e-01 , 4.5381e-02 , - 3.7442e-02 , - 6.8591e-01 , 3.5243e-01 , - 1.9135e-01 , 3.6689e-01 , - 2.1427e-01 , - 1.3946e-01 , 2.9380e-01 , 2.4939e-01 , 6.5739e-02 , 4.7131e-01 , - 8.2398e-01 , 6.1843e-02 , - 5.4207e-01 , - 4.3683e-01 , - 1.5192e-01 , - 1.5242e-02 , - 7.6256e-01 , - 4.8683e-01 , 1.7045e-01 , 1.0848e-01 , 3.5006e-01 , - 2.8152e-01 , - 7.3525e-02 , 1.7871e-01 , 4.3365e-01 , 2.8071e-01 , - 1.7845e-01 , - 4.7001e-01 , 3.0485e-01 , - 3.1472e-01 , 7.8487e-01 , - 8.2343e-01 , 2.5580e-01 , 6.2897e-02 , - 5.3286e-01 , 1.0242e-01 , - 8.8470e-02 , - 9.5680e-02 , 8.5138e-01 , - 3.2669e-03 , - 1.9355e-01 , - 1.0739e-01 , 1.0788e-01 , 4.6164e-01 , - 6.7108e-02 , 2.0659e-01 , 6.9547e-01 , 5.2934e-01 , - 4.9506e-01 , - 5.4222e-01 , 4.2463e-01 , 3.7806e-01 , - 3.4682e-01 , 2.4633e-02 , 4.5355e-01 , - 1.9444e-01 , 7.5605e-01 , - 1.6126e-01 , 7.1877e-01 , - 1.9557e-01 , - 2.2612e-01 , - 5.0139e-02 , - 1.3550e-01 , - 1.1433e-01 , - 8.1717e-01 , - 1.9096e-01 , 6.8815e-02 , 6.9301e-02 , - 2.7783e-01 , - 5.6060e-02 , 2.3175e-01 , - 4.5415e-01 , - 8.8416e-02 , 5.2196e-02 , 3.6615e-01 , - 2.9025e-01 , 1.3258e-01 , - 4.9883e-01 , 2.2678e-01 , - 4.2092e-01 , - 7.2251e-01 , - 4.0375e-01 , - 1.5807e-02 , 4.6092e-01 , 2.9596e-01 , 3.6077e-01 , 1.9079e-01 , - 2.5271e-01 , - 2.7760e-02 , 3.6855e-01 , 3.8165e-01 , 6.0619e-03 , - 7.6378e-01 , - 3.7182e-01 , - 4.4542e-02 , - 2.0117e-01 , - 1.1995e-01 , 2.3850e-01 , - 4.1636e-01 , - 4.8439e-01 , - 2.0748e-02 , 5.4735e-01 , - 7.2940e-01 , - 4.1707e-01 , 5.9896e-01 , 1.7213e-01 , - 1.3483e-01 , - 3.8994e-01 , 3.7115e-01 , - 2.4966e-01 , - 2.7104e-01 , 1.3207e-01 , 7.1423e-02 , 2.1035e-01 , 4.5386e-01 , - 3.4646e-01 , 1.3394e-01 , - 3.7041e-01 , - 4.2550e-01 , 2.6191e-02 , 6.6384e-01 , 1.2815e-01 , 2.6748e-02 , 5.0338e-01 , 4.1966e-02 , 7.0873e-02 , 4.2947e-01 , - 1.2464e-01 , - 2.1960e-02 , - 1.8431e-01 , 7.5072e-01 , - 3.0089e-02 , 3.0614e-01 , 2.7832e-01 , - 6.7883e-01 , - 4.5706e-01 , - 1.6099e-01 , 5.7140e-01 , 5.3964e-01 , 1.6853e-01 , - 1.2111e-01 , - 7.3538e-01 , 1.0851e-01 , - 1.8549e-01 , - 2.6486e-01 , - 1.3871e-02 , 2.8989e-01 , 8.7540e-02 , - 4.0214e-01 , 1.9980e-02 , - 7.1209e-02 , 4.6514e-01 , 2.3598e-01 , 6.6215e-01 , - 5.3153e-01 , - 4.3674e-02 , 9.7224e-02 , - 1.9030e-01 , - 7.5050e-01 , 4.6526e-01 , 4.3002e-01 , - 5.4262e-01 , - 3.7726e-01 , 2.8196e-01 , - 1.6574e-01 , - 7.0038e-02 , - 1.9054e-01 , 2.9857e-01 , - 1.0482e-01 , - 1.1758e-01 , 1.2275e-02 , 1.6027e-01 , - 3.4117e-02 , - 2.9249e-01 , 2.8828e-01 , - 1.1687e-01 , - 5.2637e-02 , 5.3424e-01 , 2.1326e-01 , - 1.1130e-02 , 1.1047e-01 , 4.6660e-01 , - 6.8302e-02 , - 6.2710e-01 , - 5.3588e-01 , 5.6987e-01 , 1.0222e-01 , 2.4219e-02 , - 2.5624e-01 , 8.0474e-02 , 4.1616e-01 , - 6.5643e-01 , - 6.0552e-01 , - 3.6263e-01 , - 1.0691e-01 , - 2.3464e-01 , - 3.3408e-01 , 1.3120e-01 , 9.2258e-02 , - 1.2690e-01 , - 3.9567e-01 , 2.8039e-01 , 5.4222e-02 , 1.7499e-01 , - 8.6867e-01 , - 3.6676e-01 , 3.8382e-02 , - 6.8972e-01 , - 3.3034e-01 , 2.9412e-01 , 4.1795e-01 , - 3.7838e-01 , - 1.9996e-01 , - 7.1303e-02 , - 2.2892e-01 , - 1.6649e-01 , 7.2984e-02 , - 2.6782e-01 , 8.0018e-01 , 4.2457e-01 , - 6.1137e-02 , 1.3479e-01 , 6.0753e-01 , - 4.9129e-01 , 5.4194e-01 , 5.5168e-01 , 4.2584e-01 , 7.7317e-01 , 3.9137e-01 , 2.5688e-02 , 4.7761e-01 , 1.1689e-02 , - 3.0685e-02 , - 3.9294e-01 , 2.5061e-01 , - 3.3469e-01 , - 1.2963e-01 , - 6.4249e-01 , 4.8470e-01 , 1.0723e-01 , 4.1352e-02 , - 3.2012e-01 , - 3.1172e-02 , - 8.9335e-01 , 1.5820e-01 , 2.4536e-01 , - 3.7762e-02 , - 3.0008e-01 , - 1.4802e-01 , 3.1138e-01 , - 4.0120e-01 , 5.2545e-01 , - 2.9134e-01 , 1.2147e-01 , - 7.2720e-01 , 7.3572e-01 , - 5.5248e-01 , 2.3775e-02 , - 8.5957e-02 , - 7.8563e-02 , - 2.3558e-01 , 2.9765e-01 , 2.3477e-01 , 4.7899e-01 , - 1.3915e-01 , 2.8876e-03 , 6.6184e-02 , - 1.9383e-01 , 5.8549e-01 , 2.0872e-01 , 3.7530e-01 , 3.8544e-01 , 1.5910e-01 , - 6.1431e-01 , 2.3468e-01 , 4.7251e-01 , 3.3820e-01 , 7.2941e-01 , 2.4980e-02 , - 5.1878e-01 , 3.8167e-01 , - 5.2926e-01 , - 8.3092e-02 , 4.0554e-01 , - 3.2830e-01 , - 7.9680e-01 , 5.0002e-01 , 2.5843e-01 , - 1.7918e-01 , 9.9184e-02 , 2.4426e-02 , 1.4300e-01 , - 2.1614e-01 , - 1.9736e-01 , - 1.6995e-01 , - 3.8350e-01 , - 5.9254e-01 , - 7.3108e-01 , - 9.7870e-02 , 6.9274e-01 , 2.8090e-01 , 7.2932e-02 , - 1.8584e-01 , - 2.0107e-02 , - 3.9466e-01 , - 1.3001e-01 , - 4.5177e-01 , - 4.0892e-03 , 6.3328e-01 , - 8.3370e-02 , 3.9102e-01 , 2.0180e-01 , 2.4513e-01 , 2.1440e-01 , 3.8041e-01 , - 3.4213e-01 , 8.5629e-02 , - 1.9770e-01 , - 5.4095e-02 , - 5.3485e-01 , 7.2403e-02 , - 3.0714e-01 , - 2.7611e-01 , - 3.0493e-01 , - 3.8583e-01 , - 1.3889e-01 , - 2.8492e-01 , 2.4335e-01 , 3.2545e-03 , 4.0507e-01 , - 2.8886e-01 , 1.5966e-01 , 3.5735e-01 , 4.9109e-01 , - 1.1930e-01 , - 6.4261e-02 , 3.0875e-02 , - 2.1206e-01 , 3.6731e-01 , 3.0674e-01 , - 5.4629e-01 , - 1.9124e-02 , - 3.6374e-01 , - 2.1023e-01 , 1.7612e-01 , 6.9023e-01 , - 1.0726e-01 , - 3.1508e-01 , - 5.5917e-02 , 4.9525e-01 , - 2.5035e-01 , - 4.3870e-01 , - 2.1269e-01 , 3.6930e-01 , - 3.5634e-02 , - 8.2272e-01 , 3.5745e-01 , - 2.9108e-01 , 1.8137e-01 , 3.2459e-01 , 6.1389e-01 , 2.0270e-01 , 2.9765e-01 , 2.9563e-01 , 3.0103e-01 , - 5.6877e-01 , - 2.2441e-01 , 2.3133e-01 , - 4.2049e-01 , 2.7534e-01 , - 2.6664e-01 , 1.0737e-01 , - 3.7153e-01 , - 5.1736e-01 , 2.5754e-01 , 3.4389e-01 , - 2.1162e-01 , 3.9876e-01 , 3.0114e-01 , 3.2266e-01 , - 3.0570e-01 , 1.0993e-01 , 3.4368e-01 , 5.7563e-01 , - 1.7115e-01 , 3.3226e-01 , - 4.0898e-01 , 1.5295e-01 , - 6.6033e-01 , - 3.5574e-01 , - 4.9282e-02 , - 2.7427e-01 , 4.8897e-01 , 5.2119e-01 , - 2.0027e-01 , 5.6864e-01 , 2.7602e-01 , 2.2527e-02 , - 1.9639e-01 , - 3.1784e-01 , - 2.2034e-01 , - 1.6692e-01 , - 1.4974e-01 , - 1.6638e-01 , - 9.1813e-02 , 7.2773e-01 , 1.6606e-01 , - 1.0737e-01 , 5.5271e-01 , 3.9674e-01 , 3.2050e-01 , - 1.2518e-01 , 2.5195e-01 , 3.1479e-01 , - 4.6130e-01 , 3.1082e-01 , - 3.2721e-01 , 4.8215e-01 , 5.8966e-01 , - 2.4745e-01 , 3.3863e-01 , 4.0711e-01 , - 1.2112e-01 , - 7.8878e-02 , 3.4396e-01 , 4.3243e-01 , 1.7047e-01 , 7.0417e-01 , 1.5878e-01 , 2.3164e-01 , 1.3155e-04 , 7.7327e-01 , 4.2866e-01 , - 4.5849e-01 , 2.1252e-01 , 1.0223e-01 , - 4.8963e-01 ], grad_fn =< CatBackward > )","title":"Embeddings"},{"location":"tutorial/embeddings.html#available-language-models","text":"Huggingface's Transformer's model key shortcut names can be found here . The key shortcut names for their public model-sharing repository are available here as of v2.2.2 of the Transformers library. Below are the available transformers model architectures for use as an encoder: Transformer Model ALBERT DistilBERT BERT CamemBERT RoBERTa GPT GPT2 XLNet TransformerXL XLM XLMRoBERTa You can also use Flair's FlairEmbeddings who's model key shortcut names are located here You can also use AllenNLP's ELMOEmbeddings who's model key shortcut names are located here","title":"Available Language Models"},{"location":"tutorial/embeddings.html#getting-started-with-easywordembeddings","text":"With EasyWordEmbeddings , you can load in a language model and produce contextual embeddings with text input. You can look at each word's embeddings which have been contextualized by their surrounding text, meaning embedding outputs will change for the same word depending on the text as a whole. Below is an example of producing word embeddings from OpenAI's GPT2 language model. from adaptnlp import EasyWordEmbeddings example_text = \"This is Albert. My last name is Einstein. I like physics and atoms.\" # Instantiate embeddings tagger embeddings = EasyWordEmbeddings () # Get GPT2 embeddings of example text... A list of flair Sentence objects are generated sentences = embeddings . embed_text ( example_text , model_name_or_path = \"gpt2\" ) # Iterate through to access the embeddings for token in sentences [ 0 ]: print ( token . get_embedding ()) break Output tensor ([ - 0.1524 , - 0.0703 , 0.5778 , ... , - 0.3797 , - 0.3565 , 2.4139 ])","title":"Getting Started with EasyWordEmbeddings"},{"location":"tutorial/embeddings.html#getting-started-with-easystackedembeddings","text":"Stacked embeddings are a simple yet important concept pointed out by Flair that can help produce state-of-the-art results in downstream NLP models. It produces contextualized word embeddings like EasyWordEmbeddings , except the embeddings are the concatenation of tensors from multiple language model. Below is an example of producing stacked word embeddings from the BERT base cased and XLNet base cased language models. EasyStackedEmbeddings take in a variable number of key shortcut names to pre-trained language models. from adaptnlp import EasyStackedEmbeddings # Instantiate stacked embeddings tagger embeddings = EasyStackedEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" ) # Run the `embed_stack` method to get the stacked embeddings outlined above sentences = embeddings . embed_text ( example_text ) # Iterate through to access the embeddings for token in sentences [ 0 ]: print ( token . get_embedding ()) break Output tensor ([ 0.5918 , - 0.4142 , 1.0203 , ... , - 0.1045 , - 1.2841 , 0.0192 ])","title":"Getting Started with EasyStackedEmbeddings"},{"location":"tutorial/embeddings.html#getting-started-with-easydocumentembeddings","text":"Document embeddings you can load in a variable number of language models, just like in stacked embeddings, and produce and embedding for an entire text. Unlike EasyWordEmbeddings and EasyStackedEmbeddings , EasyDocumentEmbeddings will produce one contextualized embedding for a sequence of words using the pool or RNN method provided by Flair. If you are familiar with using Flair's RNN document embeddings, you can pass in hyperparameters through the config parameter when instantiating an EasyDocumentEmbeddings object. Below is an example of producing an embedding from the entire text using the BERT base cased and XLNet base cased language models. We also show the embeddings you get using the pool or RNN method. from adaptnlp import EasyDocumentEmbeddings # Instantiate document embedder with stacked embeddings embeddings = EasyDocumentEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" ) # Document Pool embedding...Instead of a list of flair Sentence objects, we get one Sentence object: the document text = embeddings . embed_pool ( example_text ) #get the text/document embedding text [ 0 ] . get_embedding () Output tensor ([ 0.4216 , 0.0123 , 0.3136 , ... , - 0.0683 , - 0.3761 , - 0.0974 ], grad_fn =< CatBackward > ) # Now again but with Document RNN embedding text = embeddings . embed_rnn ( example_text ) #get the text/document embedding text [ 0 ] . get_embedding () Output tensor ([ 4.0643e-02 , 4.7823e-01 , 3.5992e-01 , - 6.5744e-01 , 2.5690e-01 , - 2.2250e-02 , 6.6651e-01 , - 1.4607e-01 , 4.8427e-01 , 6.3852e-01 , 9.8436e-02 , - 1.4234e-01 , - 6.1204e-01 , 4.4708e-01 , 2.4172e-01 , 2.4852e-01 , - 1.5021e-01 , 5.1846e-01 , - 1.2435e-01 , 1.1078e-01 , 3.6920e-01 , 2.3225e-01 , - 2.2924e-01 , - 4.9226e-02 , 4.7070e-01 , - 1.3099e-01 , 7.9573e-01 , 2.7918e-01 , - 6.8034e-01 , - 5.7282e-01 , 2.8865e-01 , - 5.9626e-01 , 5.1510e-01 , 2.0294e-01 , 3.4929e-01 , - 5.5842e-02 , - 4.6091e-01 , - 3.9273e-01 , - 4.6477e-01 , 7.3891e-02 , 3.1949e-01 , - 3.3215e-01 , 1.3878e-01 , 2.8379e-01 , - 4.9557e-02 , - 4.5319e-01 , 1.1646e-02 , - 6.0409e-02 , - 5.8763e-01 , 8.0155e-01 , - 2.2879e-02 , 2.3967e-01 , 6.0385e-01 , - 4.1895e-01 , - 1.6761e-01 , 6.4883e-01 , 6.1100e-01 , - 7.7293e-01 , 1.7982e-01 , 8.7999e-02 , 4.7579e-01 , - 2.4647e-01 , 2.9902e-01 , - 4.4531e-01 , 3.4841e-01 , - 7.9070e-01 , 5.7861e-02 , - 1.3308e-01 , - 1.0392e-01 , 4.7919e-01 , - 6.1978e-01 , - 1.7192e-01 , - 4.7946e-01 , 4.5381e-02 , - 3.7442e-02 , - 6.8591e-01 , 3.5243e-01 , - 1.9135e-01 , 3.6689e-01 , - 2.1427e-01 , - 1.3946e-01 , 2.9380e-01 , 2.4939e-01 , 6.5739e-02 , 4.7131e-01 , - 8.2398e-01 , 6.1843e-02 , - 5.4207e-01 , - 4.3683e-01 , - 1.5192e-01 , - 1.5242e-02 , - 7.6256e-01 , - 4.8683e-01 , 1.7045e-01 , 1.0848e-01 , 3.5006e-01 , - 2.8152e-01 , - 7.3525e-02 , 1.7871e-01 , 4.3365e-01 , 2.8071e-01 , - 1.7845e-01 , - 4.7001e-01 , 3.0485e-01 , - 3.1472e-01 , 7.8487e-01 , - 8.2343e-01 , 2.5580e-01 , 6.2897e-02 , - 5.3286e-01 , 1.0242e-01 , - 8.8470e-02 , - 9.5680e-02 , 8.5138e-01 , - 3.2669e-03 , - 1.9355e-01 , - 1.0739e-01 , 1.0788e-01 , 4.6164e-01 , - 6.7108e-02 , 2.0659e-01 , 6.9547e-01 , 5.2934e-01 , - 4.9506e-01 , - 5.4222e-01 , 4.2463e-01 , 3.7806e-01 , - 3.4682e-01 , 2.4633e-02 , 4.5355e-01 , - 1.9444e-01 , 7.5605e-01 , - 1.6126e-01 , 7.1877e-01 , - 1.9557e-01 , - 2.2612e-01 , - 5.0139e-02 , - 1.3550e-01 , - 1.1433e-01 , - 8.1717e-01 , - 1.9096e-01 , 6.8815e-02 , 6.9301e-02 , - 2.7783e-01 , - 5.6060e-02 , 2.3175e-01 , - 4.5415e-01 , - 8.8416e-02 , 5.2196e-02 , 3.6615e-01 , - 2.9025e-01 , 1.3258e-01 , - 4.9883e-01 , 2.2678e-01 , - 4.2092e-01 , - 7.2251e-01 , - 4.0375e-01 , - 1.5807e-02 , 4.6092e-01 , 2.9596e-01 , 3.6077e-01 , 1.9079e-01 , - 2.5271e-01 , - 2.7760e-02 , 3.6855e-01 , 3.8165e-01 , 6.0619e-03 , - 7.6378e-01 , - 3.7182e-01 , - 4.4542e-02 , - 2.0117e-01 , - 1.1995e-01 , 2.3850e-01 , - 4.1636e-01 , - 4.8439e-01 , - 2.0748e-02 , 5.4735e-01 , - 7.2940e-01 , - 4.1707e-01 , 5.9896e-01 , 1.7213e-01 , - 1.3483e-01 , - 3.8994e-01 , 3.7115e-01 , - 2.4966e-01 , - 2.7104e-01 , 1.3207e-01 , 7.1423e-02 , 2.1035e-01 , 4.5386e-01 , - 3.4646e-01 , 1.3394e-01 , - 3.7041e-01 , - 4.2550e-01 , 2.6191e-02 , 6.6384e-01 , 1.2815e-01 , 2.6748e-02 , 5.0338e-01 , 4.1966e-02 , 7.0873e-02 , 4.2947e-01 , - 1.2464e-01 , - 2.1960e-02 , - 1.8431e-01 , 7.5072e-01 , - 3.0089e-02 , 3.0614e-01 , 2.7832e-01 , - 6.7883e-01 , - 4.5706e-01 , - 1.6099e-01 , 5.7140e-01 , 5.3964e-01 , 1.6853e-01 , - 1.2111e-01 , - 7.3538e-01 , 1.0851e-01 , - 1.8549e-01 , - 2.6486e-01 , - 1.3871e-02 , 2.8989e-01 , 8.7540e-02 , - 4.0214e-01 , 1.9980e-02 , - 7.1209e-02 , 4.6514e-01 , 2.3598e-01 , 6.6215e-01 , - 5.3153e-01 , - 4.3674e-02 , 9.7224e-02 , - 1.9030e-01 , - 7.5050e-01 , 4.6526e-01 , 4.3002e-01 , - 5.4262e-01 , - 3.7726e-01 , 2.8196e-01 , - 1.6574e-01 , - 7.0038e-02 , - 1.9054e-01 , 2.9857e-01 , - 1.0482e-01 , - 1.1758e-01 , 1.2275e-02 , 1.6027e-01 , - 3.4117e-02 , - 2.9249e-01 , 2.8828e-01 , - 1.1687e-01 , - 5.2637e-02 , 5.3424e-01 , 2.1326e-01 , - 1.1130e-02 , 1.1047e-01 , 4.6660e-01 , - 6.8302e-02 , - 6.2710e-01 , - 5.3588e-01 , 5.6987e-01 , 1.0222e-01 , 2.4219e-02 , - 2.5624e-01 , 8.0474e-02 , 4.1616e-01 , - 6.5643e-01 , - 6.0552e-01 , - 3.6263e-01 , - 1.0691e-01 , - 2.3464e-01 , - 3.3408e-01 , 1.3120e-01 , 9.2258e-02 , - 1.2690e-01 , - 3.9567e-01 , 2.8039e-01 , 5.4222e-02 , 1.7499e-01 , - 8.6867e-01 , - 3.6676e-01 , 3.8382e-02 , - 6.8972e-01 , - 3.3034e-01 , 2.9412e-01 , 4.1795e-01 , - 3.7838e-01 , - 1.9996e-01 , - 7.1303e-02 , - 2.2892e-01 , - 1.6649e-01 , 7.2984e-02 , - 2.6782e-01 , 8.0018e-01 , 4.2457e-01 , - 6.1137e-02 , 1.3479e-01 , 6.0753e-01 , - 4.9129e-01 , 5.4194e-01 , 5.5168e-01 , 4.2584e-01 , 7.7317e-01 , 3.9137e-01 , 2.5688e-02 , 4.7761e-01 , 1.1689e-02 , - 3.0685e-02 , - 3.9294e-01 , 2.5061e-01 , - 3.3469e-01 , - 1.2963e-01 , - 6.4249e-01 , 4.8470e-01 , 1.0723e-01 , 4.1352e-02 , - 3.2012e-01 , - 3.1172e-02 , - 8.9335e-01 , 1.5820e-01 , 2.4536e-01 , - 3.7762e-02 , - 3.0008e-01 , - 1.4802e-01 , 3.1138e-01 , - 4.0120e-01 , 5.2545e-01 , - 2.9134e-01 , 1.2147e-01 , - 7.2720e-01 , 7.3572e-01 , - 5.5248e-01 , 2.3775e-02 , - 8.5957e-02 , - 7.8563e-02 , - 2.3558e-01 , 2.9765e-01 , 2.3477e-01 , 4.7899e-01 , - 1.3915e-01 , 2.8876e-03 , 6.6184e-02 , - 1.9383e-01 , 5.8549e-01 , 2.0872e-01 , 3.7530e-01 , 3.8544e-01 , 1.5910e-01 , - 6.1431e-01 , 2.3468e-01 , 4.7251e-01 , 3.3820e-01 , 7.2941e-01 , 2.4980e-02 , - 5.1878e-01 , 3.8167e-01 , - 5.2926e-01 , - 8.3092e-02 , 4.0554e-01 , - 3.2830e-01 , - 7.9680e-01 , 5.0002e-01 , 2.5843e-01 , - 1.7918e-01 , 9.9184e-02 , 2.4426e-02 , 1.4300e-01 , - 2.1614e-01 , - 1.9736e-01 , - 1.6995e-01 , - 3.8350e-01 , - 5.9254e-01 , - 7.3108e-01 , - 9.7870e-02 , 6.9274e-01 , 2.8090e-01 , 7.2932e-02 , - 1.8584e-01 , - 2.0107e-02 , - 3.9466e-01 , - 1.3001e-01 , - 4.5177e-01 , - 4.0892e-03 , 6.3328e-01 , - 8.3370e-02 , 3.9102e-01 , 2.0180e-01 , 2.4513e-01 , 2.1440e-01 , 3.8041e-01 , - 3.4213e-01 , 8.5629e-02 , - 1.9770e-01 , - 5.4095e-02 , - 5.3485e-01 , 7.2403e-02 , - 3.0714e-01 , - 2.7611e-01 , - 3.0493e-01 , - 3.8583e-01 , - 1.3889e-01 , - 2.8492e-01 , 2.4335e-01 , 3.2545e-03 , 4.0507e-01 , - 2.8886e-01 , 1.5966e-01 , 3.5735e-01 , 4.9109e-01 , - 1.1930e-01 , - 6.4261e-02 , 3.0875e-02 , - 2.1206e-01 , 3.6731e-01 , 3.0674e-01 , - 5.4629e-01 , - 1.9124e-02 , - 3.6374e-01 , - 2.1023e-01 , 1.7612e-01 , 6.9023e-01 , - 1.0726e-01 , - 3.1508e-01 , - 5.5917e-02 , 4.9525e-01 , - 2.5035e-01 , - 4.3870e-01 , - 2.1269e-01 , 3.6930e-01 , - 3.5634e-02 , - 8.2272e-01 , 3.5745e-01 , - 2.9108e-01 , 1.8137e-01 , 3.2459e-01 , 6.1389e-01 , 2.0270e-01 , 2.9765e-01 , 2.9563e-01 , 3.0103e-01 , - 5.6877e-01 , - 2.2441e-01 , 2.3133e-01 , - 4.2049e-01 , 2.7534e-01 , - 2.6664e-01 , 1.0737e-01 , - 3.7153e-01 , - 5.1736e-01 , 2.5754e-01 , 3.4389e-01 , - 2.1162e-01 , 3.9876e-01 , 3.0114e-01 , 3.2266e-01 , - 3.0570e-01 , 1.0993e-01 , 3.4368e-01 , 5.7563e-01 , - 1.7115e-01 , 3.3226e-01 , - 4.0898e-01 , 1.5295e-01 , - 6.6033e-01 , - 3.5574e-01 , - 4.9282e-02 , - 2.7427e-01 , 4.8897e-01 , 5.2119e-01 , - 2.0027e-01 , 5.6864e-01 , 2.7602e-01 , 2.2527e-02 , - 1.9639e-01 , - 3.1784e-01 , - 2.2034e-01 , - 1.6692e-01 , - 1.4974e-01 , - 1.6638e-01 , - 9.1813e-02 , 7.2773e-01 , 1.6606e-01 , - 1.0737e-01 , 5.5271e-01 , 3.9674e-01 , 3.2050e-01 , - 1.2518e-01 , 2.5195e-01 , 3.1479e-01 , - 4.6130e-01 , 3.1082e-01 , - 3.2721e-01 , 4.8215e-01 , 5.8966e-01 , - 2.4745e-01 , 3.3863e-01 , 4.0711e-01 , - 1.2112e-01 , - 7.8878e-02 , 3.4396e-01 , 4.3243e-01 , 1.7047e-01 , 7.0417e-01 , 1.5878e-01 , 2.3164e-01 , 1.3155e-04 , 7.7327e-01 , 4.2866e-01 , - 4.5849e-01 , 2.1252e-01 , 1.0223e-01 , - 4.8963e-01 ], grad_fn =< CatBackward > )","title":"Getting Started with EasyDocumentEmbeddings"},{"location":"tutorial/fine-tuning-language-model.html","text":"What is a language model? \u00b6 Language modeling is the task of generating a probability distribution over a sequence of words. The language models that we are using can assign the probabilitiy of an upcoming word(s) given a sequence of words. The GPT2 language model is a good example of a Causal Language Model which can predict words following a sequence of words. This predicted word can then be used along the given sequence of words to predict another word and so on. This is how we actually a variant of how we produce models for the NLP task of text generation. Why would you want to fine-tune a language model? \u00b6 Fine-tuning a language model comes in handy when data of a target task comes from a different distribution compared to the general-domain data that was used for pretraining a language model. When fine-tuning the language model on data from a target task, the general-domain pretrained model is able to converge quickly and adapt to the idiosyncrasies of the target data. This can be seen from the efforts of ULMFiT and Jeremy Howard's and Sebastian Ruder's approach on NLP transfer learning. With AdaptNLP's LMFineTuner , we can start to fine-tune state-of-the-art pretrained transformers architecture language models provided by Hugging Face's Transformers library. LMFineTuner is built on transformers.Trainer so additional documentation on it can be found at Hugging Face's documentation here Below are the available transformers language models for fine-tuning with LMFineTuner Transformer Model Model Type/Architecture String Key ALBERT \"albert\" DistilBERT \"distilbert\" BERT \"bert\" CamemBERT \"camembert\" RoBERTa \"roberta\" GPT \"gpt\" GPT2 \"gpt2\" You can fine-tune on any transformers language models with the above architecture in Huggingface's Transformers library. Key shortcut names are located here . The same goes for Huggingface's public model-sharing repository, which is available here as of v2.2.2 of the Transformers library. This tutorial will go over the following simple-to-use componenets of using the LMFineTuner to fine-tune pre-trained language models on your custom text data. 1. Data loading and training arguments 2. Language model training 3. Language model evaluation 1. Data loading and training arguments \u00b6 We'll first start by downloading some example raw text files. If you want to fine-tune a model on your own custom data, just provide the file paths to the training and evaluation text files that contain text from your target task. You don't require a lot of formatting with the data since a language model does not necessarily require \"labeled\" data. All you need is the text you'd like use to \"expand\" the domain of knowledge that your language model is training on. ! wget https : // s3 . amazonaws . com / research . metamind . io / wikitext / wikitext - 2 - raw - v1 . zip ! unzip wikitext - 2 - raw - v1 . zip train_file = \"./wikitext-2-raw/wiki.train.raw\" eval_file = \"./wikitext-2-raw/wiki.test.raw\" Output -- 2020 - 08 - 31 15 : 38 : 50 -- https : // s3 . amazonaws . com / research . metamind . io / wikitext / wikitext - 2 - raw - v1 . zip Resolving s3 . amazonaws . com ( s3 . amazonaws . com ) ... 52.217 . 64.78 Connecting to s3 . amazonaws . com ( s3 . amazonaws . com ) | 52.217 . 64.78 | : 443. .. connected . HTTP request sent , awaiting response ... 200 OK Length : 4721645 ( 4.5 M ) [ application / zip ] Saving to : \u2018 wikitext - 2 - raw - v1 . zip \u2019 wikitext - 2 - raw - v1 . z 100 % [ ===================> ] 4.50 M 2.92 MB / s in 1.5 s 2020 - 08 - 31 15 : 38 : 52 ( 2.92 MB / s ) - \u2018 wikitext - 2 - raw - v1 . zip \u2019 saved [ 4721645 / 4721645 ] Archive : wikitext - 2 - raw - v1 . zip creating : wikitext - 2 - raw / inflating : wikitext - 2 - raw / wiki . test . raw inflating : wikitext - 2 - raw / wiki . valid . raw inflating : wikitext - 2 - raw / wiki . train . raw Now that we have the text data we want to fine-tune our language model on, we can move on to configuring the training component. One of the first things we'll need to specify before we start training are the training arguments. Training arguments consist mainly of the hyperparameters we want to provide the model. These may include batch size, initial learning rate, number of epochs, etc. We will be using the transformers.TrainingArguments data class to store our training args. These are compatible with the transformers.Trainer as well as AdaptNLP's train methods. For more documention on the TrainingArguments class, please look here . There are a lot of arguments available, but we will pass in the important args and use default values for the rest. The training arguments below specify the output directory for you model and checkpoints. from transformers import TrainingArguments training_args = TrainingArguments ( output_dir = './models' , num_train_epochs = 1 , per_device_train_batch_size = 1 , per_device_eval_batch_size = 1 , warmup_steps = 500 , weight_decay = 0.01 , evaluate_during_training = False , logging_dir = './logs' , save_steps = 2500 , eval_steps = 100 ) 2. Language model training \u00b6 Now that we have our data and training arguments, let's instantiate the LMFineTuner and load in a pre-trained language model we would like to fine-tune. In this case, we will use the gpt2 pre-trained language model. Note: You can load in any model with the allowable architecture that we've specified above. You can even load in custom pre-trained models or models that you find in the Hugging Face repository that have already been fine-tuned and trained on NLP target tasks. from adaptnlp import LMFineTuner finetuner = LMFineTuner ( model_name_or_path = \"gpt2\" ) Now we can run the built-in train() method by passing in the training arguments. The training method will also be where you specify your data arguments which include the your train and eval datasets, the pre-trained model ID (this should have been loaded from your earlier cells, but can be loaded dynamically), text column name, label column name, and ordered label names (only required if loading in paths to CSV data file for dataset args). Notice how we pass the mlm argument as False? The mlm argument should be true if we are using a masked language model variant such as BERT architecture language models. More information can be found on Hugging Face's documentation here Please checkout AdaptNLP's package reference for more information here . finetuner . train ( training_args = training_args , train_file = eval_file , eval_file = eval_file , mlm = False , overwrite_cache = False ) Output 08 / 31 / 2020 15 : 45 : 44 - INFO - transformers . training_args - PyTorch : setting up devices 08 / 31 / 2020 15 : 45 : 44 - WARNING - adaptnlp . language_model - Process rank : - 1 , device : cuda : 0 , n_gpu : 1 , distributed training : False , 16 - bits training : False 08 / 31 / 2020 15 : 45 : 44 - INFO - adaptnlp . language_model - Training / evaluation parameters : { \"output_dir\" : \"./models\" , \"overwrite_output_dir\" : false , \"do_train\" : false , \"do_eval\" : false , \"do_predict\" : false , \"evaluate_during_training\" : false , \"per_device_train_batch_size\" : 1 , \"per_device_eval_batch_size\" : 1 , \"per_gpu_train_batch_size\" : null , \"per_gpu_eval_batch_size\" : null , \"gradient_accumulation_steps\" : 1 , \"learning_rate\" : 5e-05 , \"weight_decay\" : 0.01 , \"adam_epsilon\" : 1e-08 , \"max_grad_norm\" : 1.0 , \"num_train_epochs\" : 1 , \"max_steps\" : - 1 , \"warmup_steps\" : 500 , \"logging_dir\" : \"./logs\" , \"logging_first_step\" : false , \"logging_steps\" : 500 , \"save_steps\" : 2500 , \"save_total_limit\" : null , \"no_cuda\" : false , \"seed\" : 42 , \"fp16\" : false , \"fp16_opt_level\" : \"O1\" , \"local_rank\" : - 1 , \"tpu_num_cores\" : null , \"tpu_metrics_debug\" : false , \"debug\" : false , \"dataloader_drop_last\" : false , \"eval_steps\" : 100 , \"past_index\" : - 1 } 08 / 31 / 2020 15 : 45 : 44 - INFO - filelock - Lock 139826145788648 acquired on ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw . lock 08 / 31 / 2020 15 : 45 : 44 - INFO - transformers . data . datasets . language_modeling - Creating features from dataset file at ./ wikitext - 2 - raw 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . data . datasets . language_modeling - Saving features into cached file ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw [ took 0.004 s ] 08 / 31 / 2020 15 : 45 : 45 - INFO - filelock - Lock 139826145788648 released on ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw . lock 08 / 31 / 2020 15 : 45 : 45 - INFO - filelock - Lock 139826145788312 acquired on ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw . lock 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . data . datasets . language_modeling - Loading features from cached file ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw [ took 0.006 s ] 08 / 31 / 2020 15 : 45 : 45 - INFO - filelock - Lock 139826145788312 released on ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw . lock 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - You are instantiating a Trainer but W & B is not installed . To use wandb logging , run ` pip install wandb ; wandb login ` see https : // docs . wandb . com / huggingface . 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - ***** Running training ***** 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Num examples = 279 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Num Epochs = 1 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Instantaneous batch size per device = 1 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Total train batch size ( w . parallel , distributed & accumulation ) = 1 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Gradient Accumulation steps = 1 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Total optimization steps = 279 Epoch : 0 %| | 0 / 1 [ 00 : 00 < ? , ? it / s ] Iteration : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 279 / 279 [ 01 : 04 < 00 : 00 , 4.33 it / s ] \u001b [ A Epoch : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 / 1 [ 01 : 04 < 00 : 00 , 64.48 s / it ] 08 / 31 / 2020 15 : 46 : 49 - INFO - transformers . trainer - Training completed . Do not forget to share your model on huggingface . co / models = ) 08 / 31 / 2020 15 : 46 : 49 - INFO - transformers . trainer - Saving model checkpoint to ./ models 08 / 31 / 2020 15 : 46 : 49 - INFO - transformers . configuration_utils - Configuration saved in ./ models / config . json 08 / 31 / 2020 15 : 46 : 50 - INFO - transformers . modeling_utils - Model weights saved in ./ models / pytorch_model . bin 3. Language model evaluation \u00b6 To run evaluation on the model with your eval dataset, all you need to call is the built-in finetuner.evaluate() , since you've already loaded in your eval dataset during training. finetuner . evaluate () And now you have your very own pre-trained language model that's been fine-tuned on your personal domain data! Since we've just fine-tuned a causal language model, we can actually load this straight into an EasyTextGenerator class object and play around with our language model to evaluate it qualitatively with our own \"eyes\". All we have to do is pass in the directory that we've output our trained language model, in this case it's located in \"./models\" from adaptnlp import EasyTextGenerator text = \"China and the U.S. will begin to\" generator = EasyTextGenerator () # Generate generated_text = generator . generate ( text , model_name_or_path = \"./models\" , num_tokens_to_produce = 50 ) print ( generated_text ) Output Generating : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 / 1 [ 00 : 00 < 00 : 00 , 2.26 it / s ] [ 'China and the U.S. will begin to develop their own nuclear weapons in the coming years. \\n\\n The U.S. has been developing a range of nuclear weapons since the 1950s, but the U.S. has never used them in combat. The U.S. has been' ] You can compare this with the original pre-trained gpt2 model as well. # Generate generated_text = generator . generate ( text , model_name_or_path = \"gpt2\" , num_tokens_to_produce = 50 ) print ( generated_text ) Output Special tokens have been added in the vocabulary , make sure the associated word emebedding are fine - tuned or trained . Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized : [ 'h.0.attn.masked_bias' , 'h.1.attn.masked_bias' , 'h.2.attn.masked_bias' , 'h.3.attn.masked_bias' , 'h.4.attn.masked_bias' , 'h.5.attn.masked_bias' , 'h.6.attn.masked_bias' , 'h.7.attn.masked_bias' , 'h.8.attn.masked_bias' , 'h.9.attn.masked_bias' , 'h.10.attn.masked_bias' , 'h.11.attn.masked_bias' , 'lm_head.weight' ] You should probably TRAIN this model on a down - stream task to be able to use it for predictions and inference . Generating : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 / 1 [ 00 : 00 < 00 : 00 , 2.33 it / s ] [ 'China and the U.S. will begin to see the effects of the new sanctions on the Russian economy. \\n\\n \"The U.S. is going to be the first to see the effects of the new sanctions,\" said Michael O \\' Hanlon, a senior fellow at the Center for Strategic' ]","title":"Fine Tuning Language Model"},{"location":"tutorial/fine-tuning-language-model.html#what-is-a-language-model","text":"Language modeling is the task of generating a probability distribution over a sequence of words. The language models that we are using can assign the probabilitiy of an upcoming word(s) given a sequence of words. The GPT2 language model is a good example of a Causal Language Model which can predict words following a sequence of words. This predicted word can then be used along the given sequence of words to predict another word and so on. This is how we actually a variant of how we produce models for the NLP task of text generation.","title":"What is a language model?"},{"location":"tutorial/fine-tuning-language-model.html#why-would-you-want-to-fine-tune-a-language-model","text":"Fine-tuning a language model comes in handy when data of a target task comes from a different distribution compared to the general-domain data that was used for pretraining a language model. When fine-tuning the language model on data from a target task, the general-domain pretrained model is able to converge quickly and adapt to the idiosyncrasies of the target data. This can be seen from the efforts of ULMFiT and Jeremy Howard's and Sebastian Ruder's approach on NLP transfer learning. With AdaptNLP's LMFineTuner , we can start to fine-tune state-of-the-art pretrained transformers architecture language models provided by Hugging Face's Transformers library. LMFineTuner is built on transformers.Trainer so additional documentation on it can be found at Hugging Face's documentation here Below are the available transformers language models for fine-tuning with LMFineTuner Transformer Model Model Type/Architecture String Key ALBERT \"albert\" DistilBERT \"distilbert\" BERT \"bert\" CamemBERT \"camembert\" RoBERTa \"roberta\" GPT \"gpt\" GPT2 \"gpt2\" You can fine-tune on any transformers language models with the above architecture in Huggingface's Transformers library. Key shortcut names are located here . The same goes for Huggingface's public model-sharing repository, which is available here as of v2.2.2 of the Transformers library. This tutorial will go over the following simple-to-use componenets of using the LMFineTuner to fine-tune pre-trained language models on your custom text data. 1. Data loading and training arguments 2. Language model training 3. Language model evaluation","title":"Why would you want to fine-tune a language model?"},{"location":"tutorial/fine-tuning-language-model.html#1-data-loading-and-training-arguments","text":"We'll first start by downloading some example raw text files. If you want to fine-tune a model on your own custom data, just provide the file paths to the training and evaluation text files that contain text from your target task. You don't require a lot of formatting with the data since a language model does not necessarily require \"labeled\" data. All you need is the text you'd like use to \"expand\" the domain of knowledge that your language model is training on. ! wget https : // s3 . amazonaws . com / research . metamind . io / wikitext / wikitext - 2 - raw - v1 . zip ! unzip wikitext - 2 - raw - v1 . zip train_file = \"./wikitext-2-raw/wiki.train.raw\" eval_file = \"./wikitext-2-raw/wiki.test.raw\" Output -- 2020 - 08 - 31 15 : 38 : 50 -- https : // s3 . amazonaws . com / research . metamind . io / wikitext / wikitext - 2 - raw - v1 . zip Resolving s3 . amazonaws . com ( s3 . amazonaws . com ) ... 52.217 . 64.78 Connecting to s3 . amazonaws . com ( s3 . amazonaws . com ) | 52.217 . 64.78 | : 443. .. connected . HTTP request sent , awaiting response ... 200 OK Length : 4721645 ( 4.5 M ) [ application / zip ] Saving to : \u2018 wikitext - 2 - raw - v1 . zip \u2019 wikitext - 2 - raw - v1 . z 100 % [ ===================> ] 4.50 M 2.92 MB / s in 1.5 s 2020 - 08 - 31 15 : 38 : 52 ( 2.92 MB / s ) - \u2018 wikitext - 2 - raw - v1 . zip \u2019 saved [ 4721645 / 4721645 ] Archive : wikitext - 2 - raw - v1 . zip creating : wikitext - 2 - raw / inflating : wikitext - 2 - raw / wiki . test . raw inflating : wikitext - 2 - raw / wiki . valid . raw inflating : wikitext - 2 - raw / wiki . train . raw Now that we have the text data we want to fine-tune our language model on, we can move on to configuring the training component. One of the first things we'll need to specify before we start training are the training arguments. Training arguments consist mainly of the hyperparameters we want to provide the model. These may include batch size, initial learning rate, number of epochs, etc. We will be using the transformers.TrainingArguments data class to store our training args. These are compatible with the transformers.Trainer as well as AdaptNLP's train methods. For more documention on the TrainingArguments class, please look here . There are a lot of arguments available, but we will pass in the important args and use default values for the rest. The training arguments below specify the output directory for you model and checkpoints. from transformers import TrainingArguments training_args = TrainingArguments ( output_dir = './models' , num_train_epochs = 1 , per_device_train_batch_size = 1 , per_device_eval_batch_size = 1 , warmup_steps = 500 , weight_decay = 0.01 , evaluate_during_training = False , logging_dir = './logs' , save_steps = 2500 , eval_steps = 100 )","title":"1. Data loading and training arguments"},{"location":"tutorial/fine-tuning-language-model.html#2-language-model-training","text":"Now that we have our data and training arguments, let's instantiate the LMFineTuner and load in a pre-trained language model we would like to fine-tune. In this case, we will use the gpt2 pre-trained language model. Note: You can load in any model with the allowable architecture that we've specified above. You can even load in custom pre-trained models or models that you find in the Hugging Face repository that have already been fine-tuned and trained on NLP target tasks. from adaptnlp import LMFineTuner finetuner = LMFineTuner ( model_name_or_path = \"gpt2\" ) Now we can run the built-in train() method by passing in the training arguments. The training method will also be where you specify your data arguments which include the your train and eval datasets, the pre-trained model ID (this should have been loaded from your earlier cells, but can be loaded dynamically), text column name, label column name, and ordered label names (only required if loading in paths to CSV data file for dataset args). Notice how we pass the mlm argument as False? The mlm argument should be true if we are using a masked language model variant such as BERT architecture language models. More information can be found on Hugging Face's documentation here Please checkout AdaptNLP's package reference for more information here . finetuner . train ( training_args = training_args , train_file = eval_file , eval_file = eval_file , mlm = False , overwrite_cache = False ) Output 08 / 31 / 2020 15 : 45 : 44 - INFO - transformers . training_args - PyTorch : setting up devices 08 / 31 / 2020 15 : 45 : 44 - WARNING - adaptnlp . language_model - Process rank : - 1 , device : cuda : 0 , n_gpu : 1 , distributed training : False , 16 - bits training : False 08 / 31 / 2020 15 : 45 : 44 - INFO - adaptnlp . language_model - Training / evaluation parameters : { \"output_dir\" : \"./models\" , \"overwrite_output_dir\" : false , \"do_train\" : false , \"do_eval\" : false , \"do_predict\" : false , \"evaluate_during_training\" : false , \"per_device_train_batch_size\" : 1 , \"per_device_eval_batch_size\" : 1 , \"per_gpu_train_batch_size\" : null , \"per_gpu_eval_batch_size\" : null , \"gradient_accumulation_steps\" : 1 , \"learning_rate\" : 5e-05 , \"weight_decay\" : 0.01 , \"adam_epsilon\" : 1e-08 , \"max_grad_norm\" : 1.0 , \"num_train_epochs\" : 1 , \"max_steps\" : - 1 , \"warmup_steps\" : 500 , \"logging_dir\" : \"./logs\" , \"logging_first_step\" : false , \"logging_steps\" : 500 , \"save_steps\" : 2500 , \"save_total_limit\" : null , \"no_cuda\" : false , \"seed\" : 42 , \"fp16\" : false , \"fp16_opt_level\" : \"O1\" , \"local_rank\" : - 1 , \"tpu_num_cores\" : null , \"tpu_metrics_debug\" : false , \"debug\" : false , \"dataloader_drop_last\" : false , \"eval_steps\" : 100 , \"past_index\" : - 1 } 08 / 31 / 2020 15 : 45 : 44 - INFO - filelock - Lock 139826145788648 acquired on ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw . lock 08 / 31 / 2020 15 : 45 : 44 - INFO - transformers . data . datasets . language_modeling - Creating features from dataset file at ./ wikitext - 2 - raw 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . data . datasets . language_modeling - Saving features into cached file ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw [ took 0.004 s ] 08 / 31 / 2020 15 : 45 : 45 - INFO - filelock - Lock 139826145788648 released on ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw . lock 08 / 31 / 2020 15 : 45 : 45 - INFO - filelock - Lock 139826145788312 acquired on ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw . lock 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . data . datasets . language_modeling - Loading features from cached file ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw [ took 0.006 s ] 08 / 31 / 2020 15 : 45 : 45 - INFO - filelock - Lock 139826145788312 released on ./ wikitext - 2 - raw / cached_lm_GPT2TokenizerFast_1024_wiki . test . raw . lock 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - You are instantiating a Trainer but W & B is not installed . To use wandb logging , run ` pip install wandb ; wandb login ` see https : // docs . wandb . com / huggingface . 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - ***** Running training ***** 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Num examples = 279 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Num Epochs = 1 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Instantaneous batch size per device = 1 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Total train batch size ( w . parallel , distributed & accumulation ) = 1 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Gradient Accumulation steps = 1 08 / 31 / 2020 15 : 45 : 45 - INFO - transformers . trainer - Total optimization steps = 279 Epoch : 0 %| | 0 / 1 [ 00 : 00 < ? , ? it / s ] Iteration : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 279 / 279 [ 01 : 04 < 00 : 00 , 4.33 it / s ] \u001b [ A Epoch : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 / 1 [ 01 : 04 < 00 : 00 , 64.48 s / it ] 08 / 31 / 2020 15 : 46 : 49 - INFO - transformers . trainer - Training completed . Do not forget to share your model on huggingface . co / models = ) 08 / 31 / 2020 15 : 46 : 49 - INFO - transformers . trainer - Saving model checkpoint to ./ models 08 / 31 / 2020 15 : 46 : 49 - INFO - transformers . configuration_utils - Configuration saved in ./ models / config . json 08 / 31 / 2020 15 : 46 : 50 - INFO - transformers . modeling_utils - Model weights saved in ./ models / pytorch_model . bin","title":"2. Language model training"},{"location":"tutorial/fine-tuning-language-model.html#3-language-model-evaluation","text":"To run evaluation on the model with your eval dataset, all you need to call is the built-in finetuner.evaluate() , since you've already loaded in your eval dataset during training. finetuner . evaluate () And now you have your very own pre-trained language model that's been fine-tuned on your personal domain data! Since we've just fine-tuned a causal language model, we can actually load this straight into an EasyTextGenerator class object and play around with our language model to evaluate it qualitatively with our own \"eyes\". All we have to do is pass in the directory that we've output our trained language model, in this case it's located in \"./models\" from adaptnlp import EasyTextGenerator text = \"China and the U.S. will begin to\" generator = EasyTextGenerator () # Generate generated_text = generator . generate ( text , model_name_or_path = \"./models\" , num_tokens_to_produce = 50 ) print ( generated_text ) Output Generating : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 / 1 [ 00 : 00 < 00 : 00 , 2.26 it / s ] [ 'China and the U.S. will begin to develop their own nuclear weapons in the coming years. \\n\\n The U.S. has been developing a range of nuclear weapons since the 1950s, but the U.S. has never used them in combat. The U.S. has been' ] You can compare this with the original pre-trained gpt2 model as well. # Generate generated_text = generator . generate ( text , model_name_or_path = \"gpt2\" , num_tokens_to_produce = 50 ) print ( generated_text ) Output Special tokens have been added in the vocabulary , make sure the associated word emebedding are fine - tuned or trained . Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized : [ 'h.0.attn.masked_bias' , 'h.1.attn.masked_bias' , 'h.2.attn.masked_bias' , 'h.3.attn.masked_bias' , 'h.4.attn.masked_bias' , 'h.5.attn.masked_bias' , 'h.6.attn.masked_bias' , 'h.7.attn.masked_bias' , 'h.8.attn.masked_bias' , 'h.9.attn.masked_bias' , 'h.10.attn.masked_bias' , 'h.11.attn.masked_bias' , 'lm_head.weight' ] You should probably TRAIN this model on a down - stream task to be able to use it for predictions and inference . Generating : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 / 1 [ 00 : 00 < 00 : 00 , 2.33 it / s ] [ 'China and the U.S. will begin to see the effects of the new sanctions on the Russian economy. \\n\\n \"The U.S. is going to be the first to see the effects of the new sanctions,\" said Michael O \\' Hanlon, a senior fellow at the Center for Strategic' ]","title":"3. Language model evaluation"},{"location":"tutorial/question-answering.html","text":"Question Answering is the NLP task of producing a legible answer from being provided two text inputs: the context and the question in regards to the context. Examples of Question Answering models are span-based models that output a start and end index that outline the relevant \"answer\" from the context provided. With these models, we can extract answers from various questions and queries regarding any unstructured text. Below, we'll walk through how we can use AdaptNLP's EasyQuestionAnswering module to extract span-based text answers from unstructured text using state-of-the-art question answering models. Getting Started with EasyQuestionAnswering \u00b6 You can use EasyQuestionAnswering to run span-based question answering models. Providing a context and query, we get an output of top n_best_size answer predictions along with token span indices and probability scores. We'll first get started by importing the EasyQuestionAnswering class from AdaptNLP. After that, we set some example text that we'll use further down and then instantiate the QA model. from adaptnlp import EasyQuestionAnswering text = \"\"\"Amazon.com, Inc.[6] (/\u02c8\u00e6m\u0259z\u0252n/), is an American multinational technology company based in Seattle, Washington that focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. It is considered one of the Big Four technology companies along with Google, Apple, and Facebook.[7][8][9] Amazon is known for its disruption of well-established industries through technological innovation and mass scale.[10][11][12] It is the world's largest e-commerce marketplace, AI assistant provider, and cloud computing platform[13] as measured by revenue and market capitalization.[14] Amazon is the largest Internet company by revenue in the world.[15] It is the second largest private employer in the United States[16] and one of the world's most valuable companies. Amazon is the second largest technology company by revenue. Amazon was founded by Jeff Bezos on July 5, 1994, in Bellevue, Washington. The company initially started as an online marketplace for books but later expanded to sell electronics, software, video games, apparel, furniture, food, toys, and jewelry. In 2015, Amazon surpassed Walmart as the most valuable retailer in the United States by market capitalization.[17] In 2017, Amazon acquired Whole Foods Market for $13.4 billion, which vastly increased Amazon's presence as a brick-and-mortar retailer.[18] In 2018, Bezos announced that its two-day delivery service, Amazon Prime, had surpassed 100 million subscribers worldwide \"\"\" qa_model = EasyQuestionAnswering () Running inference with predict_qa(query: Union[List[str], str], context: Union[List[str], str], n_best_size: int, mini_batch_size: int, model_name_or_path: str, **kwargs) \u00b6 Now that we have the question answering model instantiated, we can run inference using the built-in predict_qa method. Note You can set model_name_or_path to any of Transformer's pretrained question answering models. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxForQuestionAnswering model. Here is an example of running inference on Transformer's DistilBERT QA model fine-tuned on SQUAD: top_prediction , all_nbest_json = qa_model . predict_qa ( query = \"What does Amazon do?\" , context = text , n_best_size = 5 , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) print ( top_prediction ) print ( all_nbest_json ) Output disruption of well - established industries [ OrderedDict ([( 'text' , 'disruption of well-established industries' ), ( 'probability' , 0.6033442567874925 ), ( 'start_logit' , 6.112505912780762 ), ( 'end_logit' , 4.161777019500732 ), ( 'start_index' , 45 ), ( 'end_index' , 48 )]), OrderedDict ([( 'text' , 'disruption' ), ( 'probability' , 0.297660848770138 ), ( 'start_logit' , 6.112505912780762 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 45 ), ( 'end_index' , 45 )]), OrderedDict ([( 'text' , 'its disruption of well-established industries' ), ( 'probability' , 0.05852587255867402 ), ( 'start_logit' , 3.779486894607544 ), ( 'end_logit' , 4.161777019500732 ), ( 'start_index' , 44 ), ( 'end_index' , 48 )]), OrderedDict ([( 'text' , 'its disruption' ), ( 'probability' , 0.02887383231852614 ), ( 'start_logit' , 3.779486894607544 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 44 ), ( 'end_index' , 45 )]), OrderedDict ([( 'text' , 'Amazon is known for its disruption' ), ( 'probability' , 0.011595189565169283 ), ( 'start_logit' , 2.8671414852142334 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 40 ), ( 'end_index' , 45 )])] We can do the same thing but now more question/context pairs! questions = [ \"What does Amazon do?\" , \"What happened July 5, 1994?\" , \"How much did Amazon acquire Whole Foods for?\" ] top_prediction , all_nbest_json = qa_model . predict_qa ( query = questions , context = [ text ] * 3 , n_best_size = 5 , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) print ( top_prediction ) print ( all_nbest_json ) Output OrderedDict ([( '0' , 'disruption of well-established industries' ), ( '1' , 'Jeff Bezos' ), ( '2' , '$13.4 billion' )]) OrderedDict ([( '0' , [ OrderedDict ([( 'text' , 'disruption of well-established industries' ), ( 'probability' , 0.6033442567874925 ), ( 'start_logit' , 6.112505912780762 ), ( 'end_logit' , 4.161777019500732 ), ( 'start_index' , 45 ), ( 'end_index' , 48 )]), OrderedDict ([( 'text' , 'disruption' ), ( 'probability' , 0.297660848770138 ), ( 'start_logit' , 6.112505912780762 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 45 ), ( 'end_index' , 45 )]), OrderedDict ([( 'text' , 'its disruption of well-established industries' ), ( 'probability' , 0.05852587255867402 ), ( 'start_logit' , 3.779486894607544 ), ( 'end_logit' , 4.161777019500732 ), ( 'start_index' , 44 ), ( 'end_index' , 48 )]), OrderedDict ([( 'text' , 'its disruption' ), ( 'probability' , 0.02887383231852614 ), ( 'start_logit' , 3.779486894607544 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 44 ), ( 'end_index' , 45 )]), OrderedDict ([( 'text' , 'Amazon is known for its disruption' ), ( 'probability' , 0.011595189565169283 ), ( 'start_logit' , 2.8671414852142334 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 40 ), ( 'end_index' , 45 )])]), ( '1' , [ OrderedDict ([( 'text' , 'Jeff Bezos' ), ( 'probability' , 0.5127511010234306 ), ( 'start_logit' , 2.8254265785217285 ), ( 'end_logit' , 0.6868380904197693 ), ( 'start_index' , 119 ), ( 'end_index' , 120 )]), OrderedDict ([( 'text' , 'Amazon was founded by Jeff Bezos' ), ( 'probability' , 0.4715363478154372 ), ( 'start_logit' , 2.7416322231292725 ), ( 'end_logit' , 0.6868380904197693 ), ( 'start_index' , 115 ), ( 'end_index' , 120 )]), OrderedDict ([( 'text' , 'founded by Jeff Bezos' ), ( 'probability' , 0.00861648484186261 ), ( 'start_logit' , - 1.2606867551803589 ), ( 'end_logit' , 0.6868380904197693 ), ( 'start_index' , 117 ), ( 'end_index' , 120 )]), OrderedDict ([( 'text' , 'Bezos' ), ( 'probability' , 0.007096066319269671 ), ( 'start_logit' , - 1.45482337474823 ), ( 'end_logit' , 0.6868380904197693 ), ( 'start_index' , 120 ), ( 'end_index' , 120 )])]), ( '2' , [ OrderedDict ([( 'text' , '$13.4 billion' ), ( 'probability' , 0.9661266044018949 ), ( 'start_logit' , 9.706584930419922 ), ( 'end_logit' , 9.610511779785156 ), ( 'start_index' , 178 ), ( 'end_index' , 179 )]), OrderedDict ([( 'text' , '13.4 billion' ), ( 'probability' , 0.021838208850347186 ), ( 'start_logit' , 5.9169511795043945 ), ( 'end_logit' , 9.610511779785156 ), ( 'start_index' , 178 ), ( 'end_index' , 179 )]), OrderedDict ([( 'text' , '$13.4 billion,' ), ( 'probability' , 0.01157609551257036 ), ( 'start_logit' , 9.706584930419922 ), ( 'end_logit' , 5.186159133911133 ), ( 'start_index' , 178 ), ( 'end_index' , 179 )]), OrderedDict ([( 'text' , '13.4 billion,' ), ( 'probability' , 0.0002616646620880307 ), ( 'start_logit' , 5.9169511795043945 ), ( 'end_logit' , 5.186159133911133 ), ( 'start_index' , 178 ), ( 'end_index' , 179 )]), OrderedDict ([( 'text' , '$' ), ( 'probability' , 0.0001974265730996087 ), ( 'start_logit' , 9.706584930419922 ), ( 'end_logit' , 1.11482834815979 ), ( 'start_index' , 178 ), ( 'end_index' , 178 )])])]) Note Check out TransformersQuestionAnswering for a more in-depth look into the additional parameters you can pass into the EasyQuestionAnswering.predict_qa method. XLNET and XLM models will be supported in the near future.","title":"Question Answering"},{"location":"tutorial/question-answering.html#getting-started-with-easyquestionanswering","text":"You can use EasyQuestionAnswering to run span-based question answering models. Providing a context and query, we get an output of top n_best_size answer predictions along with token span indices and probability scores. We'll first get started by importing the EasyQuestionAnswering class from AdaptNLP. After that, we set some example text that we'll use further down and then instantiate the QA model. from adaptnlp import EasyQuestionAnswering text = \"\"\"Amazon.com, Inc.[6] (/\u02c8\u00e6m\u0259z\u0252n/), is an American multinational technology company based in Seattle, Washington that focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. It is considered one of the Big Four technology companies along with Google, Apple, and Facebook.[7][8][9] Amazon is known for its disruption of well-established industries through technological innovation and mass scale.[10][11][12] It is the world's largest e-commerce marketplace, AI assistant provider, and cloud computing platform[13] as measured by revenue and market capitalization.[14] Amazon is the largest Internet company by revenue in the world.[15] It is the second largest private employer in the United States[16] and one of the world's most valuable companies. Amazon is the second largest technology company by revenue. Amazon was founded by Jeff Bezos on July 5, 1994, in Bellevue, Washington. The company initially started as an online marketplace for books but later expanded to sell electronics, software, video games, apparel, furniture, food, toys, and jewelry. In 2015, Amazon surpassed Walmart as the most valuable retailer in the United States by market capitalization.[17] In 2017, Amazon acquired Whole Foods Market for $13.4 billion, which vastly increased Amazon's presence as a brick-and-mortar retailer.[18] In 2018, Bezos announced that its two-day delivery service, Amazon Prime, had surpassed 100 million subscribers worldwide \"\"\" qa_model = EasyQuestionAnswering ()","title":"Getting Started with EasyQuestionAnswering"},{"location":"tutorial/question-answering.html#running-inference-with-predict_qaquery-unionliststr-str-context-unionliststr-str-n_best_size-int-mini_batch_size-int-model_name_or_path-str-kwargs","text":"Now that we have the question answering model instantiated, we can run inference using the built-in predict_qa method. Note You can set model_name_or_path to any of Transformer's pretrained question answering models. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxForQuestionAnswering model. Here is an example of running inference on Transformer's DistilBERT QA model fine-tuned on SQUAD: top_prediction , all_nbest_json = qa_model . predict_qa ( query = \"What does Amazon do?\" , context = text , n_best_size = 5 , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) print ( top_prediction ) print ( all_nbest_json ) Output disruption of well - established industries [ OrderedDict ([( 'text' , 'disruption of well-established industries' ), ( 'probability' , 0.6033442567874925 ), ( 'start_logit' , 6.112505912780762 ), ( 'end_logit' , 4.161777019500732 ), ( 'start_index' , 45 ), ( 'end_index' , 48 )]), OrderedDict ([( 'text' , 'disruption' ), ( 'probability' , 0.297660848770138 ), ( 'start_logit' , 6.112505912780762 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 45 ), ( 'end_index' , 45 )]), OrderedDict ([( 'text' , 'its disruption of well-established industries' ), ( 'probability' , 0.05852587255867402 ), ( 'start_logit' , 3.779486894607544 ), ( 'end_logit' , 4.161777019500732 ), ( 'start_index' , 44 ), ( 'end_index' , 48 )]), OrderedDict ([( 'text' , 'its disruption' ), ( 'probability' , 0.02887383231852614 ), ( 'start_logit' , 3.779486894607544 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 44 ), ( 'end_index' , 45 )]), OrderedDict ([( 'text' , 'Amazon is known for its disruption' ), ( 'probability' , 0.011595189565169283 ), ( 'start_logit' , 2.8671414852142334 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 40 ), ( 'end_index' , 45 )])] We can do the same thing but now more question/context pairs! questions = [ \"What does Amazon do?\" , \"What happened July 5, 1994?\" , \"How much did Amazon acquire Whole Foods for?\" ] top_prediction , all_nbest_json = qa_model . predict_qa ( query = questions , context = [ text ] * 3 , n_best_size = 5 , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) print ( top_prediction ) print ( all_nbest_json ) Output OrderedDict ([( '0' , 'disruption of well-established industries' ), ( '1' , 'Jeff Bezos' ), ( '2' , '$13.4 billion' )]) OrderedDict ([( '0' , [ OrderedDict ([( 'text' , 'disruption of well-established industries' ), ( 'probability' , 0.6033442567874925 ), ( 'start_logit' , 6.112505912780762 ), ( 'end_logit' , 4.161777019500732 ), ( 'start_index' , 45 ), ( 'end_index' , 48 )]), OrderedDict ([( 'text' , 'disruption' ), ( 'probability' , 0.297660848770138 ), ( 'start_logit' , 6.112505912780762 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 45 ), ( 'end_index' , 45 )]), OrderedDict ([( 'text' , 'its disruption of well-established industries' ), ( 'probability' , 0.05852587255867402 ), ( 'start_logit' , 3.779486894607544 ), ( 'end_logit' , 4.161777019500732 ), ( 'start_index' , 44 ), ( 'end_index' , 48 )]), OrderedDict ([( 'text' , 'its disruption' ), ( 'probability' , 0.02887383231852614 ), ( 'start_logit' , 3.779486894607544 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 44 ), ( 'end_index' , 45 )]), OrderedDict ([( 'text' , 'Amazon is known for its disruption' ), ( 'probability' , 0.011595189565169283 ), ( 'start_logit' , 2.8671414852142334 ), ( 'end_logit' , 3.4552438259124756 ), ( 'start_index' , 40 ), ( 'end_index' , 45 )])]), ( '1' , [ OrderedDict ([( 'text' , 'Jeff Bezos' ), ( 'probability' , 0.5127511010234306 ), ( 'start_logit' , 2.8254265785217285 ), ( 'end_logit' , 0.6868380904197693 ), ( 'start_index' , 119 ), ( 'end_index' , 120 )]), OrderedDict ([( 'text' , 'Amazon was founded by Jeff Bezos' ), ( 'probability' , 0.4715363478154372 ), ( 'start_logit' , 2.7416322231292725 ), ( 'end_logit' , 0.6868380904197693 ), ( 'start_index' , 115 ), ( 'end_index' , 120 )]), OrderedDict ([( 'text' , 'founded by Jeff Bezos' ), ( 'probability' , 0.00861648484186261 ), ( 'start_logit' , - 1.2606867551803589 ), ( 'end_logit' , 0.6868380904197693 ), ( 'start_index' , 117 ), ( 'end_index' , 120 )]), OrderedDict ([( 'text' , 'Bezos' ), ( 'probability' , 0.007096066319269671 ), ( 'start_logit' , - 1.45482337474823 ), ( 'end_logit' , 0.6868380904197693 ), ( 'start_index' , 120 ), ( 'end_index' , 120 )])]), ( '2' , [ OrderedDict ([( 'text' , '$13.4 billion' ), ( 'probability' , 0.9661266044018949 ), ( 'start_logit' , 9.706584930419922 ), ( 'end_logit' , 9.610511779785156 ), ( 'start_index' , 178 ), ( 'end_index' , 179 )]), OrderedDict ([( 'text' , '13.4 billion' ), ( 'probability' , 0.021838208850347186 ), ( 'start_logit' , 5.9169511795043945 ), ( 'end_logit' , 9.610511779785156 ), ( 'start_index' , 178 ), ( 'end_index' , 179 )]), OrderedDict ([( 'text' , '$13.4 billion,' ), ( 'probability' , 0.01157609551257036 ), ( 'start_logit' , 9.706584930419922 ), ( 'end_logit' , 5.186159133911133 ), ( 'start_index' , 178 ), ( 'end_index' , 179 )]), OrderedDict ([( 'text' , '13.4 billion,' ), ( 'probability' , 0.0002616646620880307 ), ( 'start_logit' , 5.9169511795043945 ), ( 'end_logit' , 5.186159133911133 ), ( 'start_index' , 178 ), ( 'end_index' , 179 )]), OrderedDict ([( 'text' , '$' ), ( 'probability' , 0.0001974265730996087 ), ( 'start_logit' , 9.706584930419922 ), ( 'end_logit' , 1.11482834815979 ), ( 'start_index' , 178 ), ( 'end_index' , 178 )])])]) Note Check out TransformersQuestionAnswering for a more in-depth look into the additional parameters you can pass into the EasyQuestionAnswering.predict_qa method. XLNET and XLM models will be supported in the near future.","title":"Running inference with predict_qa(query: Union[List[str], str], context: Union[List[str], str], n_best_size: int, mini_batch_size: int, model_name_or_path: str, **kwargs)"},{"location":"tutorial/sequence-classification.html","text":"What is Sequence Classification? \u00b6 Sequence Classification (or Text Classification) is the NLP task of predicting a label for a sequence of words. For example, a string of That movie was terrible because the acting was bad could be tagged with a label of negative . A string of That movie was great because the acting was good could be tagged with a label of positive . A model that can predict sentiment from text is called a sentiment classifier, which is an example of a sequence classification model. Below, we'll walk through how we can use AdaptNLP's EasySequenceClassification module to easily do the following: \u00b6 Load pre-trained models and tag data using mini-batched inference Train and fine-tune a pre-trained model on your own dataset Evaluate your model 1. Load pre-trained models and tag data using mini-batched inference \u00b6 We'll first get started by importing the EasySequenceClassifier class from AdaptNLP and instantiating the EasySequenceClassifier class object. from adaptnlp import EasySequenceClassifier from pprint import pprint classifier = EasySequenceClassifier () You can dynamically load models as you run inference. Let's check out the Hugging Face's model repository for some pre-trained sequence classification models that some wonderful people have uploaded. The repository can be found here Let's tag some text with a model that NLP Town has trained called nlptown/bert-base-multilingual-uncased-sentiment . This is a multi-lingual model that predicts how many stars (1-5) a text review has given a product. More information can be found via. the Transformers model card here # Inference example_text = \"This didn't work at all\" sentences = classifier . tag_text ( text = example_text , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , mini_batch_size = 1 , ) print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output 2020 - 08 - 31 02 : 21 : 25 , 011 loading file nlptown / bert - base - multilingual - uncased - sentiment Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 / 1 [ 00 : 00 < 00 : 00 , 98.67 it / s ] Tag Score Outputs : { \"This didn't work at all\" : [ 1 star ( 0.8421 ), 2 stars ( 0.1379 ), 3 stars ( 0.018 ), 4 stars ( 0.0012 ), 5 stars ( 0.0007 )]} multiple_text = [ \"This didn't work well at all.\" , \"I really liked it.\" , \"It was really useful.\" , \"It broke after I bought it.\" ] sentences = classifier . tag_text ( text = multiple_text , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , mini_batch_size = 2 ) print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 / 2 [ 00 : 00 < 00 : 00 , 131.57 it / s ] Tag Score Outputs : { \"This didn't work well at all.\" : [ 1 star ( 0.622 ), 2 stars ( 0.3356 ), 3 stars ( 0.0403 ), 4 stars ( 0.0016 ), 5 stars ( 0.0005 )]} { 'I really liked it.' : [ 1 star ( 0.0032 ), 2 stars ( 0.0048 ), 3 stars ( 0.054 ), 4 stars ( 0.4813 ), 5 stars ( 0.4567 )]} { 'It was really useful.' : [ 1 star ( 0.006 ), 2 stars ( 0.0093 ), 3 stars ( 0.0701 ), 4 stars ( 0.4136 ), 5 stars ( 0.501 )]} { 'It broke after I bought it.' : [ 1 star ( 0.4489 ), 2 stars ( 0.3935 ), 3 stars ( 0.1416 ), 4 stars ( 0.0121 ), 5 stars ( 0.0039 )]} Note The output is going to be a probility distribution of what the text should be tagged. If you're running this on a GPU, you can specify the mini_batch_size parameter to run mini-batch inference against your data for faster run time. You can set model_name_or_path to any of Transformer's or Flair's pre-trained sequence classification models. Transformers models are again located here . You can also pass in the path of a custom trained Transformers model. Let's tag some text with another model, specifically Oliver Guhr's German sentiment model called oliverguhr/german-sentiment-bert . # Predict german_text = [ \"Das hat \u00fcberhaupt nicht gut funktioniert.\" , \"Ich mochte es wirklich.\" , \"Es war wirklich n\u00fctzlich.\" , \"Es ist kaputt gegangen, nachdem ich es gekauft habe.\" ] sentences = classifier . tag_text ( german_text , model_name_or_path = \"oliverguhr/german-sentiment-bert\" , mini_batch_size = 1 ) print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output 2020 - 08 - 31 02 : 21 : 39 , 109 loading file oliverguhr / german - sentiment - bert Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 00 < 00 : 00 , 132.76 it / s ] Tag Score Outputs : { 'Das hat \u00fcberhaupt nicht gut funktioniert.' : [ positive ( 0.0008 ), negative ( 0.9991 ), neutral ( 0.0 )]} { 'Ich mochte es wirklich.' : [ positive ( 0.7023 ), negative ( 0.2029 ), neutral ( 0.0947 )]} { 'Es war wirklich n\u00fctzlich.' : [ positive ( 0.9813 ), negative ( 0.0184 ), neutral ( 0.0002 )]} { 'Es ist kaputt gegangen, nachdem ich es gekauft habe.' : [ positive ( 0.0042 ), negative ( 0.9957 ), neutral ( 0.0001 )]} Don't forget you can still quickly run inference with the multi-lingual review sentiment model you loaded in earlier(memory permitting)! Just change the model_name_or_path param to the model you used before. # Predict german_text = [ \"Das hat \u00fcberhaupt nicht gut funktioniert.\" , \"Ich mochte es wirklich.\" , \"Es war wirklich n\u00fctzlich.\" , \"Es ist kaputt gegangen, nachdem ich es gekauft habe.\" ] sentences = classifier . tag_text ( german_text , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , mini_batch_size = 1 ) print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 00 < 00 : 00 , 107.33 it / s ] Tag Score Outputs : { 'Das hat \u00fcberhaupt nicht gut funktioniert.' : [ 1 star ( 0.7224 ), 2 stars ( 0.2326 ), 3 stars ( 0.0418 ), 4 stars ( 0.0024 ), 5 stars ( 0.0008 )]} { 'Ich mochte es wirklich.' : [ 1 star ( 0.0092 ), 2 stars ( 0.0097 ), 3 stars ( 0.0582 ), 4 stars ( 0.3038 ), 5 stars ( 0.6191 )]} { 'Es war wirklich n\u00fctzlich.' : [ 1 star ( 0.0124 ), 2 stars ( 0.0158 ), 3 stars ( 0.0853 ), 4 stars ( 0.3754 ), 5 stars ( 0.5111 )]} { 'Es ist kaputt gegangen, nachdem ich es gekauft habe.' : [ 1 star ( 0.5459 ), 2 stars ( 0.3205 ), 3 stars ( 0.12 ), 4 stars ( 0.0104 ), 5 stars ( 0.0032 )]} Let's release the german sentiment model to free up some memory for our next step...training! classifier . release_model ( model_name_or_path = \"oliverguhr/german-sentiment-bert\" ) 2. Train and fine-tune a pre-trained model on your own dataset \u00b6 Let's imagine you have your own dataset with text/label pairs you'd like to create a sequence classification model for. With the easy sequence classifier, you can take advantage of transfer learning by fine-tuning pre-trained models on your own custom datasets. Note The EasySequenceClassifier is integrated heavily with the datasets.Dataset and transformers.Trainer class objects, so please check out the datasets and transformers documentation for more information. We'll first need a \"custom\" dataset to start training our model. Our EasySequenceClassifier.train() method can run with either datasets.Dataset objects or CSV data file paths. Since the datasets library makes it so easy, we'll use the datasets.load_dataset() method to load in the IMDB Sentiment dataset. We'll show an example with a CSV later. from datasets import load_dataset train_dataset , eval_dataset = load_dataset ( 'imdb' , split = [ 'train' , 'test' ]) # Uncomment below if you want to use less data so you don't spend an hour+ on training and evaluation train_dataset , eval_dataset = load_dataset ( 'imdb' , split = [ 'train[:1%]' , 'test[:1%]' ]) pprint ( vars ( train_dataset . info )) Output { 'builder_name' : 'imdb' , 'citation' : '@InProceedings{maas-EtAl:2011:ACL-HLT2011, \\n ' ' author = {Maas, Andrew L. and Daly, Raymond E. and ' 'Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, ' 'Christopher}, \\n ' ' title = {Learning Word Vectors for Sentiment Analysis}, \\n ' ' booktitle = {Proceedings of the 49th Annual Meeting of the ' 'Association for Computational Linguistics: Human Language ' 'Technologies}, \\n ' ' month = {June} , \\n ' ' year = {2011} , \\n ' ' address = {Portland, Oregon, USA}, \\n ' ' publisher = {Association for Computational Linguistics}, \\n ' ' pages = {142--150}, \\n ' ' url = {http://www.aclweb.org/anthology/P11-1015} \\n ' '} \\n ' , 'config_name' : 'plain_text' , 'dataset_size' : 133190346 , 'description' : 'Large Movie Review Dataset. \\n ' 'This is a dataset for binary sentiment classification ' 'containing substantially more data than previous benchmark ' 'datasets. We provide a set of 25,000 highly polar movie ' 'reviews for training, and 25,000 for testing. There is ' 'additional unlabeled data for use as well.' , 'download_checksums' : { 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz' : { 'checksum' : 'c40f74a18d3b61f90feba1e17730e0d38e8b97c05fde7008942e91923d1658fe' , 'num_bytes' : 84125825 }}, 'download_size' : 84125825 , 'features' : { 'label' : ClassLabel ( num_classes = 2 , names = [ 'neg' , 'pos' ], names_file = None , id = None ), 'text' : Value ( dtype = 'string' , id = None )}, 'homepage' : 'http://ai.stanford.edu/~amaas/data/sentiment/' , 'license' : '' , 'post_processed' : PostProcessedInfo ( features = None , resources_checksums = { 'train' : {}, 'test' : {}, 'unsupervised' : {}, 'train[:10%]' : {}, 'train[:1%]' : {}, 'test[:1%]' : {}}), 'post_processing_size' : 0 , 'size_in_bytes' : 217316171 , 'splits' : { 'test' : SplitInfo ( name = 'test' , num_bytes = 32650697 , num_examples = 25000 , dataset_name = 'imdb' ), 'train' : SplitInfo ( name = 'train' , num_bytes = 33432835 , num_examples = 25000 , dataset_name = 'imdb' ), 'unsupervised' : SplitInfo ( name = 'unsupervised' , num_bytes = 67106814 , num_examples = 50000 , dataset_name = 'imdb' )}, 'supervised_keys' : None , 'version' : 1.0 . 0 } Let 's take a brief look at what the IMDB Sentiment dataset looks like. We can see that the label column has two classes of 0 and 1. You can see the name of the classes mapped to the integers with `train_dataset.features[\"names\"]`. train_dataset . set_format ( type = \"pandas\" , columns = [ \"text\" , \"label\" ]) train_dataset [:] Output .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } label text 0 1 Bromwell High is a cartoon comedy. It ran at t... 1 1 Homelessness (or Houselessness as George Carli... 2 1 Brilliant over-acting by Lesley Ann Warren. Be... 3 1 This is easily the most underrated film inn th... 4 1 This is not the typical Mel Brooks film. It wa... ... ... ... 245 1 That hilarious line is typical of what these n... 246 1 Faith and Mortality... viewed through the lens... 247 1 The unlikely duo of Zero Mostel and Harry Bela... 248 1 *some spoilers*<br /><br />I was pleasantly su... 249 1 ... and I DO mean it. If not literally (after ... 250 rows \u00d7 2 columns # We just run this to reformat back to a 'python' dataset train_dataset . set_format ( columns = [ \"text\" , \"label\" ]) Uncomment below to see training done with CSV files. The cell below will just save the datasets.Dataset objects you have in train_dataset and eval_dataset as CSVs and will train the model with the CSV file paths. Ignore to just continue to training. #train_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label\"]) #eval_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label\"]) #train_dataset[:].to_csv(\"./IMDB train.csv\", index=False) #eval_dataset[:].to_csv(\"./IMDB eval.csv\", index=False) #train_dataset = \"./IMDB train.csv\" #eval_dataset = \"./IMDB eval.csv\" One of the first things we'll need to specify before we start training are the training arguments. Training arguments consist mainly of the hyperparameters we want to provide the model. These may include batch size, initial learning rate, number of epochs, etc. We will be using the transformers.TrainingArguments data class to store our training args. These are compatible with the transformers.Trainer as well as AdaptNLP's train methods. For more documention on the TrainingArguments class, please look here . There are a lot of arguments available, but we will pass in the important args and use default values for the rest. The training arguments below specify the output directory for you model and checkpoints. from transformers import TrainingArguments training_args = TrainingArguments ( output_dir = './models' , num_train_epochs = 1 , per_device_train_batch_size = 4 , per_device_eval_batch_size = 4 , warmup_steps = 500 , weight_decay = 0.01 , evaluate_during_training = True , logging_dir = './logs' , save_steps = 100 ) Now we can run the built-in train() method by passing in the training arguments. The training method will also be where you specify your data arguments which include the your train and eval datasets, the pre-trained model ID (this should have been loaded from your earlier cells, but can be loaded dynamically), text column name, label column name, and ordered label names (only required if loading in paths to CSV data file for dataset args). Please checkout AdaptNLP's package reference for more information here . classifier . train ( training_args = training_args , train_dataset = train_dataset , eval_dataset = eval_dataset , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , text_col_nm = \"text\" , label_col_nm = \"label\" , label_names = [ \"positive\" , \"negative\" ] ) Evaluate your model \u00b6 After training, you can evaluate the model with the eval dataset you passed in for training. classifier . evaluate ( model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" ) Now you can see it's a little weird that we're still using the model_name_or_path of the pre-trained model we fine-tuned and took advantage of via. transfer learning. We can release the model we've fine-tuned, and then load it back in using the directory that we've serialized the fine-tuned model. classifier . release_model ( model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" ) sentences = classifier . tag_text ( multiple_text , model_name_or_path = \"./models\" , mini_batch_size = 1 ) print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 00 < 00 : 00 , 122.16 it / s ] Tag Score Outputs : { \"This didn't work well at all.\" : [ neg ( 0.7344 ), pos ( 0.2656 )]} { 'I really liked it.' : [ neg ( 0.2935 ), pos ( 0.7065 )]} { 'It was really useful.' : [ neg ( 0.3237 ), pos ( 0.6763 )]} { 'It broke after I bought it.' : [ neg ( 0.6209 ), pos ( 0.3791 )]} And we're done!","title":"Sequence Classification"},{"location":"tutorial/sequence-classification.html#what-is-sequence-classification","text":"Sequence Classification (or Text Classification) is the NLP task of predicting a label for a sequence of words. For example, a string of That movie was terrible because the acting was bad could be tagged with a label of negative . A string of That movie was great because the acting was good could be tagged with a label of positive . A model that can predict sentiment from text is called a sentiment classifier, which is an example of a sequence classification model.","title":"What is Sequence Classification?"},{"location":"tutorial/sequence-classification.html#below-well-walk-through-how-we-can-use-adaptnlps-easysequenceclassification-module-to-easily-do-the-following","text":"Load pre-trained models and tag data using mini-batched inference Train and fine-tune a pre-trained model on your own dataset Evaluate your model","title":"Below, we'll walk through how we can use AdaptNLP's EasySequenceClassification module to easily do the following:"},{"location":"tutorial/sequence-classification.html#1-load-pre-trained-models-and-tag-data-using-mini-batched-inference","text":"We'll first get started by importing the EasySequenceClassifier class from AdaptNLP and instantiating the EasySequenceClassifier class object. from adaptnlp import EasySequenceClassifier from pprint import pprint classifier = EasySequenceClassifier () You can dynamically load models as you run inference. Let's check out the Hugging Face's model repository for some pre-trained sequence classification models that some wonderful people have uploaded. The repository can be found here Let's tag some text with a model that NLP Town has trained called nlptown/bert-base-multilingual-uncased-sentiment . This is a multi-lingual model that predicts how many stars (1-5) a text review has given a product. More information can be found via. the Transformers model card here # Inference example_text = \"This didn't work at all\" sentences = classifier . tag_text ( text = example_text , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , mini_batch_size = 1 , ) print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output 2020 - 08 - 31 02 : 21 : 25 , 011 loading file nlptown / bert - base - multilingual - uncased - sentiment Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 / 1 [ 00 : 00 < 00 : 00 , 98.67 it / s ] Tag Score Outputs : { \"This didn't work at all\" : [ 1 star ( 0.8421 ), 2 stars ( 0.1379 ), 3 stars ( 0.018 ), 4 stars ( 0.0012 ), 5 stars ( 0.0007 )]} multiple_text = [ \"This didn't work well at all.\" , \"I really liked it.\" , \"It was really useful.\" , \"It broke after I bought it.\" ] sentences = classifier . tag_text ( text = multiple_text , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , mini_batch_size = 2 ) print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 / 2 [ 00 : 00 < 00 : 00 , 131.57 it / s ] Tag Score Outputs : { \"This didn't work well at all.\" : [ 1 star ( 0.622 ), 2 stars ( 0.3356 ), 3 stars ( 0.0403 ), 4 stars ( 0.0016 ), 5 stars ( 0.0005 )]} { 'I really liked it.' : [ 1 star ( 0.0032 ), 2 stars ( 0.0048 ), 3 stars ( 0.054 ), 4 stars ( 0.4813 ), 5 stars ( 0.4567 )]} { 'It was really useful.' : [ 1 star ( 0.006 ), 2 stars ( 0.0093 ), 3 stars ( 0.0701 ), 4 stars ( 0.4136 ), 5 stars ( 0.501 )]} { 'It broke after I bought it.' : [ 1 star ( 0.4489 ), 2 stars ( 0.3935 ), 3 stars ( 0.1416 ), 4 stars ( 0.0121 ), 5 stars ( 0.0039 )]} Note The output is going to be a probility distribution of what the text should be tagged. If you're running this on a GPU, you can specify the mini_batch_size parameter to run mini-batch inference against your data for faster run time. You can set model_name_or_path to any of Transformer's or Flair's pre-trained sequence classification models. Transformers models are again located here . You can also pass in the path of a custom trained Transformers model. Let's tag some text with another model, specifically Oliver Guhr's German sentiment model called oliverguhr/german-sentiment-bert . # Predict german_text = [ \"Das hat \u00fcberhaupt nicht gut funktioniert.\" , \"Ich mochte es wirklich.\" , \"Es war wirklich n\u00fctzlich.\" , \"Es ist kaputt gegangen, nachdem ich es gekauft habe.\" ] sentences = classifier . tag_text ( german_text , model_name_or_path = \"oliverguhr/german-sentiment-bert\" , mini_batch_size = 1 ) print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output 2020 - 08 - 31 02 : 21 : 39 , 109 loading file oliverguhr / german - sentiment - bert Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 00 < 00 : 00 , 132.76 it / s ] Tag Score Outputs : { 'Das hat \u00fcberhaupt nicht gut funktioniert.' : [ positive ( 0.0008 ), negative ( 0.9991 ), neutral ( 0.0 )]} { 'Ich mochte es wirklich.' : [ positive ( 0.7023 ), negative ( 0.2029 ), neutral ( 0.0947 )]} { 'Es war wirklich n\u00fctzlich.' : [ positive ( 0.9813 ), negative ( 0.0184 ), neutral ( 0.0002 )]} { 'Es ist kaputt gegangen, nachdem ich es gekauft habe.' : [ positive ( 0.0042 ), negative ( 0.9957 ), neutral ( 0.0001 )]} Don't forget you can still quickly run inference with the multi-lingual review sentiment model you loaded in earlier(memory permitting)! Just change the model_name_or_path param to the model you used before. # Predict german_text = [ \"Das hat \u00fcberhaupt nicht gut funktioniert.\" , \"Ich mochte es wirklich.\" , \"Es war wirklich n\u00fctzlich.\" , \"Es ist kaputt gegangen, nachdem ich es gekauft habe.\" ] sentences = classifier . tag_text ( german_text , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , mini_batch_size = 1 ) print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 00 < 00 : 00 , 107.33 it / s ] Tag Score Outputs : { 'Das hat \u00fcberhaupt nicht gut funktioniert.' : [ 1 star ( 0.7224 ), 2 stars ( 0.2326 ), 3 stars ( 0.0418 ), 4 stars ( 0.0024 ), 5 stars ( 0.0008 )]} { 'Ich mochte es wirklich.' : [ 1 star ( 0.0092 ), 2 stars ( 0.0097 ), 3 stars ( 0.0582 ), 4 stars ( 0.3038 ), 5 stars ( 0.6191 )]} { 'Es war wirklich n\u00fctzlich.' : [ 1 star ( 0.0124 ), 2 stars ( 0.0158 ), 3 stars ( 0.0853 ), 4 stars ( 0.3754 ), 5 stars ( 0.5111 )]} { 'Es ist kaputt gegangen, nachdem ich es gekauft habe.' : [ 1 star ( 0.5459 ), 2 stars ( 0.3205 ), 3 stars ( 0.12 ), 4 stars ( 0.0104 ), 5 stars ( 0.0032 )]} Let's release the german sentiment model to free up some memory for our next step...training! classifier . release_model ( model_name_or_path = \"oliverguhr/german-sentiment-bert\" )","title":"1. Load pre-trained models and tag data using mini-batched inference"},{"location":"tutorial/sequence-classification.html#2-train-and-fine-tune-a-pre-trained-model-on-your-own-dataset","text":"Let's imagine you have your own dataset with text/label pairs you'd like to create a sequence classification model for. With the easy sequence classifier, you can take advantage of transfer learning by fine-tuning pre-trained models on your own custom datasets. Note The EasySequenceClassifier is integrated heavily with the datasets.Dataset and transformers.Trainer class objects, so please check out the datasets and transformers documentation for more information. We'll first need a \"custom\" dataset to start training our model. Our EasySequenceClassifier.train() method can run with either datasets.Dataset objects or CSV data file paths. Since the datasets library makes it so easy, we'll use the datasets.load_dataset() method to load in the IMDB Sentiment dataset. We'll show an example with a CSV later. from datasets import load_dataset train_dataset , eval_dataset = load_dataset ( 'imdb' , split = [ 'train' , 'test' ]) # Uncomment below if you want to use less data so you don't spend an hour+ on training and evaluation train_dataset , eval_dataset = load_dataset ( 'imdb' , split = [ 'train[:1%]' , 'test[:1%]' ]) pprint ( vars ( train_dataset . info )) Output { 'builder_name' : 'imdb' , 'citation' : '@InProceedings{maas-EtAl:2011:ACL-HLT2011, \\n ' ' author = {Maas, Andrew L. and Daly, Raymond E. and ' 'Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, ' 'Christopher}, \\n ' ' title = {Learning Word Vectors for Sentiment Analysis}, \\n ' ' booktitle = {Proceedings of the 49th Annual Meeting of the ' 'Association for Computational Linguistics: Human Language ' 'Technologies}, \\n ' ' month = {June} , \\n ' ' year = {2011} , \\n ' ' address = {Portland, Oregon, USA}, \\n ' ' publisher = {Association for Computational Linguistics}, \\n ' ' pages = {142--150}, \\n ' ' url = {http://www.aclweb.org/anthology/P11-1015} \\n ' '} \\n ' , 'config_name' : 'plain_text' , 'dataset_size' : 133190346 , 'description' : 'Large Movie Review Dataset. \\n ' 'This is a dataset for binary sentiment classification ' 'containing substantially more data than previous benchmark ' 'datasets. We provide a set of 25,000 highly polar movie ' 'reviews for training, and 25,000 for testing. There is ' 'additional unlabeled data for use as well.' , 'download_checksums' : { 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz' : { 'checksum' : 'c40f74a18d3b61f90feba1e17730e0d38e8b97c05fde7008942e91923d1658fe' , 'num_bytes' : 84125825 }}, 'download_size' : 84125825 , 'features' : { 'label' : ClassLabel ( num_classes = 2 , names = [ 'neg' , 'pos' ], names_file = None , id = None ), 'text' : Value ( dtype = 'string' , id = None )}, 'homepage' : 'http://ai.stanford.edu/~amaas/data/sentiment/' , 'license' : '' , 'post_processed' : PostProcessedInfo ( features = None , resources_checksums = { 'train' : {}, 'test' : {}, 'unsupervised' : {}, 'train[:10%]' : {}, 'train[:1%]' : {}, 'test[:1%]' : {}}), 'post_processing_size' : 0 , 'size_in_bytes' : 217316171 , 'splits' : { 'test' : SplitInfo ( name = 'test' , num_bytes = 32650697 , num_examples = 25000 , dataset_name = 'imdb' ), 'train' : SplitInfo ( name = 'train' , num_bytes = 33432835 , num_examples = 25000 , dataset_name = 'imdb' ), 'unsupervised' : SplitInfo ( name = 'unsupervised' , num_bytes = 67106814 , num_examples = 50000 , dataset_name = 'imdb' )}, 'supervised_keys' : None , 'version' : 1.0 . 0 } Let 's take a brief look at what the IMDB Sentiment dataset looks like. We can see that the label column has two classes of 0 and 1. You can see the name of the classes mapped to the integers with `train_dataset.features[\"names\"]`. train_dataset . set_format ( type = \"pandas\" , columns = [ \"text\" , \"label\" ]) train_dataset [:] Output .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } label text 0 1 Bromwell High is a cartoon comedy. It ran at t... 1 1 Homelessness (or Houselessness as George Carli... 2 1 Brilliant over-acting by Lesley Ann Warren. Be... 3 1 This is easily the most underrated film inn th... 4 1 This is not the typical Mel Brooks film. It wa... ... ... ... 245 1 That hilarious line is typical of what these n... 246 1 Faith and Mortality... viewed through the lens... 247 1 The unlikely duo of Zero Mostel and Harry Bela... 248 1 *some spoilers*<br /><br />I was pleasantly su... 249 1 ... and I DO mean it. If not literally (after ... 250 rows \u00d7 2 columns # We just run this to reformat back to a 'python' dataset train_dataset . set_format ( columns = [ \"text\" , \"label\" ]) Uncomment below to see training done with CSV files. The cell below will just save the datasets.Dataset objects you have in train_dataset and eval_dataset as CSVs and will train the model with the CSV file paths. Ignore to just continue to training. #train_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label\"]) #eval_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label\"]) #train_dataset[:].to_csv(\"./IMDB train.csv\", index=False) #eval_dataset[:].to_csv(\"./IMDB eval.csv\", index=False) #train_dataset = \"./IMDB train.csv\" #eval_dataset = \"./IMDB eval.csv\" One of the first things we'll need to specify before we start training are the training arguments. Training arguments consist mainly of the hyperparameters we want to provide the model. These may include batch size, initial learning rate, number of epochs, etc. We will be using the transformers.TrainingArguments data class to store our training args. These are compatible with the transformers.Trainer as well as AdaptNLP's train methods. For more documention on the TrainingArguments class, please look here . There are a lot of arguments available, but we will pass in the important args and use default values for the rest. The training arguments below specify the output directory for you model and checkpoints. from transformers import TrainingArguments training_args = TrainingArguments ( output_dir = './models' , num_train_epochs = 1 , per_device_train_batch_size = 4 , per_device_eval_batch_size = 4 , warmup_steps = 500 , weight_decay = 0.01 , evaluate_during_training = True , logging_dir = './logs' , save_steps = 100 ) Now we can run the built-in train() method by passing in the training arguments. The training method will also be where you specify your data arguments which include the your train and eval datasets, the pre-trained model ID (this should have been loaded from your earlier cells, but can be loaded dynamically), text column name, label column name, and ordered label names (only required if loading in paths to CSV data file for dataset args). Please checkout AdaptNLP's package reference for more information here . classifier . train ( training_args = training_args , train_dataset = train_dataset , eval_dataset = eval_dataset , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , text_col_nm = \"text\" , label_col_nm = \"label\" , label_names = [ \"positive\" , \"negative\" ] )","title":"2. Train and fine-tune a pre-trained model on your own dataset"},{"location":"tutorial/sequence-classification.html#evaluate-your-model","text":"After training, you can evaluate the model with the eval dataset you passed in for training. classifier . evaluate ( model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" ) Now you can see it's a little weird that we're still using the model_name_or_path of the pre-trained model we fine-tuned and took advantage of via. transfer learning. We can release the model we've fine-tuned, and then load it back in using the directory that we've serialized the fine-tuned model. classifier . release_model ( model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" ) sentences = classifier . tag_text ( multiple_text , model_name_or_path = \"./models\" , mini_batch_size = 1 ) print ( \"Tag Score Outputs: \\n \" ) for sentence in sentences : pprint ({ sentence . to_original_text (): sentence . labels }) Output Predicting text : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 00 < 00 : 00 , 122.16 it / s ] Tag Score Outputs : { \"This didn't work well at all.\" : [ neg ( 0.7344 ), pos ( 0.2656 )]} { 'I really liked it.' : [ neg ( 0.2935 ), pos ( 0.7065 )]} { 'It was really useful.' : [ neg ( 0.3237 ), pos ( 0.6763 )]} { 'It broke after I bought it.' : [ neg ( 0.6209 ), pos ( 0.3791 )]} And we're done!","title":"Evaluate your model"},{"location":"tutorial/summarization.html","text":"Summarization is the NLP task of compressing one or many documents but still retain the input's original context and meaning. Below, we'll walk through how we can use AdaptNLP's EasySummarizer module to summarize large amounts of text with state-of-the-art models. Getting Started with EasySummarizer \u00b6 We'll first get started by importing the EasySummarizer class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the summarizer. from adaptnlp import EasySummarizer # Text from encyclopedia Britannica on Einstein text = [ \"\"\"Einstein\u2019s education was disrupted by his father\u2019s repeated failures at business. In 1894, after his company failed to get an important contract to electrify the city of Munich, Hermann Einstein moved to Milan to work with a relative. Einstein was left at a boardinghouse in Munich and expected to finish his education. Alone, miserable, and repelled by the looming prospect of military duty when he turned 16, Einstein ran away six months later and landed on the doorstep of his surprised parents. His parents realized the enormous problems that he faced as a school dropout and draft dodger with no employable skills. His prospects did not look promising. Fortunately, Einstein could apply directly to the Eidgen\u00f6ssische Polytechnische Schule (\u201cSwiss Federal Polytechnic School\u201d; in 1911, following expansion in 1909 to full university status, it was renamed the Eidgen\u00f6ssische Technische Hochschule, or \u201cSwiss Federal Institute of Technology\u201d) in Z\u00fcrich without the equivalent of a high school diploma if he passed its stiff entrance examinations. His marks showed that he excelled in mathematics and physics, but he failed at French, chemistry, and biology. Because of his exceptional math scores, he was allowed into the polytechnic on the condition that he first finish his formal schooling. He went to a special high school run by Jost Winteler in Aarau, Switzerland, and graduated in 1896. He also renounced his German citizenship at that time. (He was stateless until 1901, when he was granted Swiss citizenship.) He became lifelong friends with the Winteler family, with whom he had been boarding. (Winteler\u2019s daughter, Marie, was Einstein\u2019s first love; Einstein\u2019s sister, Maja, would eventually marry Winteler\u2019s son Paul; and his close friend Michele Besso would marry their eldest daughter, Anna.)\"\"\" , \"\"\"Einstein would write that two \u201cwonders\u201d deeply affected his early years. The first was his encounter with a compass at age five. He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his 'sacred little geometry book'. Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would never amount to anything.\"\"\" ] summarizer = EasySummarizer () Summarizing with summarize(text: str, model_name_or_path: str, mini_batch_size: int, num_beams:int, min_length: int, max_length: int, early_stopping: bool **kwargs) \u00b6 Now that we have the summarizer instantiated, we are ready to load in a model and compress the text with the built-in summarize() method. This method takes in parameters: text , model_name_or_path , and mini_batch_size as well as optional keyword arguments from the Transformers.PreTrainedModel.generate() method. Note You can set model_name_or_path to any of Transformers pretrained Summarization Models with Language Model heads. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxWithLMHead model. The method returns a list of Strings. Here is one example using the T5-small model: # Summarize summaries = summarizer . summarize ( text = text , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , num_beams = 4 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Summaries: \\n \" ) for s in summaries : print ( s , \" \\n \" ) Output Einstein was left at a boardinghouse in Munich and expected to finish his education . in 1894, after his company failed to get an important contract to electrify the city of Munich, Hermann Einstein moved to Milan to work with a relative . his parents realized the enormous problems he faced as a school dropout and draft dodger with no employable skills . Einstein was mystified invisible forces could deflect the needle . the second wonder came at age 12 when he discovered a book of geometry . Einstein became deeply religious at age 12 . Another example is shown below using the Bart-large trained on CNN data: # Summarize summaries = summarizer . summarize ( text = text , model_name_or_path = \"bart-large-cnn\" , mini_batch_size = 1 , num_beams = 2 , min_length = 40 , max_length = 300 , early_stopping = True ) print ( \"Summaries: \\n \" ) for s in summaries : print ( s , \" \\n \" ) Output Einstein's education was disrupted by his father's repeated failures at business. He ran away at 16 to escape the prospect of military duty. He went to a special high school run by Jost Winteler in Aarau, Switzerland. Einstein would write that two \u2018wonders\u2019 deeply affected his early years. The first was his encounter with a compass at age five. The second wonder came at age 12 when he discovered a book of geometry. Below are some examples of Hugging Face's Pre-Trained Summarization models that you can use (These do not include models hosted in Hugging Face's model repo): Model ID T5 't5-small' 't5-base' 't5-large' 't5-3B' 't5-11B' Bart 'bart-large-cnn'","title":"Summarization"},{"location":"tutorial/summarization.html#getting-started-with-easysummarizer","text":"We'll first get started by importing the EasySummarizer class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the summarizer. from adaptnlp import EasySummarizer # Text from encyclopedia Britannica on Einstein text = [ \"\"\"Einstein\u2019s education was disrupted by his father\u2019s repeated failures at business. In 1894, after his company failed to get an important contract to electrify the city of Munich, Hermann Einstein moved to Milan to work with a relative. Einstein was left at a boardinghouse in Munich and expected to finish his education. Alone, miserable, and repelled by the looming prospect of military duty when he turned 16, Einstein ran away six months later and landed on the doorstep of his surprised parents. His parents realized the enormous problems that he faced as a school dropout and draft dodger with no employable skills. His prospects did not look promising. Fortunately, Einstein could apply directly to the Eidgen\u00f6ssische Polytechnische Schule (\u201cSwiss Federal Polytechnic School\u201d; in 1911, following expansion in 1909 to full university status, it was renamed the Eidgen\u00f6ssische Technische Hochschule, or \u201cSwiss Federal Institute of Technology\u201d) in Z\u00fcrich without the equivalent of a high school diploma if he passed its stiff entrance examinations. His marks showed that he excelled in mathematics and physics, but he failed at French, chemistry, and biology. Because of his exceptional math scores, he was allowed into the polytechnic on the condition that he first finish his formal schooling. He went to a special high school run by Jost Winteler in Aarau, Switzerland, and graduated in 1896. He also renounced his German citizenship at that time. (He was stateless until 1901, when he was granted Swiss citizenship.) He became lifelong friends with the Winteler family, with whom he had been boarding. (Winteler\u2019s daughter, Marie, was Einstein\u2019s first love; Einstein\u2019s sister, Maja, would eventually marry Winteler\u2019s son Paul; and his close friend Michele Besso would marry their eldest daughter, Anna.)\"\"\" , \"\"\"Einstein would write that two \u201cwonders\u201d deeply affected his early years. The first was his encounter with a compass at age five. He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his 'sacred little geometry book'. Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would never amount to anything.\"\"\" ] summarizer = EasySummarizer ()","title":"Getting Started with EasySummarizer"},{"location":"tutorial/summarization.html#summarizing-with-summarizetext-str-model_name_or_path-str-mini_batch_size-int-num_beamsint-min_length-int-max_length-int-early_stopping-bool-kwargs","text":"Now that we have the summarizer instantiated, we are ready to load in a model and compress the text with the built-in summarize() method. This method takes in parameters: text , model_name_or_path , and mini_batch_size as well as optional keyword arguments from the Transformers.PreTrainedModel.generate() method. Note You can set model_name_or_path to any of Transformers pretrained Summarization Models with Language Model heads. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxWithLMHead model. The method returns a list of Strings. Here is one example using the T5-small model: # Summarize summaries = summarizer . summarize ( text = text , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , num_beams = 4 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Summaries: \\n \" ) for s in summaries : print ( s , \" \\n \" ) Output Einstein was left at a boardinghouse in Munich and expected to finish his education . in 1894, after his company failed to get an important contract to electrify the city of Munich, Hermann Einstein moved to Milan to work with a relative . his parents realized the enormous problems he faced as a school dropout and draft dodger with no employable skills . Einstein was mystified invisible forces could deflect the needle . the second wonder came at age 12 when he discovered a book of geometry . Einstein became deeply religious at age 12 . Another example is shown below using the Bart-large trained on CNN data: # Summarize summaries = summarizer . summarize ( text = text , model_name_or_path = \"bart-large-cnn\" , mini_batch_size = 1 , num_beams = 2 , min_length = 40 , max_length = 300 , early_stopping = True ) print ( \"Summaries: \\n \" ) for s in summaries : print ( s , \" \\n \" ) Output Einstein's education was disrupted by his father's repeated failures at business. He ran away at 16 to escape the prospect of military duty. He went to a special high school run by Jost Winteler in Aarau, Switzerland. Einstein would write that two \u2018wonders\u2019 deeply affected his early years. The first was his encounter with a compass at age five. The second wonder came at age 12 when he discovered a book of geometry. Below are some examples of Hugging Face's Pre-Trained Summarization models that you can use (These do not include models hosted in Hugging Face's model repo): Model ID T5 't5-small' 't5-base' 't5-large' 't5-3B' 't5-11B' Bart 'bart-large-cnn'","title":"Summarizing with summarize(text: str, model_name_or_path: str, mini_batch_size: int, num_beams:int, min_length: int, max_length: int, early_stopping: bool **kwargs)"},{"location":"tutorial/text-generation.html","text":"Text Generation \u00b6 Text generation is the NLP task of generating a coherent sequence of words, usually from a language model. The current leading methods, most notably OpenAI\u2019s GPT-2 and GPT-3, rely on feeding tokens (words or characters) into a pre-trained language model which then uses this seed data to construct a sequence of text. AdaptNLP provides simple methods to easily fine-tune these state-of-the-art models and generate text for any use case. Below, we'll walk through how we can use AdaptNLP's EasyTextGenerator module to generate text to complete a given String. Getting Started with TextGeneration \u00b6 We'll first get started by importing the EasyTextGenerator class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the generator. from adaptnlp import EasyTextGenerator # Text from encyclopedia Britannica on Einstein text = \"What has happened?\" generator = EasyTextGenerator () Generate with generate(text:str, model_name_or_path: str, mini_batch_size: int, num_tokens_to_produce: int **kwargs) \u00b6 Now that we have the summarizer instantiated, we are ready to load in a model and compress the text with the built-in generate() method. This method takes in parameters: text , model_name_or_path , mini_batch_size , and num_tokens_to_produce as well as optional keyword arguments from the Transformers.PreTrainedModel.generate() method. Note You can set model_name_or_path to any of Transformers pretrained Generation Models with Language Model heads. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxWithLMHead model. The method returns a list of Strings. Here is one example using the gpt2 model: # Generate generated_text = generator . generate ( \"What has happened\" , model_name_or_path = \"gpt2\" , mini_batch_size = 2 , num_tokens_to_produce = 50 ) print ( generated_text ) Output [\"What has happened to the world's most important technology?\\n\\nThe world's most important technology is the Internet. It's the most important technology in the world. It's the most important technology in the world. It's the most important technology in the world.\"]","title":"Text Generation"},{"location":"tutorial/text-generation.html#text-generation","text":"Text generation is the NLP task of generating a coherent sequence of words, usually from a language model. The current leading methods, most notably OpenAI\u2019s GPT-2 and GPT-3, rely on feeding tokens (words or characters) into a pre-trained language model which then uses this seed data to construct a sequence of text. AdaptNLP provides simple methods to easily fine-tune these state-of-the-art models and generate text for any use case. Below, we'll walk through how we can use AdaptNLP's EasyTextGenerator module to generate text to complete a given String.","title":"Text Generation"},{"location":"tutorial/text-generation.html#getting-started-with-textgeneration","text":"We'll first get started by importing the EasyTextGenerator class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the generator. from adaptnlp import EasyTextGenerator # Text from encyclopedia Britannica on Einstein text = \"What has happened?\" generator = EasyTextGenerator ()","title":"Getting Started with TextGeneration"},{"location":"tutorial/text-generation.html#generate-with-generatetextstr-model_name_or_path-str-mini_batch_size-int-num_tokens_to_produce-int-kwargs","text":"Now that we have the summarizer instantiated, we are ready to load in a model and compress the text with the built-in generate() method. This method takes in parameters: text , model_name_or_path , mini_batch_size , and num_tokens_to_produce as well as optional keyword arguments from the Transformers.PreTrainedModel.generate() method. Note You can set model_name_or_path to any of Transformers pretrained Generation Models with Language Model heads. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxWithLMHead model. The method returns a list of Strings. Here is one example using the gpt2 model: # Generate generated_text = generator . generate ( \"What has happened\" , model_name_or_path = \"gpt2\" , mini_batch_size = 2 , num_tokens_to_produce = 50 ) print ( generated_text ) Output [\"What has happened to the world's most important technology?\\n\\nThe world's most important technology is the Internet. It's the most important technology in the world. It's the most important technology in the world. It's the most important technology in the world.\"]","title":"Generate with generate(text:str, model_name_or_path: str, mini_batch_size: int, num_tokens_to_produce: int **kwargs)"},{"location":"tutorial/token-tagging.html","text":"Token tagging (or token classification) is the NLP task of assigning a label to each individual word in the provided text. Examples of token tagging models are Named Entity Recognition(NER) and Parts of Speech(POS) models. With these models, we can generate tagged entities or parts of speech from unstructured text like \"Persons\" and \"Nouns\" Below, we'll walk through how we can use AdaptNLP's EasytokenTagger module to label unstructured text with state-of-the-art token tagging models. Getting Started with EasyTokenTagger \u00b6 We'll first get started by importing the EasyTokenTagger module from AdaptNLP. After that, we set some example text and instantiate the tagger. from adaptnlp import EasyTokenTagger example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.''' tagger = EasyTokenTagger () Tagging with tag_text(text: str, model_name_or_path: str, **kwargs) \u00b6 Now that we have the tagger instantiated, we are ready to load in a token tagging model and tag the text with the built-in tag_text() method. This method takes in parameters: text and model_name_or_path . The method returns a list of Flair's Sentence objects. Note: Additional keyword arguments can be passed in as parameters for Flair's token tagging predict() method i.e. mini_batch_size , embedding_storage_mode , verbose , etc. # Tag the string sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner-ontonotes\" ) All of Flair's pretrained token taggers are available for loading through the model_name_or_path parameter, and they can be found here . A path to a custom trained Flair Token Tagger can also be passed through the model_name_or_path param. Flair's pretrained token taggers (taken from the link above): English Models \u00b6 ID Task Training Dataset Accuracy 'ner' 4-class Named Entity Recognition Conll-03 93.03 (F1) 'ner-ontonotes' 18-class Named Entity Recognition Ontonotes 89.06 (F1) 'chunk' Syntactic Chunking Conll-2000 96.47 (F1) 'pos' Part-of-Speech Tagging Ontonotes 98.6 (Accuracy) 'frame' Semantic Frame Detection Propbank 3.0 97.54 (F1) Faster Models for CPU use \u00b6 ID Task Training Dataset Accuracy 'ner-fast' 4-class Named Entity Recognition Conll-03 92.75 (F1) 'ner-ontonotes-fast' 18-class Named Entity Recognition Ontonotes 89.27 (F1) 'chunk-fast' Syntactic Chunking Conll-2000 96.22 (F1) 'pos-fast' Part-of-Speech Tagging Ontonotes 98.47 (Accuracy) 'frame-fast' Semantic Frame Detection Propbank 3.0 97.31 (F1) Multilingual Models \u00b6 ID Task Training Dataset Accuracy 'ner-multi' 4-class Named Entity Recognition Conll-03 (4 languages) 89.27 (average F1) 'ner-multi-fast' 4-class Named Entity Recognition Conll-03 (4 languages) 87.91 (average F1) 'ner-multi-fast-learn' 4-class Named Entity Recognition Conll-03 (4 languages) 88.18 (average F1) 'pos-multi' Part-of-Speech Tagging Universal Dependency Treebank (12 languages) 96.41 (average acc.) 'pos-multi-fast' Part-of-Speech Tagging Universal Dependency Treebank (12 languages) 92.88 (average acc.) German Models \u00b6 ID Task Training Dataset Accuracy Contributor 'de-ner' 4-class Named Entity Recognition Conll-03 87.94 (F1) 'de-ner-germeval' 4+4-class Named Entity Recognition Germeval 84.90 (F1) 'de-pos' Part-of-Speech Tagging UD German - HDT 98.33 (Accuracy) 'de-pos-fine-grained' Part-of-Speech Tagging German Tweets 93.06 (Accuracy) stefan-it Other Languages \u00b6 ID Task Training Dataset Accuracy Contributor 'fr-ner' Named Entity Recognition WikiNER (aij-wikiner-fr-wp3) 95.57 (F1) mhham 'nl-ner' Named Entity Recognition CoNLL 2002 89.56 (F1) stefan-it 'da-ner' Named Entity Recognition Danish NER dataset AmaliePauli 'da-pos' Named Entity Recognition Danish Dependency Treebank AmaliePauli Now that the text has been tagged, take the returned sentences and see your results: # See Results print ( \"List string outputs of tags: \\n \" ) for sen in sentences : print ( sen . to_tagged_string ()) Output Novetta <B-ORG> Solutions <E-ORG> is the best . Albert <B-PERSON> Einstein <E-PERSON> used to be employed at Novetta <B-ORG> Solutions <E-ORG> . The Wright <S-PERSON> brothers loved to visit the JBF <S-ORG> headquarters , and they would have a chat with Albert <S-PERSON> . If you want just the entities, you can run the below but you'll need to specify the label_type as \"ner\" or \"pos\" etc. (more information can be found in Flair's documentation): print ( \"List entities tagged: \\n \" ) for sen in sentences : for entity in sen . get_spans ( label_type = \"ner\" ): print ( entity ) Output Span [ 1 , 2 ]: \"Novetta Solutions\" [ \u2212 Labels : ORG ( 0.9644 )] Span [ 7 , 8 ]: \"Albert Einstein\" [ \u2212 Labels : PERSON ( 0.9969 )] Span [ 14 , 15 ]: \"Novetta Solutions\" [ \u2212 Labels : ORG ( 0.9796 )] Span [ 18 ]: \"Wright\" [ \u2212 Labels : PERSON ( 0.9995 )] Span [ 24 ]: \"JBF\" [ \u2212 Labels : ORG ( 0.9898 )] Span [ 34 ]: \"Albert\" [ \u2212 Labels : PERSON ( 0.9999 )] Here are some additional label_typess that support some of Flair's pre-trained token taggers: label_types Description 'ner' For Named Entity Recognition tagged text 'pos' For Parts of Speech tagged text 'np' For Syntactic Chunking tagged text NOTE: You can add your own label_typess when running the sequence classifier trainer in AdaptNLP. Tagging with tag_all(text: str, model_name_or_path: str, **kwargs) \u00b6 As you tag text with multiple pretrained token tagging models, your tagger will have multiple models loaded...memory permitting. You can then use the built-in tag_all() method to tag your text with all models that are currently loaded in your tagger. See an example below: from adaptnlp import EasyTokenTagger example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.''' # Load models by tagging text tagger = EasyTokenTagger () tagger . tag_text ( text = example_text , model_name_or_path = \"ner-ontonotes\" ) tagger . tag_text ( text = example_text , model_name_or_path = \"pos\" ) # Now that the \"pos\" and \"ner-ontonotes\" models are loaded, run tag_all() sentences = tagger . tag_all ( text = example_text ) Now we can see below that you get a list of Flair sentences tagged with the \"ner-ontonotes\" AND \"pos\" model: print ( \"List entities tagged: \\n \" ) for sen in sentences : for entity in sen . get_spans ( label_type = \"pos\" ): print ( entity ) Output Span [ 1 ]: \"Novetta\" [ \u2212 Labels : NNP ( 0.9998 )] Span [ 2 ]: \"Solutions\" [ \u2212 Labels : NNPS ( 0.8235 )] Span [ 3 ]: \"is\" [ \u2212 Labels : VBZ ( 1.0 )] Span [ 4 ]: \"the\" [ \u2212 Labels : DT ( 1.0 )] Span [ 5 ]: \"best\" [ \u2212 Labels : JJS ( 0.9996 )] Span [ 6 ]: \".\" [ \u2212 Labels : . ( 0.9995 )] Span [ 7 ]: \"Albert\" [ \u2212 Labels : NNP ( 1.0 )] Span [ 8 ]: \"Einstein\" [ \u2212 Labels : NNP ( 1.0 )] Span [ 9 ]: \"used\" [ \u2212 Labels : VBD ( 0.9981 )] Span [ 10 ]: \"to\" [ \u2212 Labels : TO ( 0.9999 )] Span [ 11 ]: \"be\" [ \u2212 Labels : VB ( 1.0 )] Span [ 12 ]: \"employed\" [ \u2212 Labels : VBN ( 0.9971 )] Span [ 13 ]: \"at\" [ \u2212 Labels : IN ( 1.0 )] Span [ 14 ]: \"Novetta\" [ \u2212 Labels : NNP ( 1.0 )] Span [ 15 ]: \"Solutions\" [ \u2212 Labels : NNPS ( 0.6877 )] Span [ 16 ]: \".\" [ \u2212 Labels : . ( 0.5807 )] Span [ 17 ]: \"The\" [ \u2212 Labels : DT ( 1.0 )] Span [ 18 ]: \"Wright\" [ \u2212 Labels : NNP ( 0.9999 )] Span [ 19 ]: \"brothers\" [ \u2212 Labels : NNS ( 1.0 )] Span [ 20 ]: \"loved\" [ \u2212 Labels : VBD ( 1.0 )] Span [ 21 ]: \"to\" [ \u2212 Labels : TO ( 0.9994 )] Span [ 22 ]: \"visit\" [ \u2212 Labels : VB ( 1.0 )] Span [ 23 ]: \"the\" [ \u2212 Labels : DT ( 1.0 )] Span [ 24 ]: \"JBF\" [ \u2212 Labels : NNP ( 1.0 )] Span [ 25 ]: \"headquarters\" [ \u2212 Labels : NN ( 0.9325 )] Span [ 26 ]: \",\" [ \u2212 Labels : , ( 1.0 )] Span [ 27 ]: \"and\" [ \u2212 Labels : CC ( 1.0 )] Span [ 28 ]: \"they\" [ \u2212 Labels : PRP ( 1.0 )] Span [ 29 ]: \"would\" [ \u2212 Labels : MD ( 1.0 )] Span [ 30 ]: \"have\" [ \u2212 Labels : VB ( 1.0 )] Span [ 31 ]: \"a\" [ \u2212 Labels : DT ( 1.0 )] Span [ 32 ]: \"chat\" [ \u2212 Labels : NN ( 1.0 )] Span [ 33 ]: \"with\" [ \u2212 Labels : IN ( 1.0 )] Span [ 34 ]: \"Albert\" [ \u2212 Labels : NNP ( 1.0 )] Span [ 35 ]: \".\" [ \u2212 Labels : . ( 1.0 )] print ( \"List entities tagged: \\n \" ) for sen in sentences : for entity in sen . get_spans ( label_type = \"ner\" ): print ( entity ) Output Span [ 1 , 2 ]: \"Novetta Solutions\" [ \u2212 Labels : ORG ( 0.9644 )] Span [ 7 , 8 ]: \"Albert Einstein\" [ \u2212 Labels : PERSON ( 0.9969 )] Span [ 14 , 15 ]: \"Novetta Solutions\" [ \u2212 Labels : ORG ( 0.9796 )] Span [ 18 ]: \"Wright\" [ \u2212 Labels : PERSON ( 0.9995 )] Span [ 24 ]: \"JBF\" [ \u2212 Labels : ORG ( 0.9898 )] Span [ 34 ]: \"Albert\" [ \u2212 Labels : PERSON ( 0.9999 )]","title":"Token Tagging"},{"location":"tutorial/token-tagging.html#getting-started-with-easytokentagger","text":"We'll first get started by importing the EasyTokenTagger module from AdaptNLP. After that, we set some example text and instantiate the tagger. from adaptnlp import EasyTokenTagger example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.''' tagger = EasyTokenTagger ()","title":"Getting Started with EasyTokenTagger"},{"location":"tutorial/token-tagging.html#tagging-with-tag_texttext-str-model_name_or_path-str-kwargs","text":"Now that we have the tagger instantiated, we are ready to load in a token tagging model and tag the text with the built-in tag_text() method. This method takes in parameters: text and model_name_or_path . The method returns a list of Flair's Sentence objects. Note: Additional keyword arguments can be passed in as parameters for Flair's token tagging predict() method i.e. mini_batch_size , embedding_storage_mode , verbose , etc. # Tag the string sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner-ontonotes\" ) All of Flair's pretrained token taggers are available for loading through the model_name_or_path parameter, and they can be found here . A path to a custom trained Flair Token Tagger can also be passed through the model_name_or_path param. Flair's pretrained token taggers (taken from the link above):","title":"Tagging with tag_text(text: str, model_name_or_path: str, **kwargs)"},{"location":"tutorial/token-tagging.html#english-models","text":"ID Task Training Dataset Accuracy 'ner' 4-class Named Entity Recognition Conll-03 93.03 (F1) 'ner-ontonotes' 18-class Named Entity Recognition Ontonotes 89.06 (F1) 'chunk' Syntactic Chunking Conll-2000 96.47 (F1) 'pos' Part-of-Speech Tagging Ontonotes 98.6 (Accuracy) 'frame' Semantic Frame Detection Propbank 3.0 97.54 (F1)","title":"English Models"},{"location":"tutorial/token-tagging.html#faster-models-for-cpu-use","text":"ID Task Training Dataset Accuracy 'ner-fast' 4-class Named Entity Recognition Conll-03 92.75 (F1) 'ner-ontonotes-fast' 18-class Named Entity Recognition Ontonotes 89.27 (F1) 'chunk-fast' Syntactic Chunking Conll-2000 96.22 (F1) 'pos-fast' Part-of-Speech Tagging Ontonotes 98.47 (Accuracy) 'frame-fast' Semantic Frame Detection Propbank 3.0 97.31 (F1)","title":"Faster Models for CPU use"},{"location":"tutorial/token-tagging.html#multilingual-models","text":"ID Task Training Dataset Accuracy 'ner-multi' 4-class Named Entity Recognition Conll-03 (4 languages) 89.27 (average F1) 'ner-multi-fast' 4-class Named Entity Recognition Conll-03 (4 languages) 87.91 (average F1) 'ner-multi-fast-learn' 4-class Named Entity Recognition Conll-03 (4 languages) 88.18 (average F1) 'pos-multi' Part-of-Speech Tagging Universal Dependency Treebank (12 languages) 96.41 (average acc.) 'pos-multi-fast' Part-of-Speech Tagging Universal Dependency Treebank (12 languages) 92.88 (average acc.)","title":"Multilingual Models"},{"location":"tutorial/token-tagging.html#german-models","text":"ID Task Training Dataset Accuracy Contributor 'de-ner' 4-class Named Entity Recognition Conll-03 87.94 (F1) 'de-ner-germeval' 4+4-class Named Entity Recognition Germeval 84.90 (F1) 'de-pos' Part-of-Speech Tagging UD German - HDT 98.33 (Accuracy) 'de-pos-fine-grained' Part-of-Speech Tagging German Tweets 93.06 (Accuracy) stefan-it","title":"German Models"},{"location":"tutorial/token-tagging.html#other-languages","text":"ID Task Training Dataset Accuracy Contributor 'fr-ner' Named Entity Recognition WikiNER (aij-wikiner-fr-wp3) 95.57 (F1) mhham 'nl-ner' Named Entity Recognition CoNLL 2002 89.56 (F1) stefan-it 'da-ner' Named Entity Recognition Danish NER dataset AmaliePauli 'da-pos' Named Entity Recognition Danish Dependency Treebank AmaliePauli Now that the text has been tagged, take the returned sentences and see your results: # See Results print ( \"List string outputs of tags: \\n \" ) for sen in sentences : print ( sen . to_tagged_string ()) Output Novetta <B-ORG> Solutions <E-ORG> is the best . Albert <B-PERSON> Einstein <E-PERSON> used to be employed at Novetta <B-ORG> Solutions <E-ORG> . The Wright <S-PERSON> brothers loved to visit the JBF <S-ORG> headquarters , and they would have a chat with Albert <S-PERSON> . If you want just the entities, you can run the below but you'll need to specify the label_type as \"ner\" or \"pos\" etc. (more information can be found in Flair's documentation): print ( \"List entities tagged: \\n \" ) for sen in sentences : for entity in sen . get_spans ( label_type = \"ner\" ): print ( entity ) Output Span [ 1 , 2 ]: \"Novetta Solutions\" [ \u2212 Labels : ORG ( 0.9644 )] Span [ 7 , 8 ]: \"Albert Einstein\" [ \u2212 Labels : PERSON ( 0.9969 )] Span [ 14 , 15 ]: \"Novetta Solutions\" [ \u2212 Labels : ORG ( 0.9796 )] Span [ 18 ]: \"Wright\" [ \u2212 Labels : PERSON ( 0.9995 )] Span [ 24 ]: \"JBF\" [ \u2212 Labels : ORG ( 0.9898 )] Span [ 34 ]: \"Albert\" [ \u2212 Labels : PERSON ( 0.9999 )] Here are some additional label_typess that support some of Flair's pre-trained token taggers: label_types Description 'ner' For Named Entity Recognition tagged text 'pos' For Parts of Speech tagged text 'np' For Syntactic Chunking tagged text NOTE: You can add your own label_typess when running the sequence classifier trainer in AdaptNLP.","title":"Other Languages"},{"location":"tutorial/token-tagging.html#tagging-with-tag_alltext-str-model_name_or_path-str-kwargs","text":"As you tag text with multiple pretrained token tagging models, your tagger will have multiple models loaded...memory permitting. You can then use the built-in tag_all() method to tag your text with all models that are currently loaded in your tagger. See an example below: from adaptnlp import EasyTokenTagger example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.''' # Load models by tagging text tagger = EasyTokenTagger () tagger . tag_text ( text = example_text , model_name_or_path = \"ner-ontonotes\" ) tagger . tag_text ( text = example_text , model_name_or_path = \"pos\" ) # Now that the \"pos\" and \"ner-ontonotes\" models are loaded, run tag_all() sentences = tagger . tag_all ( text = example_text ) Now we can see below that you get a list of Flair sentences tagged with the \"ner-ontonotes\" AND \"pos\" model: print ( \"List entities tagged: \\n \" ) for sen in sentences : for entity in sen . get_spans ( label_type = \"pos\" ): print ( entity ) Output Span [ 1 ]: \"Novetta\" [ \u2212 Labels : NNP ( 0.9998 )] Span [ 2 ]: \"Solutions\" [ \u2212 Labels : NNPS ( 0.8235 )] Span [ 3 ]: \"is\" [ \u2212 Labels : VBZ ( 1.0 )] Span [ 4 ]: \"the\" [ \u2212 Labels : DT ( 1.0 )] Span [ 5 ]: \"best\" [ \u2212 Labels : JJS ( 0.9996 )] Span [ 6 ]: \".\" [ \u2212 Labels : . ( 0.9995 )] Span [ 7 ]: \"Albert\" [ \u2212 Labels : NNP ( 1.0 )] Span [ 8 ]: \"Einstein\" [ \u2212 Labels : NNP ( 1.0 )] Span [ 9 ]: \"used\" [ \u2212 Labels : VBD ( 0.9981 )] Span [ 10 ]: \"to\" [ \u2212 Labels : TO ( 0.9999 )] Span [ 11 ]: \"be\" [ \u2212 Labels : VB ( 1.0 )] Span [ 12 ]: \"employed\" [ \u2212 Labels : VBN ( 0.9971 )] Span [ 13 ]: \"at\" [ \u2212 Labels : IN ( 1.0 )] Span [ 14 ]: \"Novetta\" [ \u2212 Labels : NNP ( 1.0 )] Span [ 15 ]: \"Solutions\" [ \u2212 Labels : NNPS ( 0.6877 )] Span [ 16 ]: \".\" [ \u2212 Labels : . ( 0.5807 )] Span [ 17 ]: \"The\" [ \u2212 Labels : DT ( 1.0 )] Span [ 18 ]: \"Wright\" [ \u2212 Labels : NNP ( 0.9999 )] Span [ 19 ]: \"brothers\" [ \u2212 Labels : NNS ( 1.0 )] Span [ 20 ]: \"loved\" [ \u2212 Labels : VBD ( 1.0 )] Span [ 21 ]: \"to\" [ \u2212 Labels : TO ( 0.9994 )] Span [ 22 ]: \"visit\" [ \u2212 Labels : VB ( 1.0 )] Span [ 23 ]: \"the\" [ \u2212 Labels : DT ( 1.0 )] Span [ 24 ]: \"JBF\" [ \u2212 Labels : NNP ( 1.0 )] Span [ 25 ]: \"headquarters\" [ \u2212 Labels : NN ( 0.9325 )] Span [ 26 ]: \",\" [ \u2212 Labels : , ( 1.0 )] Span [ 27 ]: \"and\" [ \u2212 Labels : CC ( 1.0 )] Span [ 28 ]: \"they\" [ \u2212 Labels : PRP ( 1.0 )] Span [ 29 ]: \"would\" [ \u2212 Labels : MD ( 1.0 )] Span [ 30 ]: \"have\" [ \u2212 Labels : VB ( 1.0 )] Span [ 31 ]: \"a\" [ \u2212 Labels : DT ( 1.0 )] Span [ 32 ]: \"chat\" [ \u2212 Labels : NN ( 1.0 )] Span [ 33 ]: \"with\" [ \u2212 Labels : IN ( 1.0 )] Span [ 34 ]: \"Albert\" [ \u2212 Labels : NNP ( 1.0 )] Span [ 35 ]: \".\" [ \u2212 Labels : . ( 1.0 )] print ( \"List entities tagged: \\n \" ) for sen in sentences : for entity in sen . get_spans ( label_type = \"ner\" ): print ( entity ) Output Span [ 1 , 2 ]: \"Novetta Solutions\" [ \u2212 Labels : ORG ( 0.9644 )] Span [ 7 , 8 ]: \"Albert Einstein\" [ \u2212 Labels : PERSON ( 0.9969 )] Span [ 14 , 15 ]: \"Novetta Solutions\" [ \u2212 Labels : ORG ( 0.9796 )] Span [ 18 ]: \"Wright\" [ \u2212 Labels : PERSON ( 0.9995 )] Span [ 24 ]: \"JBF\" [ \u2212 Labels : ORG ( 0.9898 )] Span [ 34 ]: \"Albert\" [ \u2212 Labels : PERSON ( 0.9999 )]","title":"Tagging with tag_all(text: str, model_name_or_path: str, **kwargs)"},{"location":"tutorial/training-sequence-classification.html","text":"A sequence classifier predicts a categorical label from the unstructured sequence of text that is provided as input. AdaptNLP's SequenceClassifierTrainer uses Flair's sequence classification prediction head with Transformer and/or Flair's contextualized embeddings. You can specify the encoder you want to use from any of the following pretrained transformer language models provided by Huggingface's Transformers library. The model key shortcut names are located here . The key shortcut names of their public model-sharing repository are available here as of v2.2.2 of the Transformers library. Below are the available transformers model architectures for use as an encoder: Transformer Model ALBERT DistilBERT BERT CamemBERT RoBERTa GPT GPT2 XLNet TransformerXL XLM XLMRoBERTa You can also use Flair's FlairEmbeddings who's model key shortcut names are located here You can also use AllenNLP's ELMOEmbeddings who's model key shortcut names are located here Getting Started with SequenceClassifierTrainer \u00b6 We want to start by specifying three things: 1. corpus : A Flair Corpus data model object that contains train, test, and dev datasets. This can also be a path to a directory that contains train.csv , test.csv , and dev.csv files. If a path to the files is provided, you will require a column_name_map parameter that maps the indices of the text and label column headers i.e. {0: \"text\", 1: \"label\"} the colummn with text being at index 0 of the csv 2. output_dir : A path to a directory to store trainer and model files 3. doc_embeddings : The EasyDocumentEmbeddings object that has the specified key shortcut names to pretrained language models that the trainer will use as its encoder. from adaptnlp import EasyDocumentEmbeddings , SequenceClassifierTrainer from flair.datasets import TREC_6 corpus = TREC_6 () # Or path to directory of train.csv, test.csv, dev.csv files at \"Path/to/data/directory\" OUTPUT_DIR = \"Path/to/model/output/directory\" doc_embeddings = EasyDocumentEmbeddings ( \"bert-base-cased\" , methods = [ \"rnn\" ]) # We can specify to load the pool or rnn # methods to avoid loading both. Then we want to instantiate the trainer with the following parameters sc_configs = { \"corpus\" : corpus , \"encoder\" : doc_embeddings , \"column_name_map\" : { 0 : \"text\" , 1 : \"label\" }, \"corpus_in_memory\" : True , \"predictive_head\" : \"flair\" , } sc_trainer = SequenceClassifierTrainer ( ** sc_configs ) Output 2020 - 07 - 02 14 : 25 : 28 , 470 [ b 'HUM' , b 'DESC' , b 'ENTY' , b 'NUM' , b 'LOC' , b 'ABBR' ] We can then find the optimal learning rate with the help of the cyclical learning rates method by Leslie Smith. Using this along with our novel approach in automatically extracting an optimal learning rate, we can streamline training without pausing to manually extract the optimal learning rate. The built-in find_learning_rate() will automatically reinitialize the parameteres and optimizer after running the cyclical learning rates method. sc_lr_configs = { \"output_dir\" : OUTPUT_DIR , \"start_learning_rate\" : 1e-8 , \"end_learning_rate\" : 10 , \"iterations\" : 100 , \"mini_batch_size\" : 32 , \"stop_early\" : True , \"smoothing_factor\" : 0.8 , \"plot_learning_rate\" : True , } learning_rate = sc_trainer . find_learning_rate ( ** sc_lr_configs ) Output [ 1.5135612484362082e-08 ] [ 1.8620871366628676e-08 ] [ 2.2908676527677733e-08 ] [ 2.818382931264454e-08 ] [ 3.4673685045253164e-08 ] [ 4.265795188015927e-08 ] [ 5.2480746024977265e-08 ] [ 6.456542290346554e-08 ] [ 7.943282347242815e-08 ] [ 9.772372209558107e-08 ] [ 1.2022644346174127e-07 ] [ 1.4791083881682077e-07 ] [ 1.819700858609984e-07 ] [ 2.2387211385683393e-07 ] [ 2.754228703338167e-07 ] [ 3.3884415613920264e-07 ] [ 4.168693834703353e-07 ] [ 5.128613839913649e-07 ] [ 6.309573444801935e-07 ] [ 7.762471166286916e-07 ] [ 9.54992586021436e-07 ] [ 1.1748975549395298e-06 ] [ 1.4454397707459273e-06 ] [ 1.778279410038923e-06 ] [ 2.187761623949553e-06 ] [ 2.6915348039269168e-06 ] [ 3.311311214825913e-06 ] [ 4.0738027780411255e-06 ] [ 5.011872336272722e-06 ] [ 6.165950018614822e-06 ] [ 7.585775750291839e-06 ] [ 9.332543007969913e-06 ] [ 1.1481536214968832e-05 ] [ 1.4125375446227536e-05 ] [ 1.737800828749375e-05 ] [ 2.1379620895022316e-05 ] [ 2.6302679918953824e-05 ] [ 3.2359365692962836e-05 ] [ 3.981071705534974e-05 ] [ 4.8977881936844595e-05 ] [ 6.025595860743576e-05 ] [ 7.413102413009174e-05 ] [ 9.120108393559098e-05 ] [ 0.00011220184543019637 ] [ 0.00013803842646028855 ] [ 0.00016982436524617435 ] [ 0.00020892961308540387 ] [ 0.00025703957827688637 ] [ 0.00031622776601683794 ] [ 0.0003890451449942807 ] [ 0.00047863009232263854 ] [ 0.0005888436553555893 ] [ 0.0007244359600749906 ] [ 0.0008912509381337464 ] [ 0.0010964781961431862 ] [ 0.001348962882591652 ] [ 0.0016595869074375593 ] [ 0.002041737944669528 ] [ 0.002511886431509579 ] [ 0.00309029543251359 ] [ 0.003801893963205612 ] [ 0.004677351412871982 ] [ 0.005754399373371571 ] [ 0.007079457843841382 ] [ 0.008709635899560813 ] [ 0.010715193052376074 ] [ 0.013182567385564083 ] [ 0.016218100973589285 ] [ 0.019952623149688778 ] [ 0.024547089156850287 ] [ 0.030199517204020147 ] [ 0.03715352290971724 ] [ 0.0457088189614875 ] [ 0.05623413251903491 ] [ 0.06918309709189367 ] [ 0.08511380382023769 ] [ 0.10471285480509002 ] [ 0.1288249551693135 ] [ 0.1584893192461115 ] [ 0.19498445997580477 ] [ 0.2398832919019488 ] [ 0.29512092266663836 ] [ 0.3630780547701011 ] [ 0.446683592150963 ] [ 0.5495408738576244 ] [ 0.6760829753919818 ] [ 0.8317637711026711 ] [ 1.0232929922807545 ] 2020 - 07 - 02 14 : 31 : 22 , 204 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 31 : 22 , 205 loss diverged - stopping early ! 2020 - 07 - 02 14 : 31 : 22 , 364 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 31 : 22 , 365 learning rate finder finished - plot Path / to / model / output / directory / learning_rate . tsv 2020 - 07 - 02 14 : 31 : 22 , 366 ---------------------------------------------------------------------------------------------------- Learning_rate plots are saved in Path / to / model / output / directory / learning_rate . png Recommended Learning Rate 0.016218100973589285 We can then kick off training below. sc_train_configs = { \"output_dir\" : OUTPUT_DIR , \"learning_rate\" : learning_rate , \"mini_batch_size\" : 32 , \"anneal_factor\" : 0.5 , \"patience\" : 5 , \"max_epochs\" : 150 , \"plot_weights\" : False , \"batch_growth_annealing\" : False , } sc_trainer . train ( ** sc_train_configs ) Output 2020 - 07 - 02 14 : 39 : 15 , 191 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 194 Model : \"TextClassifier( ( document_embeddings ): DocumentRNNEmbeddings ( ( embeddings ): StackedEmbeddings ( ( list_embedding_0 ): BertEmbeddings ( ( model ): BertModel ( ( embeddings ): BertEmbeddings ( ( word_embeddings ): Embedding ( 28996 , 768 , padding_idx = 0 ) ( position_embeddings ): Embedding ( 512 , 768 ) ( token_type_embeddings ): Embedding ( 2 , 768 ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( encoder ): BertEncoder ( ( layer ): ModuleList ( ( 0 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 1 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 2 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 3 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 4 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 5 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 6 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 7 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 8 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 9 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 10 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 11 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ) ) ( pooler ): BertPooler ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( activation ): Tanh () ) ) ) ) ( word_reprojection_map ): Linear ( in_features = 3072 , out_features = 256 , bias = True ) ( rnn ): GRU ( 256 , 512 , batch_first = True ) ( dropout ): Dropout ( p = 0.5 , inplace = False ) ) ( decoder ): Linear ( in_features = 512 , out_features = 6 , bias = True ) ( loss_function ): CrossEntropyLoss () ( beta ): 1.0 ( weights ): None ( weight_tensor ) None ) \" 2020 - 07 - 02 14 : 39 : 15 , 196 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 197 Corpus : \"Corpus: 4907 train + 545 dev + 500 test sentences\" 2020 - 07 - 02 14 : 39 : 15 , 198 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 199 Parameters : 2020 - 07 - 02 14 : 39 : 15 , 199 - learning_rate : \"0.016218100973589285\" 2020 - 07 - 02 14 : 39 : 15 , 200 - mini_batch_size : \"32\" 2020 - 07 - 02 14 : 39 : 15 , 201 - patience : \"5\" 2020 - 07 - 02 14 : 39 : 15 , 201 - anneal_factor : \"0.5\" 2020 - 07 - 02 14 : 39 : 15 , 202 - max_epochs : \"150\" 2020 - 07 - 02 14 : 39 : 15 , 203 - shuffle : \"True\" 2020 - 07 - 02 14 : 39 : 15 , 203 - train_with_dev : \"False\" 2020 - 07 - 02 14 : 39 : 15 , 204 - batch_growth_annealing : \"False\" 2020 - 07 - 02 14 : 39 : 15 , 205 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 206 Model training base path : \"Path/to/model/output/directory\" 2020 - 07 - 02 14 : 39 : 15 , 206 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 207 Device : cpu 2020 - 07 - 02 14 : 39 : 15 , 208 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 209 Embeddings storage mode : cpu 2020 - 07 - 02 14 : 39 : 15 , 214 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 36 , 964 epoch 1 - iter 15 / 154 - loss 2.12653748 - samples / sec : 22.31 2020 - 07 - 02 14 : 39 : 55 , 974 epoch 1 - iter 30 / 154 - loss 2.03583712 - samples / sec : 25.44 2020 - 07 - 02 14 : 40 : 14 , 865 epoch 1 - iter 45 / 154 - loss 2.02372188 - samples / sec : 25.81 2020 - 07 - 02 14 : 40 : 35 , 062 epoch 1 - iter 60 / 154 - loss 2.03083786 - samples / sec : 23.91 2020 - 07 - 02 14 : 40 : 56 , 537 epoch 1 - iter 75 / 154 - loss 2.00187496 - samples / sec : 22.48 2020 - 07 - 02 14 : 41 : 15 , 410 epoch 1 - iter 90 / 154 - loss 1.98854279 - samples / sec : 25.77 2020 - 07 - 02 14 : 41 : 34 , 838 epoch 1 - iter 105 / 154 - loss 1.97349383 - samples / sec : 24.85 2020 - 07 - 02 14 : 41 : 55 , 114 epoch 1 - iter 120 / 154 - loss 1.96310420 - samples / sec : 23.81 2020 - 07 - 02 14 : 42 : 15 , 951 epoch 1 - iter 135 / 154 - loss 1.94268769 - samples / sec : 23.17 2020 - 07 - 02 14 : 42 : 35 , 886 epoch 1 - iter 150 / 154 - loss 1.92316744 - samples / sec : 24.23 2020 - 07 - 02 14 : 42 : 39 , 888 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 42 : 39 , 890 EPOCH 1 done : loss 1.9191 - lr 0.0162181 2020 - 07 - 02 14 : 43 : 02 , 558 DEV : loss 1.538955569267273 - score 0.7994 2020 - 07 - 02 14 : 43 : 02 , 626 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 14 : 43 : 03 , 149 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 43 : 26 , 692 epoch 2 - iter 15 / 154 - loss 1.69178906 - samples / sec : 20.60 2020 - 07 - 02 14 : 43 : 46 , 382 epoch 2 - iter 30 / 154 - loss 1.70241214 - samples / sec : 24.53 2020 - 07 - 02 14 : 44 : 06 , 135 epoch 2 - iter 45 / 154 - loss 1.69157395 - samples / sec : 24.69 2020 - 07 - 02 14 : 44 : 25 , 998 epoch 2 - iter 60 / 154 - loss 1.68283709 - samples / sec : 24.31 2020 - 07 - 02 14 : 44 : 45 , 498 epoch 2 - iter 75 / 154 - loss 1.65560424 - samples / sec : 24.78 2020 - 07 - 02 14 : 45 : 07 , 466 epoch 2 - iter 90 / 154 - loss 1.64676977 - samples / sec : 21.97 2020 - 07 - 02 14 : 45 : 27 , 106 epoch 2 - iter 105 / 154 - loss 1.63899740 - samples / sec : 24.59 2020 - 07 - 02 14 : 45 : 47 , 150 epoch 2 - iter 120 / 154 - loss 1.62948714 - samples / sec : 24.08 2020 - 07 - 02 14 : 46 : 06 , 680 epoch 2 - iter 135 / 154 - loss 1.61551479 - samples / sec : 24.88 2020 - 07 - 02 14 : 46 : 25 , 444 epoch 2 - iter 150 / 154 - loss 1.59960103 - samples / sec : 25.74 2020 - 07 - 02 14 : 46 : 30 , 454 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 46 : 30 , 455 EPOCH 2 done : loss 1.5982 - lr 0.0162181 2020 - 07 - 02 14 : 46 : 53 , 208 DEV : loss 1.5088865756988525 - score 0.8012 2020 - 07 - 02 14 : 46 : 53 , 278 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 14 : 46 : 57 , 546 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 47 : 19 , 428 epoch 3 - iter 15 / 154 - loss 1.42914403 - samples / sec : 22.43 2020 - 07 - 02 14 : 47 : 39 , 206 epoch 3 - iter 30 / 154 - loss 1.39760986 - samples / sec : 24.41 2020 - 07 - 02 14 : 47 : 58 , 944 epoch 3 - iter 45 / 154 - loss 1.38527177 - samples / sec : 24.45 2020 - 07 - 02 14 : 48 : 17 , 976 epoch 3 - iter 60 / 154 - loss 1.37054651 - samples / sec : 25.58 2020 - 07 - 02 14 : 48 : 37 , 911 epoch 3 - iter 75 / 154 - loss 1.35359440 - samples / sec : 24.21 2020 - 07 - 02 14 : 48 : 56 , 977 epoch 3 - iter 90 / 154 - loss 1.34525723 - samples / sec : 25.52 2020 - 07 - 02 14 : 49 : 15 , 853 epoch 3 - iter 105 / 154 - loss 1.33801149 - samples / sec : 25.58 2020 - 07 - 02 14 : 49 : 36 , 253 epoch 3 - iter 120 / 154 - loss 1.33194426 - samples / sec : 23.66 2020 - 07 - 02 14 : 49 : 56 , 601 epoch 3 - iter 135 / 154 - loss 1.32245981 - samples / sec : 23.89 2020 - 07 - 02 14 : 50 : 16 , 942 epoch 3 - iter 150 / 154 - loss 1.31203588 - samples / sec : 23.73 2020 - 07 - 02 14 : 50 : 21 , 354 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 50 : 21 , 355 EPOCH 3 done : loss 1.3032 - lr 0.0162181 2020 - 07 - 02 14 : 50 : 43 , 683 DEV : loss 1.2037978172302246 - score 0.8544 2020 - 07 - 02 14 : 50 : 43 , 903 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 14 : 50 : 48 , 297 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 51 : 09 , 072 epoch 4 - iter 15 / 154 - loss 1.21338186 - samples / sec : 23.39 2020 - 07 - 02 14 : 51 : 28 , 841 epoch 4 - iter 30 / 154 - loss 1.22687368 - samples / sec : 24.42 2020 - 07 - 02 14 : 51 : 48 , 861 epoch 4 - iter 45 / 154 - loss 1.18060581 - samples / sec : 24.12 2020 - 07 - 02 14 : 52 : 10 , 066 epoch 4 - iter 60 / 154 - loss 1.16962456 - samples / sec : 22.76 2020 - 07 - 02 14 : 52 : 30 , 187 epoch 4 - iter 75 / 154 - loss 1.14393969 - samples / sec : 23.99 2020 - 07 - 02 14 : 52 : 50 , 295 epoch 4 - iter 90 / 154 - loss 1.13386970 - samples / sec : 24.18 2020 - 07 - 02 14 : 53 : 09 , 576 epoch 4 - iter 105 / 154 - loss 1.12137398 - samples / sec : 25.06 2020 - 07 - 02 14 : 53 : 28 , 453 epoch 4 - iter 120 / 154 - loss 1.10854916 - samples / sec : 25.60 2020 - 07 - 02 14 : 53 : 48 , 347 epoch 4 - iter 135 / 154 - loss 1.10391057 - samples / sec : 24.27 2020 - 07 - 02 14 : 54 : 08 , 560 epoch 4 - iter 150 / 154 - loss 1.09810837 - samples / sec : 24.06 2020 - 07 - 02 14 : 54 : 14 , 154 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 54 : 14 , 156 EPOCH 4 done : loss 1.0947 - lr 0.0162181 2020 - 07 - 02 14 : 54 : 36 , 541 DEV : loss 0.9538484215736389 - score 0.8979 2020 - 07 - 02 14 : 54 : 36 , 611 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 14 : 54 : 40 , 893 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 55 : 03 , 151 epoch 5 - iter 15 / 154 - loss 1.02293763 - samples / sec : 22.02 2020 - 07 - 02 14 : 55 : 24 , 535 epoch 5 - iter 30 / 154 - loss 1.01685485 - samples / sec : 22.57 2020 - 07 - 02 14 : 55 : 44 , 082 epoch 5 - iter 45 / 154 - loss 1.00741227 - samples / sec : 24.70 2020 - 07 - 02 14 : 56 : 04 , 053 epoch 5 - iter 60 / 154 - loss 0.99412741 - samples / sec : 24.35 2020 - 07 - 02 14 : 56 : 24 , 697 epoch 5 - iter 75 / 154 - loss 0.97703705 - samples / sec : 23.38 2020 - 07 - 02 14 : 56 : 44 , 511 epoch 5 - iter 90 / 154 - loss 0.95509407 - samples / sec : 24.36 2020 - 07 - 02 14 : 57 : 04 , 272 epoch 5 - iter 105 / 154 - loss 0.95036031 - samples / sec : 24.61 2020 - 07 - 02 14 : 57 : 23 , 543 epoch 5 - iter 120 / 154 - loss 0.94678519 - samples / sec : 25.06 2020 - 07 - 02 14 : 57 : 42 , 375 epoch 5 - iter 135 / 154 - loss 0.93587750 - samples / sec : 25.64 2020 - 07 - 02 14 : 58 : 01 , 328 epoch 5 - iter 150 / 154 - loss 0.93406403 - samples / sec : 25.66 2020 - 07 - 02 14 : 58 : 05 , 957 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 58 : 05 , 959 EPOCH 5 done : loss 0.9277 - lr 0.0162181 2020 - 07 - 02 14 : 58 : 28 , 230 DEV : loss 0.8651217818260193 - score 0.8972 2020 - 07 - 02 14 : 58 : 28 , 297 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 14 : 58 : 28 , 299 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 58 : 50 , 917 epoch 6 - iter 15 / 154 - loss 0.87874780 - samples / sec : 21.69 2020 - 07 - 02 14 : 59 : 10 , 424 epoch 6 - iter 30 / 154 - loss 0.84258909 - samples / sec : 24.75 2020 - 07 - 02 14 : 59 : 30 , 557 epoch 6 - iter 45 / 154 - loss 0.83004284 - samples / sec : 23.97 2020 - 07 - 02 14 : 59 : 50 , 051 epoch 6 - iter 60 / 154 - loss 0.82869032 - samples / sec : 24.99 2020 - 07 - 02 15 : 00 : 09 , 318 epoch 6 - iter 75 / 154 - loss 0.82800252 - samples / sec : 25.06 2020 - 07 - 02 15 : 00 : 28 , 879 epoch 6 - iter 90 / 154 - loss 0.82355009 - samples / sec : 24.69 2020 - 07 - 02 15 : 00 : 49 , 260 epoch 6 - iter 105 / 154 - loss 0.80982284 - samples / sec : 23.85 2020 - 07 - 02 15 : 01 : 10 , 265 epoch 6 - iter 120 / 154 - loss 0.80053075 - samples / sec : 22.98 2020 - 07 - 02 15 : 01 : 30 , 165 epoch 6 - iter 135 / 154 - loss 0.78381255 - samples / sec : 24.43 2020 - 07 - 02 15 : 01 : 49 , 998 epoch 6 - iter 150 / 154 - loss 0.77709739 - samples / sec : 24.35 2020 - 07 - 02 15 : 01 : 54 , 600 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 01 : 54 , 601 EPOCH 6 done : loss 0.7798 - lr 0.0162181 2020 - 07 - 02 15 : 02 : 16 , 924 DEV : loss 0.5837535262107849 - score 0.9315 2020 - 07 - 02 15 : 02 : 16 , 994 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 02 : 21 , 257 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 02 : 42 , 864 epoch 7 - iter 15 / 154 - loss 0.60823991 - samples / sec : 22.71 2020 - 07 - 02 15 : 03 : 03 , 054 epoch 7 - iter 30 / 154 - loss 0.64749694 - samples / sec : 23.92 2020 - 07 - 02 15 : 03 : 23 , 289 epoch 7 - iter 45 / 154 - loss 0.67237021 - samples / sec : 23.86 2020 - 07 - 02 15 : 03 : 44 , 783 epoch 7 - iter 60 / 154 - loss 0.67263686 - samples / sec : 22.61 2020 - 07 - 02 15 : 04 : 04 , 331 epoch 7 - iter 75 / 154 - loss 0.67410455 - samples / sec : 24.71 2020 - 07 - 02 15 : 04 : 25 , 483 epoch 7 - iter 90 / 154 - loss 0.67185280 - samples / sec : 22.82 2020 - 07 - 02 15 : 04 : 44 , 808 epoch 7 - iter 105 / 154 - loss 0.66940188 - samples / sec : 25.16 2020 - 07 - 02 15 : 05 : 03 , 509 epoch 7 - iter 120 / 154 - loss 0.67044823 - samples / sec : 25.81 2020 - 07 - 02 15 : 05 : 21 , 901 epoch 7 - iter 135 / 154 - loss 0.67026268 - samples / sec : 26.26 2020 - 07 - 02 15 : 05 : 40 , 996 epoch 7 - iter 150 / 154 - loss 0.66505048 - samples / sec : 25.45 2020 - 07 - 02 15 : 05 : 45 , 685 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 05 : 45 , 686 EPOCH 7 done : loss 0.6625 - lr 0.0162181 2020 - 07 - 02 15 : 06 : 07 , 975 DEV : loss 0.532750129699707 - score 0.9346 2020 - 07 - 02 15 : 06 : 08 , 045 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 06 : 12 , 337 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 06 : 32 , 560 epoch 8 - iter 15 / 154 - loss 0.61121991 - samples / sec : 24.32 2020 - 07 - 02 15 : 06 : 52 , 919 epoch 8 - iter 30 / 154 - loss 0.59228445 - samples / sec : 23.71 2020 - 07 - 02 15 : 07 : 12 , 810 epoch 8 - iter 45 / 154 - loss 0.57655472 - samples / sec : 24.28 2020 - 07 - 02 15 : 07 : 32 , 711 epoch 8 - iter 60 / 154 - loss 0.57155969 - samples / sec : 24.27 2020 - 07 - 02 15 : 07 : 52 , 405 epoch 8 - iter 75 / 154 - loss 0.56132450 - samples / sec : 24.51 2020 - 07 - 02 15 : 08 : 13 , 561 epoch 8 - iter 90 / 154 - loss 0.55938630 - samples / sec : 22.81 2020 - 07 - 02 15 : 08 : 35 , 125 epoch 8 - iter 105 / 154 - loss 0.55755164 - samples / sec : 22.51 2020 - 07 - 02 15 : 08 : 54 , 527 epoch 8 - iter 120 / 154 - loss 0.56773651 - samples / sec : 24.89 2020 - 07 - 02 15 : 09 : 14 , 350 epoch 8 - iter 135 / 154 - loss 0.56791281 - samples / sec : 24.35 2020 - 07 - 02 15 : 09 : 34 , 306 epoch 8 - iter 150 / 154 - loss 0.56556819 - samples / sec : 24.36 2020 - 07 - 02 15 : 09 : 39 , 031 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 09 : 39 , 033 EPOCH 8 done : loss 0.5619 - lr 0.0162181 2020 - 07 - 02 15 : 10 : 01 , 373 DEV : loss 0.6124246716499329 - score 0.9138 2020 - 07 - 02 15 : 10 : 01 , 443 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 15 : 10 : 01 , 444 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 10 : 22 , 741 epoch 9 - iter 15 / 154 - loss 0.52073216 - samples / sec : 23.09 2020 - 07 - 02 15 : 10 : 41 , 808 epoch 9 - iter 30 / 154 - loss 0.51747889 - samples / sec : 25.35 2020 - 07 - 02 15 : 11 : 03 , 530 epoch 9 - iter 45 / 154 - loss 0.50766587 - samples / sec : 22.22 2020 - 07 - 02 15 : 11 : 22 , 831 epoch 9 - iter 60 / 154 - loss 0.49717654 - samples / sec : 25.19 2020 - 07 - 02 15 : 11 : 43 , 282 epoch 9 - iter 75 / 154 - loss 0.48794214 - samples / sec : 23.61 2020 - 07 - 02 15 : 12 : 03 , 162 epoch 9 - iter 90 / 154 - loss 0.48554325 - samples / sec : 24.28 2020 - 07 - 02 15 : 12 : 22 , 939 epoch 9 - iter 105 / 154 - loss 0.48294531 - samples / sec : 24.58 2020 - 07 - 02 15 : 12 : 42 , 470 epoch 9 - iter 120 / 154 - loss 0.47932543 - samples / sec : 24.72 2020 - 07 - 02 15 : 13 : 02 , 512 epoch 9 - iter 135 / 154 - loss 0.48639485 - samples / sec : 24.09 2020 - 07 - 02 15 : 13 : 22 , 715 epoch 9 - iter 150 / 154 - loss 0.48241082 - samples / sec : 24.10 2020 - 07 - 02 15 : 13 : 27 , 354 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 13 : 27 , 356 EPOCH 9 done : loss 0.4791 - lr 0.0162181 2020 - 07 - 02 15 : 13 : 49 , 788 DEV : loss 0.39634451270103455 - score 0.9511 2020 - 07 - 02 15 : 13 : 49 , 893 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 13 : 54 , 198 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 14 : 15 , 385 epoch 10 - iter 15 / 154 - loss 0.39700798 - samples / sec : 22.97 2020 - 07 - 02 15 : 14 : 35 , 579 epoch 10 - iter 30 / 154 - loss 0.44900593 - samples / sec : 24.16 2020 - 07 - 02 15 : 14 : 55 , 026 epoch 10 - iter 45 / 154 - loss 0.43003887 - samples / sec : 24.84 2020 - 07 - 02 15 : 15 : 14 , 794 epoch 10 - iter 60 / 154 - loss 0.44525786 - samples / sec : 24.45 2020 - 07 - 02 15 : 15 : 34 , 512 epoch 10 - iter 75 / 154 - loss 0.45256295 - samples / sec : 24.49 2020 - 07 - 02 15 : 15 : 53 , 671 epoch 10 - iter 90 / 154 - loss 0.45120489 - samples / sec : 25.21 2020 - 07 - 02 15 : 16 : 15 , 627 epoch 10 - iter 105 / 154 - loss 0.44198098 - samples / sec : 22.12 2020 - 07 - 02 15 : 16 : 36 , 052 epoch 10 - iter 120 / 154 - loss 0.43900539 - samples / sec : 23.63 2020 - 07 - 02 15 : 16 : 54 , 831 epoch 10 - iter 135 / 154 - loss 0.44348369 - samples / sec : 25.71 2020 - 07 - 02 15 : 17 : 14 , 348 epoch 10 - iter 150 / 154 - loss 0.44872592 - samples / sec : 24.91 2020 - 07 - 02 15 : 17 : 19 , 719 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 17 : 19 , 721 EPOCH 10 done : loss 0.4485 - lr 0.0162181 2020 - 07 - 02 15 : 17 : 42 , 071 DEV : loss 0.37473350763320923 - score 0.9554 2020 - 07 - 02 15 : 17 : 42 , 141 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 17 : 46 , 432 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 18 : 07 , 509 epoch 11 - iter 15 / 154 - loss 0.42587440 - samples / sec : 23.07 2020 - 07 - 02 15 : 18 : 26 , 425 epoch 11 - iter 30 / 154 - loss 0.41586999 - samples / sec : 25.55 2020 - 07 - 02 15 : 18 : 45 , 758 epoch 11 - iter 45 / 154 - loss 0.40622304 - samples / sec : 24.98 2020 - 07 - 02 15 : 19 : 08 , 078 epoch 11 - iter 60 / 154 - loss 0.41316052 - samples / sec : 21.62 2020 - 07 - 02 15 : 19 : 27 , 885 epoch 11 - iter 75 / 154 - loss 0.42014157 - samples / sec : 24.55 2020 - 07 - 02 15 : 19 : 46 , 742 epoch 11 - iter 90 / 154 - loss 0.40332305 - samples / sec : 25.61 2020 - 07 - 02 15 : 20 : 06 , 936 epoch 11 - iter 105 / 154 - loss 0.40566851 - samples / sec : 24.06 2020 - 07 - 02 15 : 20 : 27 , 452 epoch 11 - iter 120 / 154 - loss 0.40743910 - samples / sec : 23.53 2020 - 07 - 02 15 : 20 : 47 , 670 epoch 11 - iter 135 / 154 - loss 0.40461053 - samples / sec : 23.88 2020 - 07 - 02 15 : 21 : 07 , 230 epoch 11 - iter 150 / 154 - loss 0.40773223 - samples / sec : 24.69 2020 - 07 - 02 15 : 21 : 12 , 274 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 21 : 12 , 275 EPOCH 11 done : loss 0.4066 - lr 0.0162181 2020 - 07 - 02 15 : 21 : 34 , 988 DEV : loss 0.37664657831192017 - score 0.9498 2020 - 07 - 02 15 : 21 : 35 , 056 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 15 : 21 : 35 , 057 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 21 : 55 , 878 epoch 12 - iter 15 / 154 - loss 0.35145220 - samples / sec : 23.35 2020 - 07 - 02 15 : 22 : 15 , 511 epoch 12 - iter 30 / 154 - loss 0.35834764 - samples / sec : 24.87 2020 - 07 - 02 15 : 22 : 34 , 140 epoch 12 - iter 45 / 154 - loss 0.35214746 - samples / sec : 25.95 2020 - 07 - 02 15 : 22 : 53 , 836 epoch 12 - iter 60 / 154 - loss 0.35563465 - samples / sec : 24.53 2020 - 07 - 02 15 : 23 : 14 , 561 epoch 12 - iter 75 / 154 - loss 0.35541137 - samples / sec : 23.29 2020 - 07 - 02 15 : 23 : 35 , 306 epoch 12 - iter 90 / 154 - loss 0.35891361 - samples / sec : 23.46 2020 - 07 - 02 15 : 23 : 55 , 845 epoch 12 - iter 105 / 154 - loss 0.36141122 - samples / sec : 23.50 2020 - 07 - 02 15 : 24 : 16 , 115 epoch 12 - iter 120 / 154 - loss 0.36918869 - samples / sec : 23.83 2020 - 07 - 02 15 : 24 : 36 , 956 epoch 12 - iter 135 / 154 - loss 0.37309432 - samples / sec : 23.30 2020 - 07 - 02 15 : 24 : 55 , 936 epoch 12 - iter 150 / 154 - loss 0.37402713 - samples / sec : 25.45 2020 - 07 - 02 15 : 25 : 01 , 049 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 25 : 01 , 051 EPOCH 12 done : loss 0.3721 - lr 0.0162181 2020 - 07 - 02 15 : 25 : 24 , 063 DEV : loss 0.3611939251422882 - score 0.9523 2020 - 07 - 02 15 : 25 : 24 , 131 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 15 : 25 : 24 , 132 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 25 : 46 , 995 epoch 13 - iter 15 / 154 - loss 0.38793649 - samples / sec : 21.25 2020 - 07 - 02 15 : 26 : 08 , 016 epoch 13 - iter 30 / 154 - loss 0.38421851 - samples / sec : 22.98 2020 - 07 - 02 15 : 26 : 28 , 932 epoch 13 - iter 45 / 154 - loss 0.36525546 - samples / sec : 23.28 2020 - 07 - 02 15 : 26 : 49 , 413 epoch 13 - iter 60 / 154 - loss 0.36084372 - samples / sec : 23.57 2020 - 07 - 02 15 : 27 : 08 , 492 epoch 13 - iter 75 / 154 - loss 0.35162010 - samples / sec : 25.47 2020 - 07 - 02 15 : 27 : 28 , 847 epoch 13 - iter 90 / 154 - loss 0.35324366 - samples / sec : 23.76 2020 - 07 - 02 15 : 27 : 48 , 163 epoch 13 - iter 105 / 154 - loss 0.35290401 - samples / sec : 25.01 2020 - 07 - 02 15 : 28 : 06 , 650 epoch 13 - iter 120 / 154 - loss 0.35728267 - samples / sec : 26.34 2020 - 07 - 02 15 : 28 : 26 , 045 epoch 13 - iter 135 / 154 - loss 0.35024357 - samples / sec : 24.89 2020 - 07 - 02 15 : 28 : 46 , 154 epoch 13 - iter 150 / 154 - loss 0.34566470 - samples / sec : 24.00 2020 - 07 - 02 15 : 28 : 50 , 959 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 28 : 50 , 960 EPOCH 13 done : loss 0.3445 - lr 0.0162181 2020 - 07 - 02 15 : 29 : 14 , 173 DEV : loss 0.33947789669036865 - score 0.9578 2020 - 07 - 02 15 : 29 : 14 , 240 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 29 : 18 , 531 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 29 : 40 , 891 epoch 14 - iter 15 / 154 - loss 0.38821750 - samples / sec : 21.75 2020 - 07 - 02 15 : 29 : 59 , 401 epoch 14 - iter 30 / 154 - loss 0.35584508 - samples / sec : 26.38 2020 - 07 - 02 15 : 30 : 20 , 485 epoch 14 - iter 45 / 154 - loss 0.33287027 - samples / sec : 22.90 2020 - 07 - 02 15 : 30 : 40 , 526 epoch 14 - iter 60 / 154 - loss 0.33028220 - samples / sec : 24.09 2020 - 07 - 02 15 : 30 : 59 , 343 epoch 14 - iter 75 / 154 - loss 0.33653424 - samples / sec : 25.67 2020 - 07 - 02 15 : 31 : 19 , 277 epoch 14 - iter 90 / 154 - loss 0.33223337 - samples / sec : 24.23 2020 - 07 - 02 15 : 31 : 38 , 786 epoch 14 - iter 105 / 154 - loss 0.32999784 - samples / sec : 24.76 2020 - 07 - 02 15 : 31 : 58 , 166 epoch 14 - iter 120 / 154 - loss 0.32390204 - samples / sec : 24.94 2020 - 07 - 02 15 : 32 : 18 , 669 epoch 14 - iter 135 / 154 - loss 0.31847246 - samples / sec : 23.70 2020 - 07 - 02 15 : 32 : 38 , 980 epoch 14 - iter 150 / 154 - loss 0.31818390 - samples / sec : 23.78 2020 - 07 - 02 15 : 32 : 43 , 782 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 32 : 43 , 784 EPOCH 14 done : loss 0.3160 - lr 0.0162181 2020 - 07 - 02 15 : 33 : 06 , 216 DEV : loss 0.33674922585487366 - score 0.9517 2020 - 07 - 02 15 : 33 : 06 , 286 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 15 : 33 : 06 , 288 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 33 : 28 , 131 epoch 15 - iter 15 / 154 - loss 0.32382143 - samples / sec : 22.46 2020 - 07 - 02 15 : 33 : 47 , 269 epoch 15 - iter 30 / 154 - loss 0.32938266 - samples / sec : 25.24 2020 - 07 - 02 15 : 34 : 08 , 013 epoch 15 - iter 45 / 154 - loss 0.35580096 - samples / sec : 23.27 2020 - 07 - 02 15 : 34 : 28 , 451 epoch 15 - iter 60 / 154 - loss 0.33785067 - samples / sec : 23.79 2020 - 07 - 02 15 : 34 : 47 , 864 epoch 15 - iter 75 / 154 - loss 0.32975845 - samples / sec : 24.87 2020 - 07 - 02 15 : 35 : 08 , 211 epoch 15 - iter 90 / 154 - loss 0.31067502 - samples / sec : 23.74 2020 - 07 - 02 15 : 35 : 27 , 686 epoch 15 - iter 105 / 154 - loss 0.30810503 - samples / sec : 24.79 2020 - 07 - 02 15 : 35 : 48 , 510 epoch 15 - iter 120 / 154 - loss 0.30373972 - samples / sec : 23.17 2020 - 07 - 02 15 : 36 : 09 , 118 epoch 15 - iter 135 / 154 - loss 0.30006551 - samples / sec : 23.44 2020 - 07 - 02 15 : 36 : 28 , 709 epoch 15 - iter 150 / 154 - loss 0.29886991 - samples / sec : 24.64 2020 - 07 - 02 15 : 36 : 33 , 721 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 36 : 33 , 723 EPOCH 15 done : loss 0.3044 - lr 0.0162181 2020 - 07 - 02 15 : 36 : 56 , 211 DEV : loss 0.45606178045272827 - score 0.9517 2020 - 07 - 02 15 : 36 : 56 , 282 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 15 : 36 : 56 , 284 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 37 : 16 , 899 epoch 16 - iter 15 / 154 - loss 0.26865751 - samples / sec : 23.84 2020 - 07 - 02 15 : 37 : 38 , 328 epoch 16 - iter 30 / 154 - loss 0.25540573 - samples / sec : 22.51 2020 - 07 - 02 15 : 37 : 57 , 159 epoch 16 - iter 45 / 154 - loss 0.26173169 - samples / sec : 25.65 2020 - 07 - 02 15 : 38 : 17 , 127 epoch 16 - iter 60 / 154 - loss 0.26441356 - samples / sec : 24.17 2020 - 07 - 02 15 : 38 : 37 , 460 epoch 16 - iter 75 / 154 - loss 0.27951374 - samples / sec : 23.75 2020 - 07 - 02 15 : 38 : 57 , 341 epoch 16 - iter 90 / 154 - loss 0.28452394 - samples / sec : 24.43 2020 - 07 - 02 15 : 39 : 17 , 755 epoch 16 - iter 105 / 154 - loss 0.28827544 - samples / sec : 23.65 2020 - 07 - 02 15 : 39 : 37 , 319 epoch 16 - iter 120 / 154 - loss 0.29456732 - samples / sec : 24.69 2020 - 07 - 02 15 : 39 : 57 , 241 epoch 16 - iter 135 / 154 - loss 0.29623859 - samples / sec : 24.40 2020 - 07 - 02 15 : 40 : 17 , 446 epoch 16 - iter 150 / 154 - loss 0.29836056 - samples / sec : 23.89 2020 - 07 - 02 15 : 40 : 22 , 068 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 40 : 22 , 069 EPOCH 16 done : loss 0.3000 - lr 0.0162181 2020 - 07 - 02 15 : 40 : 44 , 528 DEV : loss 0.36227965354919434 - score 0.956 2020 - 07 - 02 15 : 40 : 44 , 597 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 15 : 40 : 44 , 598 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 41 : 06 , 680 epoch 17 - iter 15 / 154 - loss 0.32204509 - samples / sec : 22.22 2020 - 07 - 02 15 : 41 : 27 , 155 epoch 17 - iter 30 / 154 - loss 0.28370522 - samples / sec : 23.60 2020 - 07 - 02 15 : 41 : 47 , 382 epoch 17 - iter 45 / 154 - loss 0.28329503 - samples / sec : 23.87 2020 - 07 - 02 15 : 42 : 06 , 345 epoch 17 - iter 60 / 154 - loss 0.28177566 - samples / sec : 25.66 2020 - 07 - 02 15 : 42 : 26 , 573 epoch 17 - iter 75 / 154 - loss 0.28397036 - samples / sec : 23.86 2020 - 07 - 02 15 : 42 : 47 , 908 epoch 17 - iter 90 / 154 - loss 0.28389345 - samples / sec : 22.63 2020 - 07 - 02 15 : 43 : 08 , 229 epoch 17 - iter 105 / 154 - loss 0.27705963 - samples / sec : 23.90 2020 - 07 - 02 15 : 43 : 28 , 245 epoch 17 - iter 120 / 154 - loss 0.27206560 - samples / sec : 24.13 2020 - 07 - 02 15 : 43 : 47 , 708 epoch 17 - iter 135 / 154 - loss 0.27306891 - samples / sec : 24.80 2020 - 07 - 02 15 : 44 : 06 , 403 epoch 17 - iter 150 / 154 - loss 0.27053858 - samples / sec : 25.85 2020 - 07 - 02 15 : 44 : 10 , 964 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 44 : 10 , 965 EPOCH 17 done : loss 0.2706 - lr 0.0162181 2020 - 07 - 02 15 : 44 : 33 , 212 DEV : loss 0.3132312595844269 - score 0.9682 2020 - 07 - 02 15 : 44 : 33 , 281 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 44 : 37 , 552 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 45 : 00 , 030 epoch 18 - iter 15 / 154 - loss 0.25208958 - samples / sec : 21.83 2020 - 07 - 02 15 : 45 : 19 , 063 epoch 18 - iter 30 / 154 - loss 0.27032827 - samples / sec : 25.37 2020 - 07 - 02 15 : 45 : 39 , 871 epoch 18 - iter 45 / 154 - loss 0.26369591 - samples / sec : 23.35 2020 - 07 - 02 15 : 46 : 00 , 460 epoch 18 - iter 60 / 154 - loss 0.27755907 - samples / sec : 23.44 2020 - 07 - 02 15 : 46 : 21 , 331 epoch 18 - iter 75 / 154 - loss 0.26805330 - samples / sec : 23.14 2020 - 07 - 02 15 : 46 : 39 , 926 epoch 18 - iter 90 / 154 - loss 0.26553340 - samples / sec : 25.97 2020 - 07 - 02 15 : 47 : 00 , 419 epoch 18 - iter 105 / 154 - loss 0.26726629 - samples / sec : 23.71 2020 - 07 - 02 15 : 47 : 19 , 530 epoch 18 - iter 120 / 154 - loss 0.26267663 - samples / sec : 25.27 2020 - 07 - 02 15 : 47 : 38 , 403 epoch 18 - iter 135 / 154 - loss 0.26108754 - samples / sec : 25.76 2020 - 07 - 02 15 : 47 : 58 , 105 epoch 18 - iter 150 / 154 - loss 0.26345694 - samples / sec : 24.51 2020 - 07 - 02 15 : 48 : 02 , 615 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 48 : 02 , 617 EPOCH 18 done : loss 0.2690 - lr 0.0162181 2020 - 07 - 02 15 : 48 : 24 , 979 DEV : loss 0.24188274145126343 - score 0.9706 2020 - 07 - 02 15 : 48 : 25 , 049 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 48 : 29 , 343 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 48 : 52 , 980 epoch 19 - iter 15 / 154 - loss 0.26782284 - samples / sec : 20.73 2020 - 07 - 02 15 : 49 : 12 , 800 epoch 19 - iter 30 / 154 - loss 0.25746150 - samples / sec : 24.36 2020 - 07 - 02 15 : 49 : 32 , 595 epoch 19 - iter 45 / 154 - loss 0.25379602 - samples / sec : 24.38 2020 - 07 - 02 15 : 49 : 51 , 373 epoch 19 - iter 60 / 154 - loss 0.25453721 - samples / sec : 25.73 2020 - 07 - 02 15 : 50 : 12 , 023 epoch 19 - iter 75 / 154 - loss 0.24971705 - samples / sec : 23.39 2020 - 07 - 02 15 : 50 : 33 , 769 epoch 19 - iter 90 / 154 - loss 0.25132714 - samples / sec : 22.20 2020 - 07 - 02 15 : 50 : 52 , 815 epoch 19 - iter 105 / 154 - loss 0.24608561 - samples / sec : 25.34 2020 - 07 - 02 15 : 51 : 12 , 130 epoch 19 - iter 120 / 154 - loss 0.24540012 - samples / sec : 25.01 2020 - 07 - 02 15 : 51 : 30 , 127 epoch 19 - iter 135 / 154 - loss 0.24441875 - samples / sec : 26.82 2020 - 07 - 02 15 : 51 : 50 , 334 epoch 19 - iter 150 / 154 - loss 0.24105846 - samples / sec : 24.05 2020 - 07 - 02 15 : 51 : 55 , 900 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 51 : 55 , 902 EPOCH 19 done : loss 0.2417 - lr 0.0162181 2020 - 07 - 02 15 : 52 : 18 , 249 DEV : loss 0.2675461173057556 - score 0.9645 2020 - 07 - 02 15 : 52 : 18 , 315 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 15 : 52 : 18 , 317 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 52 : 40 , 145 epoch 20 - iter 15 / 154 - loss 0.22299275 - samples / sec : 22.48 2020 - 07 - 02 15 : 52 : 59 , 619 epoch 20 - iter 30 / 154 - loss 0.23901086 - samples / sec : 24.79 2020 - 07 - 02 15 : 53 : 20 , 568 epoch 20 - iter 45 / 154 - loss 0.22591903 - samples / sec : 23.04 2020 - 07 - 02 15 : 53 : 40 , 829 epoch 20 - iter 60 / 154 - loss 0.21634991 - samples / sec : 23.98 2020 - 07 - 02 15 : 54 : 01 , 041 epoch 20 - iter 75 / 154 - loss 0.22712179 - samples / sec : 23.89 2020 - 07 - 02 15 : 54 : 21 , 699 epoch 20 - iter 90 / 154 - loss 0.22611159 - samples / sec : 23.37 2020 - 07 - 02 15 : 54 : 41 , 296 epoch 20 - iter 105 / 154 - loss 0.22637414 - samples / sec : 24.81 2020 - 07 - 02 15 : 55 : 01 , 411 epoch 20 - iter 120 / 154 - loss 0.22813549 - samples / sec : 24.00 2020 - 07 - 02 15 : 55 : 21 , 366 epoch 20 - iter 135 / 154 - loss 0.22931698 - samples / sec : 24.20 2020 - 07 - 02 15 : 55 : 41 , 199 epoch 20 - iter 150 / 154 - loss 0.23357049 - samples / sec : 24.54 2020 - 07 - 02 15 : 55 : 45 , 334 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 55 : 45 , 335 EPOCH 20 done : loss 0.2297 - lr 0.0162181 2020 - 07 - 02 15 : 56 : 07 , 628 DEV : loss 0.23396362364292145 - score 0.9719 2020 - 07 - 02 15 : 56 : 07 , 700 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 56 : 11 , 976 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 56 : 33 , 755 epoch 21 - iter 15 / 154 - loss 0.24369793 - samples / sec : 22.53 2020 - 07 - 02 15 : 56 : 53 , 590 epoch 21 - iter 30 / 154 - loss 0.22235053 - samples / sec : 24.33 2020 - 07 - 02 15 : 57 : 13 , 517 epoch 21 - iter 45 / 154 - loss 0.22550796 - samples / sec : 24.23 2020 - 07 - 02 15 : 57 : 33 , 431 epoch 21 - iter 60 / 154 - loss 0.22605099 - samples / sec : 24.22 2020 - 07 - 02 15 : 57 : 53 , 093 epoch 21 - iter 75 / 154 - loss 0.21876556 - samples / sec : 24.57 2020 - 07 - 02 15 : 58 : 12 , 986 epoch 21 - iter 90 / 154 - loss 0.22181167 - samples / sec : 24.28 2020 - 07 - 02 15 : 58 : 33 , 326 epoch 21 - iter 105 / 154 - loss 0.22137965 - samples / sec : 23.75 2020 - 07 - 02 15 : 58 : 53 , 510 epoch 21 - iter 120 / 154 - loss 0.22005988 - samples / sec : 24.07 2020 - 07 - 02 15 : 59 : 14 , 691 epoch 21 - iter 135 / 154 - loss 0.22334215 - samples / sec : 22.80 2020 - 07 - 02 15 : 59 : 33 , 247 epoch 21 - iter 150 / 154 - loss 0.22272228 - samples / sec : 26.02 2020 - 07 - 02 15 : 59 : 38 , 154 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 59 : 38 , 156 EPOCH 21 done : loss 0.2209 - lr 0.0162181 2020 - 07 - 02 16 : 00 : 00 , 536 DEV : loss 0.2306308150291443 - score 0.9737 2020 - 07 - 02 16 : 00 : 00 , 619 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 16 : 00 : 04 , 925 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 00 : 27 , 080 epoch 22 - iter 15 / 154 - loss 0.15147622 - samples / sec : 21.95 2020 - 07 - 02 16 : 00 : 48 , 794 epoch 22 - iter 30 / 154 - loss 0.23374797 - samples / sec : 22.23 2020 - 07 - 02 16 : 01 : 07 , 535 epoch 22 - iter 45 / 154 - loss 0.23072744 - samples / sec : 25.76 2020 - 07 - 02 16 : 01 : 28 , 454 epoch 22 - iter 60 / 154 - loss 0.23073106 - samples / sec : 23.22 2020 - 07 - 02 16 : 01 : 47 , 677 epoch 22 - iter 75 / 154 - loss 0.22344845 - samples / sec : 25.13 2020 - 07 - 02 16 : 02 : 06 , 080 epoch 22 - iter 90 / 154 - loss 0.21383200 - samples / sec : 26.24 2020 - 07 - 02 16 : 02 : 25 , 456 epoch 22 - iter 105 / 154 - loss 0.21821867 - samples / sec : 25.10 2020 - 07 - 02 16 : 02 : 46 , 316 epoch 22 - iter 120 / 154 - loss 0.21940228 - samples / sec : 23.14 2020 - 07 - 02 16 : 03 : 06 , 127 epoch 22 - iter 135 / 154 - loss 0.21804290 - samples / sec : 24.56 2020 - 07 - 02 16 : 03 : 25 , 057 epoch 22 - iter 150 / 154 - loss 0.21798164 - samples / sec : 25.51 2020 - 07 - 02 16 : 03 : 29 , 479 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 03 : 29 , 481 EPOCH 22 done : loss 0.2170 - lr 0.0162181 2020 - 07 - 02 16 : 03 : 51 , 825 DEV : loss 0.2819221615791321 - score 0.9651 2020 - 07 - 02 16 : 03 : 51 , 897 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 16 : 03 : 51 , 900 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 04 : 14 , 615 epoch 23 - iter 15 / 154 - loss 0.20381826 - samples / sec : 21.38 2020 - 07 - 02 16 : 04 : 36 , 216 epoch 23 - iter 30 / 154 - loss 0.21659263 - samples / sec : 22.34 2020 - 07 - 02 16 : 04 : 55 , 141 epoch 23 - iter 45 / 154 - loss 0.20468760 - samples / sec : 25.52 2020 - 07 - 02 16 : 05 : 14 , 993 epoch 23 - iter 60 / 154 - loss 0.20271637 - samples / sec : 24.48 2020 - 07 - 02 16 : 05 : 34 , 130 epoch 23 - iter 75 / 154 - loss 0.19821025 - samples / sec : 25.23 2020 - 07 - 02 16 : 05 : 54 , 292 epoch 23 - iter 90 / 154 - loss 0.20070277 - samples / sec : 24.09 2020 - 07 - 02 16 : 06 : 14 , 437 epoch 23 - iter 105 / 154 - loss 0.20031097 - samples / sec : 23.97 2020 - 07 - 02 16 : 06 : 34 , 083 epoch 23 - iter 120 / 154 - loss 0.20813754 - samples / sec : 24.58 2020 - 07 - 02 16 : 06 : 52 , 903 epoch 23 - iter 135 / 154 - loss 0.21487906 - samples / sec : 25.88 2020 - 07 - 02 16 : 07 : 13 , 310 epoch 23 - iter 150 / 154 - loss 0.20735793 - samples / sec : 23.65 2020 - 07 - 02 16 : 07 : 17 , 500 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 07 : 17 , 501 EPOCH 23 done : loss 0.2128 - lr 0.0162181 2020 - 07 - 02 16 : 07 : 39 , 965 DEV : loss 0.23975355923175812 - score 0.9719 2020 - 07 - 02 16 : 07 : 40 , 036 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 16 : 07 : 40 , 037 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 08 : 02 , 215 epoch 24 - iter 15 / 154 - loss 0.24556303 - samples / sec : 22.19 2020 - 07 - 02 16 : 08 : 21 , 804 epoch 24 - iter 30 / 154 - loss 0.21485928 - samples / sec : 24.66 2020 - 07 - 02 16 : 08 : 42 , 513 epoch 24 - iter 45 / 154 - loss 0.20294512 - samples / sec : 23.31 2020 - 07 - 02 16 : 09 : 02 , 210 epoch 24 - iter 60 / 154 - loss 0.20490214 - samples / sec : 24.55 2020 - 07 - 02 16 : 09 : 23 , 391 epoch 24 - iter 75 / 154 - loss 0.21047717 - samples / sec : 22.93 2020 - 07 - 02 16 : 09 : 43 , 197 epoch 24 - iter 90 / 154 - loss 0.20396163 - samples / sec : 24.38 2020 - 07 - 02 16 : 10 : 04 , 245 epoch 24 - iter 105 / 154 - loss 0.20865799 - samples / sec : 22.94 2020 - 07 - 02 16 : 10 : 22 , 417 epoch 24 - iter 120 / 154 - loss 0.21269711 - samples / sec : 26.59 2020 - 07 - 02 16 : 10 : 41 , 693 epoch 24 - iter 135 / 154 - loss 0.20936885 - samples / sec : 25.06 2020 - 07 - 02 16 : 11 : 01 , 235 epoch 24 - iter 150 / 154 - loss 0.21083501 - samples / sec : 24.71 2020 - 07 - 02 16 : 11 : 05 , 323 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 11 : 05 , 324 EPOCH 24 done : loss 0.2130 - lr 0.0162181 2020 - 07 - 02 16 : 11 : 27 , 865 DEV : loss 0.2661949396133423 - score 0.9737 2020 - 07 - 02 16 : 11 : 27 , 934 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 16 : 11 : 27 , 936 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 11 : 50 , 567 epoch 25 - iter 15 / 154 - loss 0.24687227 - samples / sec : 21.46 2020 - 07 - 02 16 : 12 : 08 , 728 epoch 25 - iter 30 / 154 - loss 0.21076790 - samples / sec : 26.60 2020 - 07 - 02 16 : 12 : 29 , 997 epoch 25 - iter 45 / 154 - loss 0.18616831 - samples / sec : 22.90 2020 - 07 - 02 16 : 12 : 48 , 632 epoch 25 - iter 60 / 154 - loss 0.18038774 - samples / sec : 25.93 2020 - 07 - 02 16 : 13 : 09 , 459 epoch 25 - iter 75 / 154 - loss 0.18513621 - samples / sec : 23.33 2020 - 07 - 02 16 : 13 : 30 , 101 epoch 25 - iter 90 / 154 - loss 0.18450534 - samples / sec : 23.39 2020 - 07 - 02 16 : 13 : 49 , 169 epoch 25 - iter 105 / 154 - loss 0.18219731 - samples / sec : 25.32 2020 - 07 - 02 16 : 14 : 09 , 165 epoch 25 - iter 120 / 154 - loss 0.18160356 - samples / sec : 24.31 2020 - 07 - 02 16 : 14 : 29 , 648 epoch 25 - iter 135 / 154 - loss 0.18727879 - samples / sec : 23.57 2020 - 07 - 02 16 : 14 : 49 , 182 epoch 25 - iter 150 / 154 - loss 0.18395177 - samples / sec : 24.71 2020 - 07 - 02 16 : 14 : 54 , 658 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 14 : 54 , 660 EPOCH 25 done : loss 0.1870 - lr 0.0162181 2020 - 07 - 02 16 : 15 : 17 , 417 DEV : loss 0.24831292033195496 - score 0.9725 2020 - 07 - 02 16 : 15 : 17 , 486 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 16 : 15 : 17 , 487 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 15 : 40 , 274 epoch 26 - iter 15 / 154 - loss 0.15373853 - samples / sec : 21.31 2020 - 07 - 02 16 : 16 : 00 , 506 epoch 26 - iter 30 / 154 - loss 0.16712674 - samples / sec : 23.88 2020 - 07 - 02 16 : 16 : 21 , 136 epoch 26 - iter 45 / 154 - loss 0.17038985 - samples / sec : 23.63 2020 - 07 - 02 16 : 16 : 40 , 021 epoch 26 - iter 60 / 154 - loss 0.17567901 - samples / sec : 25.57 2020 - 07 - 02 16 : 16 : 59 , 197 epoch 26 - iter 75 / 154 - loss 0.18035345 - samples / sec : 25.19 2020 - 07 - 02 16 : 17 : 18 , 590 epoch 26 - iter 90 / 154 - loss 0.18694772 - samples / sec : 25.07 2020 - 07 - 02 16 : 17 : 38 , 407 epoch 26 - iter 105 / 154 - loss 0.19046586 - samples / sec : 24.37 2020 - 07 - 02 16 : 17 : 59 , 785 epoch 26 - iter 120 / 154 - loss 0.18908963 - samples / sec : 22.58 2020 - 07 - 02 16 : 18 : 19 , 241 epoch 26 - iter 135 / 154 - loss 0.18909395 - samples / sec : 24.82 2020 - 07 - 02 16 : 18 : 39 , 838 epoch 26 - iter 150 / 154 - loss 0.18379643 - samples / sec : 23.59 2020 - 07 - 02 16 : 18 : 44 , 586 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 18 : 44 , 588 EPOCH 26 done : loss 0.1831 - lr 0.0162181 2020 - 07 - 02 16 : 19 : 07 , 027 DEV : loss 0.23407800495624542 - score 0.9719 2020 - 07 - 02 16 : 19 : 07 , 096 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 16 : 19 : 07 , 097 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 19 : 28 , 706 epoch 27 - iter 15 / 154 - loss 0.20348703 - samples / sec : 22.48 2020 - 07 - 02 16 : 19 : 48 , 120 epoch 27 - iter 30 / 154 - loss 0.16796032 - samples / sec : 24.88 2020 - 07 - 02 16 : 20 : 07 , 942 epoch 27 - iter 45 / 154 - loss 0.17932437 - samples / sec : 24.36 2020 - 07 - 02 16 : 20 : 27 , 121 epoch 27 - iter 60 / 154 - loss 0.17203238 - samples / sec : 25.18 2020 - 07 - 02 16 : 20 : 48 , 312 epoch 27 - iter 75 / 154 - loss 0.16742311 - samples / sec : 22.94 2020 - 07 - 02 16 : 21 : 08 , 425 epoch 27 - iter 90 / 154 - loss 0.17116617 - samples / sec : 24.00 2020 - 07 - 02 16 : 21 : 28 , 036 epoch 27 - iter 105 / 154 - loss 0.17072401 - samples / sec : 24.79 2020 - 07 - 02 16 : 21 : 47 , 868 epoch 27 - iter 120 / 154 - loss 0.17486551 - samples / sec : 24.34 2020 - 07 - 02 16 : 22 : 08 , 240 epoch 27 - iter 135 / 154 - loss 0.17548364 - samples / sec : 23.70 2020 - 07 - 02 16 : 22 : 28 , 509 epoch 27 - iter 150 / 154 - loss 0.17920635 - samples / sec : 24.04 2020 - 07 - 02 16 : 22 : 32 , 904 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 22 : 32 , 906 EPOCH 27 done : loss 0.1777 - lr 0.0162181 2020 - 07 - 02 16 : 22 : 55 , 495 DEV : loss 0.3410264253616333 - score 0.9554 Epoch 27 : reducing learning rate of group 0 to 8.1091e-03 . 2020 - 07 - 02 16 : 22 : 55 , 577 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 16 : 22 : 55 , 578 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 23 : 16 , 576 epoch 28 - iter 15 / 154 - loss 0.18262969 - samples / sec : 23.17 2020 - 07 - 02 16 : 23 : 37 , 276 epoch 28 - iter 30 / 154 - loss 0.17605033 - samples / sec : 23.57 2020 - 07 - 02 16 : 23 : 57 , 641 epoch 28 - iter 45 / 154 - loss 0.16287563 - samples / sec : 23.70 2020 - 07 - 02 16 : 24 : 17 , 352 epoch 28 - iter 60 / 154 - loss 0.17845349 - samples / sec : 24.50 2020 - 07 - 02 16 : 24 : 37 , 712 epoch 28 - iter 75 / 154 - loss 0.16782649 - samples / sec : 23.86 2020 - 07 - 02 16 : 24 : 57 , 625 epoch 28 - iter 90 / 154 - loss 0.16389592 - samples / sec : 24.25 2020 - 07 - 02 16 : 25 : 16 , 025 epoch 28 - iter 105 / 154 - loss 0.15795042 - samples / sec : 26.24 2020 - 07 - 02 16 : 25 : 35 , 831 epoch 28 - iter 120 / 154 - loss 0.16067890 - samples / sec : 24.53 2020 - 07 - 02 16 : 25 : 54 , 702 epoch 28 - iter 135 / 154 - loss 0.16030627 - samples / sec : 25.58 2020 - 07 - 02 16 : 26 : 14 , 707 epoch 28 - iter 150 / 154 - loss 0.15914360 - samples / sec : 24.29 2020 - 07 - 02 16 : 26 : 18 , 949 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 26 : 18 , 950 EPOCH 28 done : loss 0.1580 - lr 0.0081091 2020 - 07 - 02 16 : 26 : 41 , 249 DEV : loss 0.2035386562347412 - score 0.9755 2020 - 07 - 02 16 : 26 : 41 , 318 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 16 : 26 : 45 , 583 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 27 : 07 , 154 epoch 29 - iter 15 / 154 - loss 0.14639383 - samples / sec : 22.52 2020 - 07 - 02 16 : 27 : 26 , 806 epoch 29 - iter 30 / 154 - loss 0.12783547 - samples / sec : 24.58 2020 - 07 - 02 16 : 27 : 48 , 187 epoch 29 - iter 45 / 154 - loss 0.13536035 - samples / sec : 22.56 2020 - 07 - 02 16 : 28 : 07 , 762 epoch 29 - iter 60 / 154 - loss 0.14223794 - samples / sec : 24.68 2020 - 07 - 02 16 : 28 : 26 , 724 epoch 29 - iter 75 / 154 - loss 0.14081989 - samples / sec : 25.65 2020 - 07 - 02 16 : 28 : 48 , 403 epoch 29 - iter 90 / 154 - loss 0.13874355 - samples / sec : 22.27 2020 - 07 - 02 16 : 29 : 09 , 293 epoch 29 - iter 105 / 154 - loss 0.14219119 - samples / sec : 23.12 2020 - 07 - 02 16 : 29 : 28 , 855 epoch 29 - iter 120 / 154 - loss 0.14479751 - samples / sec : 24.69 2020 - 07 - 02 16 : 29 : 48 , 408 epoch 29 - iter 135 / 154 - loss 0.14921473 - samples / sec : 24.70 2020 - 07 - 02 16 : 30 : 08 , 553 epoch 29 - iter 150 / 154 - loss 0.14897868 - samples / sec : 23.97 2020 - 07 - 02 16 : 30 : 12 , 911 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 30 : 12 , 913 EPOCH 29 done : loss 0.1476 - lr 0.0081091 2020 - 07 - 02 16 : 30 : 35 , 283 DEV : loss 0.21832378208637238 - score 0.9755 2020 - 07 - 02 16 : 30 : 35 , 493 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 16 : 30 : 35 , 495 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 30 : 56 , 765 epoch 30 - iter 15 / 154 - loss 0.18098603 - samples / sec : 22.84 2020 - 07 - 02 16 : 31 : 15 , 369 epoch 30 - iter 30 / 154 - loss 0.16143890 - samples / sec : 25.97 2020 - 07 - 02 16 : 31 : 35 , 237 epoch 30 - iter 45 / 154 - loss 0.14935495 - samples / sec : 24.50 2020 - 07 - 02 16 : 31 : 55 , 399 epoch 30 - iter 60 / 154 - loss 0.14397024 - samples / sec : 23.94 2020 - 07 - 02 16 : 32 : 14 , 663 epoch 30 - iter 75 / 154 - loss 0.14854784 - samples / sec : 25.07 2020 - 07 - 02 16 : 32 : 35 , 534 epoch 30 - iter 90 / 154 - loss 0.14540661 - samples / sec : 23.14 2020 - 07 - 02 16 : 32 : 55 , 656 epoch 30 - iter 105 / 154 - loss 0.14210871 - samples / sec : 24.00 2020 - 07 - 02 16 : 33 : 15 , 355 epoch 30 - iter 120 / 154 - loss 0.14353256 - samples / sec : 24.50 2020 - 07 - 02 16 : 33 : 36 , 347 epoch 30 - iter 135 / 154 - loss 0.14563753 - samples / sec : 23.14 2020 - 07 - 02 16 : 33 : 57 , 331 epoch 30 - iter 150 / 154 - loss 0.14559265 - samples / sec : 22.99 2020 - 07 - 02 16 : 34 : 01 , 855 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 34 : 01 , 856 EPOCH 30 done : loss 0.1459 - lr 0.0081091 2020 - 07 - 02 16 : 34 : 24 , 512 DEV : loss 0.24465428292751312 - score 0.9719 2020 - 07 - 02 16 : 34 : 24 , 579 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 16 : 34 : 24 , 580 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 34 : 46 , 219 epoch 31 - iter 15 / 154 - loss 0.15976830 - samples / sec : 22.46 2020 - 07 - 02 16 : 35 : 04 , 746 epoch 31 - iter 30 / 154 - loss 0.13884736 - samples / sec : 26.08 2020 - 07 - 02 16 : 35 : 25 , 418 epoch 31 - iter 45 / 154 - loss 0.13285553 - samples / sec : 23.56 2020 - 07 - 02 16 : 35 : 46 , 085 epoch 31 - iter 60 / 154 - loss 0.13884976 - samples / sec : 23.36 2020 - 07 - 02 16 : 36 : 06 , 922 epoch 31 - iter 75 / 154 - loss 0.14738866 - samples / sec : 23.17 2020 - 07 - 02 16 : 36 : 26 , 604 epoch 31 - iter 90 / 154 - loss 0.14702798 - samples / sec : 24.71 2020 - 07 - 02 16 : 36 : 46 , 280 epoch 31 - iter 105 / 154 - loss 0.14182458 - samples / sec : 24.55 2020 - 07 - 02 16 : 37 : 05 , 761 epoch 31 - iter 120 / 154 - loss 0.14055530 - samples / sec : 24.78 2020 - 07 - 02 16 : 37 : 26 , 799 epoch 31 - iter 135 / 154 - loss 0.13758830 - samples / sec : 23.09 2020 - 07 - 02 16 : 37 : 46 , 408 epoch 31 - iter 150 / 154 - loss 0.13833731 - samples / sec : 24.62 2020 - 07 - 02 16 : 37 : 50 , 524 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 37 : 50 , 525 EPOCH 31 done : loss 0.1374 - lr 0.0081091 2020 - 07 - 02 16 : 38 : 12 , 884 DEV : loss 0.20527389645576477 - score 0.9743 2020 - 07 - 02 16 : 38 : 12 , 956 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 16 : 38 : 12 , 957 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 38 : 34 , 427 epoch 32 - iter 15 / 154 - loss 0.11888420 - samples / sec : 23.19 2020 - 07 - 02 16 : 38 : 55 , 683 epoch 32 - iter 30 / 154 - loss 0.13450191 - samples / sec : 22.71 2020 - 07 - 02 16 : 39 : 14 , 237 epoch 32 - iter 45 / 154 - loss 0.13657612 - samples / sec : 26.21 2020 - 07 - 02 16 : 39 : 34 , 635 epoch 32 - iter 60 / 154 - loss 0.13090624 - samples / sec : 23.67 2020 - 07 - 02 16 : 39 : 53 , 688 epoch 32 - iter 75 / 154 - loss 0.13154434 - samples / sec : 25.34 2020 - 07 - 02 16 : 40 : 13 , 250 epoch 32 - iter 90 / 154 - loss 0.13931910 - samples / sec : 24.69 2020 - 07 - 02 16 : 40 : 35 , 482 epoch 32 - iter 105 / 154 - loss 0.14349992 - samples / sec : 21.70 2020 - 07 - 02 16 : 40 : 55 , 671 epoch 32 - iter 120 / 154 - loss 0.14045170 - samples / sec : 24.08 2020 - 07 - 02 16 : 41 : 14 , 900 epoch 32 - iter 135 / 154 - loss 0.14271810 - samples / sec : 25.12 2020 - 07 - 02 16 : 41 : 33 , 122 epoch 32 - iter 150 / 154 - loss 0.14134367 - samples / sec : 26.51 2020 - 07 - 02 16 : 41 : 37 , 990 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 41 : 37 , 992 EPOCH 32 done : loss 0.1423 - lr 0.0081091 2020 - 07 - 02 16 : 42 : 00 , 731 DEV : loss 0.21198561787605286 - score 0.9755 2020 - 07 - 02 16 : 42 : 00 , 803 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 16 : 42 : 00 , 805 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 42 : 23 , 967 epoch 33 - iter 15 / 154 - loss 0.17858952 - samples / sec : 20.97 2020 - 07 - 02 16 : 42 : 44 , 312 epoch 33 - iter 30 / 154 - loss 0.14277964 - samples / sec : 23.75 2020 - 07 - 02 16 : 43 : 04 , 676 epoch 33 - iter 45 / 154 - loss 0.14020735 - samples / sec : 23.93 2020 - 07 - 02 16 : 43 : 25 , 173 epoch 33 - iter 60 / 154 - loss 0.13840914 - samples / sec : 23.56 2020 - 07 - 02 16 : 43 : 44 , 300 epoch 33 - iter 75 / 154 - loss 0.13643146 - samples / sec : 25.25 2020 - 07 - 02 16 : 44 : 04 , 640 epoch 33 - iter 90 / 154 - loss 0.13610004 - samples / sec : 23.90 2020 - 07 - 02 16 : 44 : 24 , 217 epoch 33 - iter 105 / 154 - loss 0.13951315 - samples / sec : 24.65 2020 - 07 - 02 16 : 44 : 44 , 125 epoch 33 - iter 120 / 154 - loss 0.13773997 - samples / sec : 24.25 2020 - 07 - 02 16 : 45 : 02 , 697 epoch 33 - iter 135 / 154 - loss 0.13444212 - samples / sec : 26.22 2020 - 07 - 02 16 : 45 : 22 , 033 epoch 33 - iter 150 / 154 - loss 0.13648703 - samples / sec : 24.97 2020 - 07 - 02 16 : 45 : 26 , 452 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 45 : 26 , 454 EPOCH 33 done : loss 0.1352 - lr 0.0081091 2020 - 07 - 02 16 : 45 : 49 , 301 DEV : loss 0.2025325894355774 - score 0.978 2020 - 07 - 02 16 : 45 : 49 , 370 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 16 : 45 : 53 , 635 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 46 : 16 , 117 epoch 34 - iter 15 / 154 - loss 0.13611445 - samples / sec : 21.61 2020 - 07 - 02 16 : 46 : 36 , 415 epoch 34 - iter 30 / 154 - loss 0.14471161 - samples / sec : 23.79 2020 - 07 - 02 16 : 46 : 57 , 806 epoch 34 - iter 45 / 154 - loss 0.13999367 - samples / sec : 22.56 2020 - 07 - 02 16 : 47 : 17 , 926 epoch 34 - iter 60 / 154 - loss 0.13826550 - samples / sec : 24.01 2020 - 07 - 02 16 : 47 : 35 , 555 epoch 34 - iter 75 / 154 - loss 0.13903977 - samples / sec : 27.41 2020 - 07 - 02 16 : 47 : 54 , 789 epoch 34 - iter 90 / 154 - loss 0.13380337 - samples / sec : 25.11 2020 - 07 - 02 16 : 48 : 14 , 552 epoch 34 - iter 105 / 154 - loss 0.13171350 - samples / sec : 24.42 2020 - 07 - 02 16 : 48 : 34 , 064 epoch 34 - iter 120 / 154 - loss 0.13369191 - samples / sec : 24.75 2020 - 07 - 02 16 : 48 : 54 , 128 epoch 34 - iter 135 / 154 - loss 0.13411653 - samples / sec : 24.23 2020 - 07 - 02 16 : 49 : 14 , 657 epoch 34 - iter 150 / 154 - loss 0.13264349 - samples / sec : 23.51 2020 - 07 - 02 16 : 49 : 19 , 762 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 49 : 19 , 764 EPOCH 34 done : loss 0.1323 - lr 0.0081091 2020 - 07 - 02 16 : 49 : 42 , 706 DEV : loss 0.21952836215496063 - score 0.9761 2020 - 07 - 02 16 : 49 : 42 , 773 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 16 : 49 : 42 , 775 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 50 : 04 , 741 epoch 35 - iter 15 / 154 - loss 0.14532092 - samples / sec : 22.13 2020 - 07 - 02 16 : 50 : 24 , 638 epoch 35 - iter 30 / 154 - loss 0.14132146 - samples / sec : 24.27 2020 - 07 - 02 16 : 50 : 44 , 619 epoch 35 - iter 45 / 154 - loss 0.13029910 - samples / sec : 24.36 2020 - 07 - 02 16 : 51 : 04 , 282 epoch 35 - iter 60 / 154 - loss 0.13496286 - samples / sec : 24.56 2020 - 07 - 02 16 : 51 : 25 , 266 epoch 35 - iter 75 / 154 - loss 0.12704580 - samples / sec : 22.99 2020 - 07 - 02 16 : 51 : 45 , 668 epoch 35 - iter 90 / 154 - loss 0.13263493 - samples / sec : 23.66 2020 - 07 - 02 16 : 52 : 04 , 516 epoch 35 - iter 105 / 154 - loss 0.13433218 - samples / sec : 25.62 2020 - 07 - 02 16 : 52 : 25 , 365 epoch 35 - iter 120 / 154 - loss 0.12975193 - samples / sec : 23.16 2020 - 07 - 02 16 : 52 : 43 , 700 epoch 35 - iter 135 / 154 - loss 0.12926159 - samples / sec : 26.36 2020 - 07 - 02 16 : 53 : 04 , 549 epoch 35 - iter 150 / 154 - loss 0.13078938 - samples / sec : 23.15 2020 - 07 - 02 16 : 53 : 09 , 340 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 53 : 09 , 341 EPOCH 35 done : loss 0.1326 - lr 0.0081091 2020 - 07 - 02 16 : 53 : 31 , 992 DEV : loss 0.21245123445987701 - score 0.9749 2020 - 07 - 02 16 : 53 : 32 , 061 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 16 : 53 : 32 , 063 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 53 : 55 , 029 epoch 36 - iter 15 / 154 - loss 0.11573464 - samples / sec : 21.14 2020 - 07 - 02 16 : 54 : 14 , 451 epoch 36 - iter 30 / 154 - loss 0.13536664 - samples / sec : 25.16 2020 - 07 - 02 16 : 54 : 34 , 936 epoch 36 - iter 45 / 154 - loss 0.14638091 - samples / sec : 23.55 2020 - 07 - 02 16 : 54 : 54 , 032 epoch 36 - iter 60 / 154 - loss 0.14207099 - samples / sec : 25.29 2020 - 07 - 02 16 : 55 : 13 , 615 epoch 36 - iter 75 / 154 - loss 0.14031379 - samples / sec : 24.65 2020 - 07 - 02 16 : 55 : 32 , 985 epoch 36 - iter 90 / 154 - loss 0.13803298 - samples / sec : 24.92 2020 - 07 - 02 16 : 55 : 53 , 148 epoch 36 - iter 105 / 154 - loss 0.14540687 - samples / sec : 23.96 2020 - 07 - 02 16 : 56 : 12 , 803 epoch 36 - iter 120 / 154 - loss 0.14509310 - samples / sec : 24.57 2020 - 07 - 02 16 : 56 : 34 , 448 epoch 36 - iter 135 / 154 - loss 0.14601868 - samples / sec : 22.44 2020 - 07 - 02 16 : 56 : 53 , 871 epoch 36 - iter 150 / 154 - loss 0.13876525 - samples / sec : 24.84 2020 - 07 - 02 16 : 56 : 58 , 371 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 56 : 58 , 372 EPOCH 36 done : loss 0.1369 - lr 0.0081091 2020 - 07 - 02 16 : 57 : 21 , 055 DEV : loss 0.2004116028547287 - score 0.9774 2020 - 07 - 02 16 : 57 : 21 , 129 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 16 : 57 : 21 , 132 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 57 : 43 , 702 epoch 37 - iter 15 / 154 - loss 0.15859565 - samples / sec : 21.53 2020 - 07 - 02 16 : 58 : 03 , 547 epoch 37 - iter 30 / 154 - loss 0.13821206 - samples / sec : 24.33 2020 - 07 - 02 16 : 58 : 23 , 015 epoch 37 - iter 45 / 154 - loss 0.14161196 - samples / sec : 25.03 2020 - 07 - 02 16 : 58 : 42 , 188 epoch 37 - iter 60 / 154 - loss 0.14231336 - samples / sec : 25.19 2020 - 07 - 02 16 : 59 : 01 , 640 epoch 37 - iter 75 / 154 - loss 0.14150818 - samples / sec : 24.82 2020 - 07 - 02 16 : 59 : 22 , 858 epoch 37 - iter 90 / 154 - loss 0.13713271 - samples / sec : 22.75 2020 - 07 - 02 16 : 59 : 44 , 526 epoch 37 - iter 105 / 154 - loss 0.13910145 - samples / sec : 22.28 2020 - 07 - 02 17 : 00 : 03 , 844 epoch 37 - iter 120 / 154 - loss 0.13740354 - samples / sec : 25.18 2020 - 07 - 02 17 : 00 : 23 , 183 epoch 37 - iter 135 / 154 - loss 0.13456342 - samples / sec : 24.97 2020 - 07 - 02 17 : 00 : 42 , 278 epoch 37 - iter 150 / 154 - loss 0.13265983 - samples / sec : 25.29 2020 - 07 - 02 17 : 00 : 46 , 742 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 00 : 46 , 743 EPOCH 37 done : loss 0.1338 - lr 0.0081091 2020 - 07 - 02 17 : 01 : 09 , 404 DEV : loss 0.22566574811935425 - score 0.978 2020 - 07 - 02 17 : 01 : 09 , 481 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 17 : 01 : 09 , 482 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 01 : 31 , 189 epoch 38 - iter 15 / 154 - loss 0.11563111 - samples / sec : 22.40 2020 - 07 - 02 17 : 01 : 53 , 327 epoch 38 - iter 30 / 154 - loss 0.12145271 - samples / sec : 21.80 2020 - 07 - 02 17 : 02 : 13 , 138 epoch 38 - iter 45 / 154 - loss 0.13173554 - samples / sec : 24.61 2020 - 07 - 02 17 : 02 : 33 , 470 epoch 38 - iter 60 / 154 - loss 0.13281630 - samples / sec : 23.74 2020 - 07 - 02 17 : 02 : 51 , 911 epoch 38 - iter 75 / 154 - loss 0.13535262 - samples / sec : 26.19 2020 - 07 - 02 17 : 03 : 11 , 871 epoch 38 - iter 90 / 154 - loss 0.13950271 - samples / sec : 24.35 2020 - 07 - 02 17 : 03 : 30 , 440 epoch 38 - iter 105 / 154 - loss 0.13672152 - samples / sec : 26.02 2020 - 07 - 02 17 : 03 : 51 , 450 epoch 38 - iter 120 / 154 - loss 0.13120697 - samples / sec : 22.97 2020 - 07 - 02 17 : 04 : 11 , 038 epoch 38 - iter 135 / 154 - loss 0.13313321 - samples / sec : 24.81 2020 - 07 - 02 17 : 04 : 32 , 278 epoch 38 - iter 150 / 154 - loss 0.13108277 - samples / sec : 22.72 2020 - 07 - 02 17 : 04 : 36 , 919 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 04 : 36 , 920 EPOCH 38 done : loss 0.1316 - lr 0.0081091 2020 - 07 - 02 17 : 04 : 59 , 607 DEV : loss 0.19655393064022064 - score 0.978 2020 - 07 - 02 17 : 04 : 59 , 675 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 17 : 05 : 03 , 953 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 05 : 27 , 355 epoch 39 - iter 15 / 154 - loss 0.08606811 - samples / sec : 20.77 2020 - 07 - 02 17 : 05 : 47 , 467 epoch 39 - iter 30 / 154 - loss 0.09626995 - samples / sec : 24.00 2020 - 07 - 02 17 : 06 : 08 , 121 epoch 39 - iter 45 / 154 - loss 0.09990381 - samples / sec : 23.60 2020 - 07 - 02 17 : 06 : 28 , 420 epoch 39 - iter 60 / 154 - loss 0.10682175 - samples / sec : 23.77 2020 - 07 - 02 17 : 06 : 49 , 090 epoch 39 - iter 75 / 154 - loss 0.10827231 - samples / sec : 23.50 2020 - 07 - 02 17 : 07 : 07 , 984 epoch 39 - iter 90 / 154 - loss 0.11429365 - samples / sec : 25.57 2020 - 07 - 02 17 : 07 : 27 , 829 epoch 39 - iter 105 / 154 - loss 0.11071864 - samples / sec : 24.34 2020 - 07 - 02 17 : 07 : 47 , 350 epoch 39 - iter 120 / 154 - loss 0.11326766 - samples / sec : 24.92 2020 - 07 - 02 17 : 08 : 06 , 989 epoch 39 - iter 135 / 154 - loss 0.11420242 - samples / sec : 24.60 2020 - 07 - 02 17 : 08 : 24 , 776 epoch 39 - iter 150 / 154 - loss 0.11605036 - samples / sec : 27.14 2020 - 07 - 02 17 : 08 : 30 , 076 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 08 : 30 , 077 EPOCH 39 done : loss 0.1189 - lr 0.0081091 2020 - 07 - 02 17 : 08 : 52 , 666 DEV : loss 0.23231680691242218 - score 0.978 2020 - 07 - 02 17 : 08 : 52 , 733 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 17 : 08 : 52 , 734 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 09 : 14 , 407 epoch 40 - iter 15 / 154 - loss 0.10929688 - samples / sec : 22.45 2020 - 07 - 02 17 : 09 : 35 , 102 epoch 40 - iter 30 / 154 - loss 0.12555539 - samples / sec : 23.57 2020 - 07 - 02 17 : 09 : 54 , 695 epoch 40 - iter 45 / 154 - loss 0.13723706 - samples / sec : 24.64 2020 - 07 - 02 17 : 10 : 15 , 206 epoch 40 - iter 60 / 154 - loss 0.13665709 - samples / sec : 23.55 2020 - 07 - 02 17 : 10 : 34 , 944 epoch 40 - iter 75 / 154 - loss 0.13865776 - samples / sec : 24.62 2020 - 07 - 02 17 : 10 : 54 , 804 epoch 40 - iter 90 / 154 - loss 0.13386259 - samples / sec : 24.31 2020 - 07 - 02 17 : 11 : 14 , 413 epoch 40 - iter 105 / 154 - loss 0.12282358 - samples / sec : 24.61 2020 - 07 - 02 17 : 11 : 34 , 419 epoch 40 - iter 120 / 154 - loss 0.12237809 - samples / sec : 24.30 2020 - 07 - 02 17 : 11 : 53 , 691 epoch 40 - iter 135 / 154 - loss 0.12271550 - samples / sec : 25.05 2020 - 07 - 02 17 : 12 : 14 , 398 epoch 40 - iter 150 / 154 - loss 0.12171200 - samples / sec : 23.32 2020 - 07 - 02 17 : 12 : 18 , 591 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 12 : 18 , 592 EPOCH 40 done : loss 0.1226 - lr 0.0081091 2020 - 07 - 02 17 : 12 : 41 , 415 DEV : loss 0.19786295294761658 - score 0.9786 2020 - 07 - 02 17 : 12 : 41 , 486 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 17 : 12 : 45 , 767 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 13 : 08 , 933 epoch 41 - iter 15 / 154 - loss 0.10291993 - samples / sec : 20.97 2020 - 07 - 02 17 : 13 : 28 , 070 epoch 41 - iter 30 / 154 - loss 0.11291885 - samples / sec : 25.51 2020 - 07 - 02 17 : 13 : 49 , 148 epoch 41 - iter 45 / 154 - loss 0.12107014 - samples / sec : 22.90 2020 - 07 - 02 17 : 14 : 07 , 473 epoch 41 - iter 60 / 154 - loss 0.12080107 - samples / sec : 26.35 2020 - 07 - 02 17 : 14 : 26 , 414 epoch 41 - iter 75 / 154 - loss 0.12976694 - samples / sec : 25.67 2020 - 07 - 02 17 : 14 : 46 , 749 epoch 41 - iter 90 / 154 - loss 0.12766570 - samples / sec : 23.75 2020 - 07 - 02 17 : 15 : 07 , 591 epoch 41 - iter 105 / 154 - loss 0.12382418 - samples / sec : 23.16 2020 - 07 - 02 17 : 15 : 27 , 330 epoch 41 - iter 120 / 154 - loss 0.12289315 - samples / sec : 24.47 2020 - 07 - 02 17 : 15 : 46 , 626 epoch 41 - iter 135 / 154 - loss 0.12151943 - samples / sec : 25.03 2020 - 07 - 02 17 : 16 : 07 , 237 epoch 41 - iter 150 / 154 - loss 0.11910739 - samples / sec : 23.41 2020 - 07 - 02 17 : 16 : 11 , 979 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 16 : 11 , 980 EPOCH 41 done : loss 0.1180 - lr 0.0081091 2020 - 07 - 02 17 : 16 : 34 , 749 DEV : loss 0.20476281642913818 - score 0.9786 2020 - 07 - 02 17 : 16 : 34 , 818 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 17 : 16 : 34 , 819 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 16 : 58 , 607 epoch 42 - iter 15 / 154 - loss 0.11515100 - samples / sec : 20.43 2020 - 07 - 02 17 : 17 : 19 , 701 epoch 42 - iter 30 / 154 - loss 0.11943454 - samples / sec : 22.89 2020 - 07 - 02 17 : 17 : 39 , 395 epoch 42 - iter 45 / 154 - loss 0.11384823 - samples / sec : 24.79 2020 - 07 - 02 17 : 17 : 58 , 643 epoch 42 - iter 60 / 154 - loss 0.11443645 - samples / sec : 25.09 2020 - 07 - 02 17 : 18 : 18 , 648 epoch 42 - iter 75 / 154 - loss 0.11680269 - samples / sec : 24.13 2020 - 07 - 02 17 : 18 : 38 , 267 epoch 42 - iter 90 / 154 - loss 0.12064875 - samples / sec : 24.82 2020 - 07 - 02 17 : 18 : 58 , 371 epoch 42 - iter 105 / 154 - loss 0.12379217 - samples / sec : 24.01 2020 - 07 - 02 17 : 19 : 19 , 252 epoch 42 - iter 120 / 154 - loss 0.12142454 - samples / sec : 23.12 2020 - 07 - 02 17 : 19 : 37 , 911 epoch 42 - iter 135 / 154 - loss 0.12138695 - samples / sec : 25.90 2020 - 07 - 02 17 : 19 : 57 , 683 epoch 42 - iter 150 / 154 - loss 0.12124144 - samples / sec : 24.42 2020 - 07 - 02 17 : 20 : 02 , 818 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 20 : 02 , 819 EPOCH 42 done : loss 0.1232 - lr 0.0081091 2020 - 07 - 02 17 : 20 : 25 , 643 DEV : loss 0.228939026594162 - score 0.9749 2020 - 07 - 02 17 : 20 : 25 , 711 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 17 : 20 : 25 , 712 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 20 : 50 , 520 epoch 43 - iter 15 / 154 - loss 0.11688481 - samples / sec : 19.56 2020 - 07 - 02 17 : 21 : 09 , 983 epoch 43 - iter 30 / 154 - loss 0.11299946 - samples / sec : 24.82 2020 - 07 - 02 17 : 21 : 31 , 073 epoch 43 - iter 45 / 154 - loss 0.10471270 - samples / sec : 23.10 2020 - 07 - 02 17 : 21 : 50 , 293 epoch 43 - iter 60 / 154 - loss 0.11333607 - samples / sec : 25.13 2020 - 07 - 02 17 : 22 : 09 , 299 epoch 43 - iter 75 / 154 - loss 0.10945214 - samples / sec : 25.40 2020 - 07 - 02 17 : 22 : 29 , 846 epoch 43 - iter 90 / 154 - loss 0.10823468 - samples / sec : 23.49 2020 - 07 - 02 17 : 22 : 48 , 260 epoch 43 - iter 105 / 154 - loss 0.11234212 - samples / sec : 26.24 2020 - 07 - 02 17 : 23 : 08 , 351 epoch 43 - iter 120 / 154 - loss 0.11211128 - samples / sec : 24.03 2020 - 07 - 02 17 : 23 : 28 , 918 epoch 43 - iter 135 / 154 - loss 0.11339033 - samples / sec : 23.64 2020 - 07 - 02 17 : 23 : 48 , 849 epoch 43 - iter 150 / 154 - loss 0.11390336 - samples / sec : 24.22 2020 - 07 - 02 17 : 23 : 53 , 484 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 23 : 53 , 486 EPOCH 43 done : loss 0.1160 - lr 0.0081091 2020 - 07 - 02 17 : 24 : 16 , 355 DEV : loss 0.20396070182323456 - score 0.978 2020 - 07 - 02 17 : 24 : 16 , 423 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 17 : 24 : 16 , 424 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 24 : 39 , 845 epoch 44 - iter 15 / 154 - loss 0.07580647 - samples / sec : 20.76 2020 - 07 - 02 17 : 24 : 58 , 766 epoch 44 - iter 30 / 154 - loss 0.08805038 - samples / sec : 25.54 2020 - 07 - 02 17 : 25 : 19 , 081 epoch 44 - iter 45 / 154 - loss 0.08704735 - samples / sec : 23.96 2020 - 07 - 02 17 : 25 : 38 , 923 epoch 44 - iter 60 / 154 - loss 0.10597141 - samples / sec : 24.35 2020 - 07 - 02 17 : 25 : 57 , 992 epoch 44 - iter 75 / 154 - loss 0.10900353 - samples / sec : 25.31 2020 - 07 - 02 17 : 26 : 18 , 083 epoch 44 - iter 90 / 154 - loss 0.11724408 - samples / sec : 24.22 2020 - 07 - 02 17 : 26 : 36 , 721 epoch 44 - iter 105 / 154 - loss 0.11579082 - samples / sec : 25.90 2020 - 07 - 02 17 : 26 : 57 , 300 epoch 44 - iter 120 / 154 - loss 0.11354713 - samples / sec : 23.63 2020 - 07 - 02 17 : 27 : 17 , 545 epoch 44 - iter 135 / 154 - loss 0.11238661 - samples / sec : 23.84 2020 - 07 - 02 17 : 27 : 37 , 533 epoch 44 - iter 150 / 154 - loss 0.11010026 - samples / sec : 24.15 2020 - 07 - 02 17 : 27 : 41 , 777 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 27 : 41 , 778 EPOCH 44 done : loss 0.1084 - lr 0.0081091 2020 - 07 - 02 17 : 28 : 04 , 480 DEV : loss 0.2126724123954773 - score 0.9786 2020 - 07 - 02 17 : 28 : 04 , 547 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 17 : 28 : 04 , 549 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 28 : 26 , 022 epoch 45 - iter 15 / 154 - loss 0.10441768 - samples / sec : 22.64 2020 - 07 - 02 17 : 28 : 46 , 731 epoch 45 - iter 30 / 154 - loss 0.11024101 - samples / sec : 23.31 2020 - 07 - 02 17 : 29 : 07 , 281 epoch 45 - iter 45 / 154 - loss 0.11836660 - samples / sec : 23.73 2020 - 07 - 02 17 : 29 : 26 , 386 epoch 45 - iter 60 / 154 - loss 0.11498991 - samples / sec : 25.27 2020 - 07 - 02 17 : 29 : 46 , 118 epoch 45 - iter 75 / 154 - loss 0.11378261 - samples / sec : 24.47 2020 - 07 - 02 17 : 30 : 05 , 930 epoch 45 - iter 90 / 154 - loss 0.11051767 - samples / sec : 24.54 2020 - 07 - 02 17 : 30 : 24 , 450 epoch 45 - iter 105 / 154 - loss 0.11882560 - samples / sec : 26.09 2020 - 07 - 02 17 : 30 : 43 , 922 epoch 45 - iter 120 / 154 - loss 0.11922019 - samples / sec : 24.79 2020 - 07 - 02 17 : 31 : 05 , 345 epoch 45 - iter 135 / 154 - loss 0.11827121 - samples / sec : 22.69 2020 - 07 - 02 17 : 31 : 24 , 900 epoch 45 - iter 150 / 154 - loss 0.11522523 - samples / sec : 24.68 2020 - 07 - 02 17 : 31 : 29 , 527 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 31 : 29 , 529 EPOCH 45 done : loss 0.1151 - lr 0.0081091 2020 - 07 - 02 17 : 31 : 52 , 285 DEV : loss 0.21013282239437103 - score 0.9774 2020 - 07 - 02 17 : 31 : 52 , 353 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 17 : 31 : 52 , 354 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 32 : 15 , 570 epoch 46 - iter 15 / 154 - loss 0.11027370 - samples / sec : 20.92 2020 - 07 - 02 17 : 32 : 35 , 288 epoch 46 - iter 30 / 154 - loss 0.10980776 - samples / sec : 24.50 2020 - 07 - 02 17 : 32 : 55 , 630 epoch 46 - iter 45 / 154 - loss 0.11028625 - samples / sec : 23.73 2020 - 07 - 02 17 : 33 : 16 , 437 epoch 46 - iter 60 / 154 - loss 0.11294564 - samples / sec : 23.20 2020 - 07 - 02 17 : 33 : 36 , 764 epoch 46 - iter 75 / 154 - loss 0.11088956 - samples / sec : 23.90 2020 - 07 - 02 17 : 33 : 56 , 137 epoch 46 - iter 90 / 154 - loss 0.11293679 - samples / sec : 24.93 2020 - 07 - 02 17 : 34 : 16 , 271 epoch 46 - iter 105 / 154 - loss 0.11380949 - samples / sec : 23.98 2020 - 07 - 02 17 : 34 : 36 , 209 epoch 46 - iter 120 / 154 - loss 0.11566317 - samples / sec : 24.22 2020 - 07 - 02 17 : 34 : 55 , 594 epoch 46 - iter 135 / 154 - loss 0.11391440 - samples / sec : 24.91 2020 - 07 - 02 17 : 35 : 14 , 821 epoch 46 - iter 150 / 154 - loss 0.11119683 - samples / sec : 25.29 2020 - 07 - 02 17 : 35 : 19 , 264 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 35 : 19 , 265 EPOCH 46 done : loss 0.1132 - lr 0.0081091 2020 - 07 - 02 17 : 35 : 41 , 738 DEV : loss 0.2473410964012146 - score 0.9768 Epoch 46 : reducing learning rate of group 0 to 4.0545e-03 . 2020 - 07 - 02 17 : 35 : 41 , 808 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 17 : 35 : 41 , 811 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 36 : 04 , 483 epoch 47 - iter 15 / 154 - loss 0.13017201 - samples / sec : 21.44 2020 - 07 - 02 17 : 36 : 25 , 431 epoch 47 - iter 30 / 154 - loss 0.11781174 - samples / sec : 23.27 2020 - 07 - 02 17 : 36 : 44 , 855 epoch 47 - iter 45 / 154 - loss 0.10631655 - samples / sec : 24.86 2020 - 07 - 02 17 : 37 : 03 , 628 epoch 47 - iter 60 / 154 - loss 0.09923212 - samples / sec : 25.72 2020 - 07 - 02 17 : 37 : 24 , 523 epoch 47 - iter 75 / 154 - loss 0.09882059 - samples / sec : 23.12 2020 - 07 - 02 17 : 37 : 44 , 449 epoch 47 - iter 90 / 154 - loss 0.10180322 - samples / sec : 24.21 2020 - 07 - 02 17 : 38 : 04 , 677 epoch 47 - iter 105 / 154 - loss 0.10014918 - samples / sec : 23.86 2020 - 07 - 02 17 : 38 : 24 , 080 epoch 47 - iter 120 / 154 - loss 0.10079072 - samples / sec : 25.07 2020 - 07 - 02 17 : 38 : 43 , 268 epoch 47 - iter 135 / 154 - loss 0.10352145 - samples / sec : 25.17 2020 - 07 - 02 17 : 39 : 03 , 680 epoch 47 - iter 150 / 154 - loss 0.10257754 - samples / sec : 23.81 2020 - 07 - 02 17 : 39 : 08 , 929 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 39 : 08 , 930 EPOCH 47 done : loss 0.1019 - lr 0.0040545 2020 - 07 - 02 17 : 39 : 31 , 341 DEV : loss 0.1972641497850418 - score 0.9798 2020 - 07 - 02 17 : 39 : 31 , 410 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 17 : 39 : 35 , 701 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 39 : 56 , 986 epoch 48 - iter 15 / 154 - loss 0.09316256 - samples / sec : 22.84 2020 - 07 - 02 17 : 40 : 18 , 519 epoch 48 - iter 30 / 154 - loss 0.09207635 - samples / sec : 22.64 2020 - 07 - 02 17 : 40 : 38 , 501 epoch 48 - iter 45 / 154 - loss 0.09080028 - samples / sec : 24.16 2020 - 07 - 02 17 : 40 : 59 , 896 epoch 48 - iter 60 / 154 - loss 0.09034919 - samples / sec : 22.56 2020 - 07 - 02 17 : 41 : 21 , 390 epoch 48 - iter 75 / 154 - loss 0.09863393 - samples / sec : 22.45 2020 - 07 - 02 17 : 41 : 40 , 885 epoch 48 - iter 90 / 154 - loss 0.09842956 - samples / sec : 24.96 2020 - 07 - 02 17 : 41 : 59 , 314 epoch 48 - iter 105 / 154 - loss 0.09847930 - samples / sec : 26.23 2020 - 07 - 02 17 : 42 : 18 , 382 epoch 48 - iter 120 / 154 - loss 0.10359679 - samples / sec : 25.32 2020 - 07 - 02 17 : 42 : 36 , 737 epoch 48 - iter 135 / 154 - loss 0.10533498 - samples / sec : 26.51 2020 - 07 - 02 17 : 42 : 56 , 401 epoch 48 - iter 150 / 154 - loss 0.10285178 - samples / sec : 24.54 2020 - 07 - 02 17 : 43 : 01 , 894 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 43 : 01 , 896 EPOCH 48 done : loss 0.1029 - lr 0.0040545 2020 - 07 - 02 17 : 43 : 24 , 565 DEV : loss 0.20908032357692719 - score 0.9774 2020 - 07 - 02 17 : 43 : 24 , 634 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 17 : 43 : 24 , 636 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 43 : 46 , 260 epoch 49 - iter 15 / 154 - loss 0.12452545 - samples / sec : 22.74 2020 - 07 - 02 17 : 44 : 06 , 872 epoch 49 - iter 30 / 154 - loss 0.11373051 - samples / sec : 23.41 2020 - 07 - 02 17 : 44 : 25 , 356 epoch 49 - iter 45 / 154 - loss 0.11127935 - samples / sec : 26.39 2020 - 07 - 02 17 : 44 : 45 , 722 epoch 49 - iter 60 / 154 - loss 0.10167709 - samples / sec : 23.70 2020 - 07 - 02 17 : 45 : 04 , 808 epoch 49 - iter 75 / 154 - loss 0.10067866 - samples / sec : 25.31 2020 - 07 - 02 17 : 45 : 25 , 368 epoch 49 - iter 90 / 154 - loss 0.10211510 - samples / sec : 23.65 2020 - 07 - 02 17 : 45 : 45 , 284 epoch 49 - iter 105 / 154 - loss 0.09828637 - samples / sec : 24.24 2020 - 07 - 02 17 : 46 : 07 , 360 epoch 49 - iter 120 / 154 - loss 0.10255460 - samples / sec : 21.85 2020 - 07 - 02 17 : 46 : 28 , 038 epoch 49 - iter 135 / 154 - loss 0.10150222 - samples / sec : 23.51 2020 - 07 - 02 17 : 46 : 47 , 340 epoch 49 - iter 150 / 154 - loss 0.10494432 - samples / sec : 25.01 2020 - 07 - 02 17 : 46 : 51 , 762 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 46 : 51 , 764 EPOCH 49 done : loss 0.1036 - lr 0.0040545 2020 - 07 - 02 17 : 47 : 14 , 698 DEV : loss 0.21484945714473724 - score 0.978 2020 - 07 - 02 17 : 47 : 14 , 767 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 17 : 47 : 14 , 768 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 47 : 37 , 461 epoch 50 - iter 15 / 154 - loss 0.07441427 - samples / sec : 21.41 2020 - 07 - 02 17 : 47 : 57 , 684 epoch 50 - iter 30 / 154 - loss 0.09981060 - samples / sec : 23.88 2020 - 07 - 02 17 : 48 : 17 , 556 epoch 50 - iter 45 / 154 - loss 0.09582326 - samples / sec : 24.54 2020 - 07 - 02 17 : 48 : 37 , 096 epoch 50 - iter 60 / 154 - loss 0.09657856 - samples / sec : 24.71 2020 - 07 - 02 17 : 48 : 57 , 920 epoch 50 - iter 75 / 154 - loss 0.09897010 - samples / sec : 23.19 2020 - 07 - 02 17 : 49 : 17 , 314 epoch 50 - iter 90 / 154 - loss 0.09652073 - samples / sec : 25.10 2020 - 07 - 02 17 : 49 : 36 , 071 epoch 50 - iter 105 / 154 - loss 0.09323545 - samples / sec : 25.75 2020 - 07 - 02 17 : 49 : 56 , 835 epoch 50 - iter 120 / 154 - loss 0.09363949 - samples / sec : 23.24 2020 - 07 - 02 17 : 50 : 16 , 600 epoch 50 - iter 135 / 154 - loss 0.09351738 - samples / sec : 24.60 2020 - 07 - 02 17 : 50 : 37 , 072 epoch 50 - iter 150 / 154 - loss 0.09631807 - samples / sec : 23.58 2020 - 07 - 02 17 : 50 : 41 , 535 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 50 : 41 , 536 EPOCH 50 done : loss 0.0959 - lr 0.0040545 2020 - 07 - 02 17 : 51 : 04 , 378 DEV : loss 0.22444875538349152 - score 0.9792 2020 - 07 - 02 17 : 51 : 04 , 446 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 17 : 51 : 04 , 448 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 51 : 26 , 503 epoch 51 - iter 15 / 154 - loss 0.08688969 - samples / sec : 22.04 2020 - 07 - 02 17 : 51 : 45 , 925 epoch 51 - iter 30 / 154 - loss 0.09203181 - samples / sec : 24.88 2020 - 07 - 02 17 : 52 : 06 , 814 epoch 51 - iter 45 / 154 - loss 0.10646763 - samples / sec : 23.31 2020 - 07 - 02 17 : 52 : 26 , 057 epoch 51 - iter 60 / 154 - loss 0.11192265 - samples / sec : 25.09 2020 - 07 - 02 17 : 52 : 45 , 627 epoch 51 - iter 75 / 154 - loss 0.11106363 - samples / sec : 24.69 2020 - 07 - 02 17 : 53 : 06 , 167 epoch 51 - iter 90 / 154 - loss 0.11052981 - samples / sec : 23.63 2020 - 07 - 02 17 : 53 : 26 , 731 epoch 51 - iter 105 / 154 - loss 0.10707935 - samples / sec : 23.47 2020 - 07 - 02 17 : 53 : 46 , 864 epoch 51 - iter 120 / 154 - loss 0.10218134 - samples / sec : 23.97 2020 - 07 - 02 17 : 54 : 05 , 931 epoch 51 - iter 135 / 154 - loss 0.10484334 - samples / sec : 25.52 2020 - 07 - 02 17 : 54 : 25 , 902 epoch 51 - iter 150 / 154 - loss 0.10450326 - samples / sec : 24.18 2020 - 07 - 02 17 : 54 : 30 , 340 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 54 : 30 , 341 EPOCH 51 done : loss 0.1029 - lr 0.0040545 2020 - 07 - 02 17 : 54 : 53 , 014 DEV : loss 0.2026711255311966 - score 0.9792 2020 - 07 - 02 17 : 54 : 53 , 084 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 17 : 54 : 53 , 085 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 55 : 16 , 198 epoch 52 - iter 15 / 154 - loss 0.09530740 - samples / sec : 21.03 2020 - 07 - 02 17 : 55 : 38 , 136 epoch 52 - iter 30 / 154 - loss 0.10926712 - samples / sec : 22.01 2020 - 07 - 02 17 : 55 : 59 , 339 epoch 52 - iter 45 / 154 - loss 0.10953697 - samples / sec : 22.97 2020 - 07 - 02 17 : 56 : 18 , 647 epoch 52 - iter 60 / 154 - loss 0.11040190 - samples / sec : 25.01 2020 - 07 - 02 17 : 56 : 36 , 716 epoch 52 - iter 75 / 154 - loss 0.10400084 - samples / sec : 26.74 2020 - 07 - 02 17 : 56 : 56 , 401 epoch 52 - iter 90 / 154 - loss 0.10083394 - samples / sec : 24.69 2020 - 07 - 02 17 : 57 : 15 , 759 epoch 52 - iter 105 / 154 - loss 0.10321290 - samples / sec : 24.94 2020 - 07 - 02 17 : 57 : 35 , 739 epoch 52 - iter 120 / 154 - loss 0.09946122 - samples / sec : 24.33 2020 - 07 - 02 17 : 57 : 54 , 736 epoch 52 - iter 135 / 154 - loss 0.09763378 - samples / sec : 25.41 2020 - 07 - 02 17 : 58 : 13 , 939 epoch 52 - iter 150 / 154 - loss 0.09765992 - samples / sec : 25.15 2020 - 07 - 02 17 : 58 : 18 , 133 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 58 : 18 , 135 EPOCH 52 done : loss 0.0961 - lr 0.0040545 2020 - 07 - 02 17 : 58 : 40 , 895 DEV : loss 0.1978662759065628 - score 0.9792 2020 - 07 - 02 17 : 58 : 40 , 965 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 17 : 58 : 40 , 968 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 59 : 03 , 263 epoch 53 - iter 15 / 154 - loss 0.10081433 - samples / sec : 21.80 2020 - 07 - 02 17 : 59 : 23 , 122 epoch 53 - iter 30 / 154 - loss 0.12010567 - samples / sec : 24.33 2020 - 07 - 02 17 : 59 : 43 , 761 epoch 53 - iter 45 / 154 - loss 0.11009379 - samples / sec : 23.41 2020 - 07 - 02 18 : 00 : 03 , 389 epoch 53 - iter 60 / 154 - loss 0.11138892 - samples / sec : 24.61 2020 - 07 - 02 18 : 00 : 22 , 303 epoch 53 - iter 75 / 154 - loss 0.10890932 - samples / sec : 25.56 2020 - 07 - 02 18 : 00 : 42 , 664 epoch 53 - iter 90 / 154 - loss 0.10678213 - samples / sec : 23.72 2020 - 07 - 02 18 : 01 : 04 , 707 epoch 53 - iter 105 / 154 - loss 0.10528137 - samples / sec : 22.06 2020 - 07 - 02 18 : 01 : 25 , 323 epoch 53 - iter 120 / 154 - loss 0.10567519 - samples / sec : 23.42 2020 - 07 - 02 18 : 01 : 43 , 823 epoch 53 - iter 135 / 154 - loss 0.10170225 - samples / sec : 26.11 2020 - 07 - 02 18 : 02 : 03 , 223 epoch 53 - iter 150 / 154 - loss 0.09834855 - samples / sec : 25.06 2020 - 07 - 02 18 : 02 : 07 , 660 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 02 : 07 , 661 EPOCH 53 done : loss 0.0981 - lr 0.0040545 2020 - 07 - 02 18 : 02 : 30 , 200 DEV : loss 0.20592159032821655 - score 0.9774 Epoch 53 : reducing learning rate of group 0 to 2.0273e-03 . 2020 - 07 - 02 18 : 02 : 30 , 275 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 18 : 02 : 30 , 277 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 02 : 50 , 800 epoch 54 - iter 15 / 154 - loss 0.08718331 - samples / sec : 23.70 2020 - 07 - 02 18 : 03 : 10 , 995 epoch 54 - iter 30 / 154 - loss 0.09210472 - samples / sec : 24.14 2020 - 07 - 02 18 : 03 : 31 , 577 epoch 54 - iter 45 / 154 - loss 0.10180214 - samples / sec : 23.46 2020 - 07 - 02 18 : 03 : 51 , 411 epoch 54 - iter 60 / 154 - loss 0.09472846 - samples / sec : 24.33 2020 - 07 - 02 18 : 04 : 11 , 247 epoch 54 - iter 75 / 154 - loss 0.08983106 - samples / sec : 24.35 2020 - 07 - 02 18 : 04 : 30 , 488 epoch 54 - iter 90 / 154 - loss 0.09004916 - samples / sec : 25.09 2020 - 07 - 02 18 : 04 : 49 , 388 epoch 54 - iter 105 / 154 - loss 0.08870099 - samples / sec : 25.73 2020 - 07 - 02 18 : 05 : 09 , 785 epoch 54 - iter 120 / 154 - loss 0.08760914 - samples / sec : 23.68 2020 - 07 - 02 18 : 05 : 30 , 333 epoch 54 - iter 135 / 154 - loss 0.08641136 - samples / sec : 23.48 2020 - 07 - 02 18 : 05 : 49 , 789 epoch 54 - iter 150 / 154 - loss 0.08918157 - samples / sec : 24.99 2020 - 07 - 02 18 : 05 : 54 , 518 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 05 : 54 , 520 EPOCH 54 done : loss 0.0910 - lr 0.0020273 2020 - 07 - 02 18 : 06 : 16 , 972 DEV : loss 0.19774390757083893 - score 0.9798 2020 - 07 - 02 18 : 06 : 17 , 041 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 18 : 06 : 17 , 042 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 06 : 39 , 573 epoch 55 - iter 15 / 154 - loss 0.08008183 - samples / sec : 21.79 2020 - 07 - 02 18 : 06 : 59 , 164 epoch 55 - iter 30 / 154 - loss 0.08933909 - samples / sec : 24.64 2020 - 07 - 02 18 : 07 : 18 , 806 epoch 55 - iter 45 / 154 - loss 0.09008065 - samples / sec : 24.58 2020 - 07 - 02 18 : 07 : 40 , 535 epoch 55 - iter 60 / 154 - loss 0.08874504 - samples / sec : 22.36 2020 - 07 - 02 18 : 08 : 00 , 201 epoch 55 - iter 75 / 154 - loss 0.08941119 - samples / sec : 24.56 2020 - 07 - 02 18 : 08 : 20 , 983 epoch 55 - iter 90 / 154 - loss 0.08812096 - samples / sec : 23.22 2020 - 07 - 02 18 : 08 : 40 , 361 epoch 55 - iter 105 / 154 - loss 0.08716224 - samples / sec : 25.10 2020 - 07 - 02 18 : 09 : 00 , 385 epoch 55 - iter 120 / 154 - loss 0.08778634 - samples / sec : 24.11 2020 - 07 - 02 18 : 09 : 19 , 225 epoch 55 - iter 135 / 154 - loss 0.08927797 - samples / sec : 25.64 2020 - 07 - 02 18 : 09 : 38 , 592 epoch 55 - iter 150 / 154 - loss 0.09444317 - samples / sec : 25.09 2020 - 07 - 02 18 : 09 : 43 , 560 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 09 : 43 , 561 EPOCH 55 done : loss 0.0934 - lr 0.0020273 2020 - 07 - 02 18 : 10 : 05 , 988 DEV : loss 0.19739288091659546 - score 0.978 2020 - 07 - 02 18 : 10 : 06 , 059 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 18 : 10 : 06 , 061 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 10 : 28 , 412 epoch 56 - iter 15 / 154 - loss 0.08788449 - samples / sec : 21.74 2020 - 07 - 02 18 : 10 : 46 , 877 epoch 56 - iter 30 / 154 - loss 0.08437797 - samples / sec : 26.16 2020 - 07 - 02 18 : 11 : 06 , 048 epoch 56 - iter 45 / 154 - loss 0.08514273 - samples / sec : 25.19 2020 - 07 - 02 18 : 11 : 25 , 647 epoch 56 - iter 60 / 154 - loss 0.08036039 - samples / sec : 24.79 2020 - 07 - 02 18 : 11 : 45 , 286 epoch 56 - iter 75 / 154 - loss 0.08442916 - samples / sec : 24.59 2020 - 07 - 02 18 : 12 : 06 , 783 epoch 56 - iter 90 / 154 - loss 0.08954436 - samples / sec : 22.44 2020 - 07 - 02 18 : 12 : 27 , 649 epoch 56 - iter 105 / 154 - loss 0.08976238 - samples / sec : 23.30 2020 - 07 - 02 18 : 12 : 46 , 342 epoch 56 - iter 120 / 154 - loss 0.09411574 - samples / sec : 25.84 2020 - 07 - 02 18 : 13 : 05 , 596 epoch 56 - iter 135 / 154 - loss 0.09723895 - samples / sec : 25.09 2020 - 07 - 02 18 : 13 : 24 , 877 epoch 56 - iter 150 / 154 - loss 0.09649601 - samples / sec : 25.22 2020 - 07 - 02 18 : 13 : 29 , 371 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 13 : 29 , 373 EPOCH 56 done : loss 0.0952 - lr 0.0020273 2020 - 07 - 02 18 : 13 : 51 , 797 DEV : loss 0.20011773705482483 - score 0.9786 2020 - 07 - 02 18 : 13 : 51 , 866 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 18 : 13 : 51 , 867 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 14 : 12 , 911 epoch 57 - iter 15 / 154 - loss 0.08282442 - samples / sec : 23.12 2020 - 07 - 02 18 : 14 : 32 , 947 epoch 57 - iter 30 / 154 - loss 0.09025696 - samples / sec : 24.10 2020 - 07 - 02 18 : 14 : 53 , 386 epoch 57 - iter 45 / 154 - loss 0.09394956 - samples / sec : 23.61 2020 - 07 - 02 18 : 15 : 13 , 913 epoch 57 - iter 60 / 154 - loss 0.09126146 - samples / sec : 23.66 2020 - 07 - 02 18 : 15 : 32 , 907 epoch 57 - iter 75 / 154 - loss 0.09113265 - samples / sec : 25.43 2020 - 07 - 02 18 : 15 : 52 , 413 epoch 57 - iter 90 / 154 - loss 0.08752083 - samples / sec : 24.77 2020 - 07 - 02 18 : 16 : 14 , 762 epoch 57 - iter 105 / 154 - loss 0.09105406 - samples / sec : 21.73 2020 - 07 - 02 18 : 16 : 34 , 872 epoch 57 - iter 120 / 154 - loss 0.08909690 - samples / sec : 24.00 2020 - 07 - 02 18 : 16 : 54 , 106 epoch 57 - iter 135 / 154 - loss 0.08767471 - samples / sec : 25.10 2020 - 07 - 02 18 : 17 : 13 , 265 epoch 57 - iter 150 / 154 - loss 0.08853321 - samples / sec : 25.40 2020 - 07 - 02 18 : 17 : 17 , 660 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 17 : 17 , 661 EPOCH 57 done : loss 0.0898 - lr 0.0020273 2020 - 07 - 02 18 : 17 : 40 , 137 DEV : loss 0.1950114369392395 - score 0.9804 2020 - 07 - 02 18 : 17 : 40 , 206 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 18 : 17 : 44 , 473 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 18 : 05 , 641 epoch 58 - iter 15 / 154 - loss 0.09854108 - samples / sec : 23.25 2020 - 07 - 02 18 : 18 : 26 , 107 epoch 58 - iter 30 / 154 - loss 0.09823707 - samples / sec : 23.59 2020 - 07 - 02 18 : 18 : 44 , 691 epoch 58 - iter 45 / 154 - loss 0.09728588 - samples / sec : 26.01 2020 - 07 - 02 18 : 19 : 05 , 368 epoch 58 - iter 60 / 154 - loss 0.09077503 - samples / sec : 23.50 2020 - 07 - 02 18 : 19 : 25 , 551 epoch 58 - iter 75 / 154 - loss 0.09195299 - samples / sec : 23.93 2020 - 07 - 02 18 : 19 : 46 , 168 epoch 58 - iter 90 / 154 - loss 0.08965277 - samples / sec : 23.41 2020 - 07 - 02 18 : 20 : 07 , 091 epoch 58 - iter 105 / 154 - loss 0.08772397 - samples / sec : 23.22 2020 - 07 - 02 18 : 20 : 25 , 531 epoch 58 - iter 120 / 154 - loss 0.08932425 - samples / sec : 26.19 2020 - 07 - 02 18 : 20 : 44 , 199 epoch 58 - iter 135 / 154 - loss 0.09028912 - samples / sec : 25.88 2020 - 07 - 02 18 : 21 : 04 , 364 epoch 58 - iter 150 / 154 - loss 0.09111484 - samples / sec : 24.11 2020 - 07 - 02 18 : 21 : 09 , 689 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 21 : 09 , 691 EPOCH 58 done : loss 0.0919 - lr 0.0020273 2020 - 07 - 02 18 : 21 : 32 , 360 DEV : loss 0.19827120006084442 - score 0.9798 2020 - 07 - 02 18 : 21 : 32 , 428 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 18 : 21 : 32 , 429 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 21 : 53 , 186 epoch 59 - iter 15 / 154 - loss 0.07434669 - samples / sec : 23.43 2020 - 07 - 02 18 : 22 : 13 , 121 epoch 59 - iter 30 / 154 - loss 0.08210114 - samples / sec : 24.22 2020 - 07 - 02 18 : 22 : 32 , 288 epoch 59 - iter 45 / 154 - loss 0.08505506 - samples / sec : 25.19 2020 - 07 - 02 18 : 22 : 52 , 595 epoch 59 - iter 60 / 154 - loss 0.08686025 - samples / sec : 23.79 2020 - 07 - 02 18 : 23 : 11 , 985 epoch 59 - iter 75 / 154 - loss 0.08708367 - samples / sec : 25.06 2020 - 07 - 02 18 : 23 : 32 , 762 epoch 59 - iter 90 / 154 - loss 0.09318458 - samples / sec : 23.24 2020 - 07 - 02 18 : 23 : 54 , 143 epoch 59 - iter 105 / 154 - loss 0.09390901 - samples / sec : 22.56 2020 - 07 - 02 18 : 24 : 15 , 226 epoch 59 - iter 120 / 154 - loss 0.09243909 - samples / sec : 23.06 2020 - 07 - 02 18 : 24 : 34 , 418 epoch 59 - iter 135 / 154 - loss 0.09128275 - samples / sec : 25.17 2020 - 07 - 02 18 : 24 : 54 , 375 epoch 59 - iter 150 / 154 - loss 0.09075951 - samples / sec : 24.19 2020 - 07 - 02 18 : 24 : 58 , 631 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 24 : 58 , 632 EPOCH 59 done : loss 0.0899 - lr 0.0020273 2020 - 07 - 02 18 : 25 : 21 , 438 DEV : loss 0.2027978152036667 - score 0.9804 2020 - 07 - 02 18 : 25 : 21 , 509 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 18 : 25 : 21 , 511 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 25 : 43 , 241 epoch 60 - iter 15 / 154 - loss 0.08677354 - samples / sec : 22.39 2020 - 07 - 02 18 : 26 : 02 , 464 epoch 60 - iter 30 / 154 - loss 0.08324124 - samples / sec : 25.13 2020 - 07 - 02 18 : 26 : 22 , 590 epoch 60 - iter 45 / 154 - loss 0.08803943 - samples / sec : 23.99 2020 - 07 - 02 18 : 26 : 43 , 159 epoch 60 - iter 60 / 154 - loss 0.09347135 - samples / sec : 23.47 2020 - 07 - 02 18 : 27 : 03 , 283 epoch 60 - iter 75 / 154 - loss 0.09846938 - samples / sec : 23.98 2020 - 07 - 02 18 : 27 : 23 , 185 epoch 60 - iter 90 / 154 - loss 0.09680327 - samples / sec : 24.42 2020 - 07 - 02 18 : 27 : 42 , 460 epoch 60 - iter 105 / 154 - loss 0.09433226 - samples / sec : 25.05 2020 - 07 - 02 18 : 28 : 02 , 370 epoch 60 - iter 120 / 154 - loss 0.09501102 - samples / sec : 24.25 2020 - 07 - 02 18 : 28 : 22 , 500 epoch 60 - iter 135 / 154 - loss 0.09191312 - samples / sec : 24.15 2020 - 07 - 02 18 : 28 : 43 , 973 epoch 60 - iter 150 / 154 - loss 0.09032161 - samples / sec : 22.47 2020 - 07 - 02 18 : 28 : 48 , 579 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 28 : 48 , 581 EPOCH 60 done : loss 0.0893 - lr 0.0020273 2020 - 07 - 02 18 : 29 : 11 , 275 DEV : loss 0.20585915446281433 - score 0.981 2020 - 07 - 02 18 : 29 : 11 , 343 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 18 : 29 : 15 , 614 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 29 : 38 , 156 epoch 61 - iter 15 / 154 - loss 0.09983344 - samples / sec : 21.55 2020 - 07 - 02 18 : 29 : 58 , 238 epoch 61 - iter 30 / 154 - loss 0.08600884 - samples / sec : 24.05 2020 - 07 - 02 18 : 30 : 17 , 840 epoch 61 - iter 45 / 154 - loss 0.08722653 - samples / sec : 24.88 2020 - 07 - 02 18 : 30 : 38 , 254 epoch 61 - iter 60 / 154 - loss 0.09154642 - samples / sec : 23.66 2020 - 07 - 02 18 : 30 : 58 , 140 epoch 61 - iter 75 / 154 - loss 0.09060006 - samples / sec : 24.27 2020 - 07 - 02 18 : 31 : 18 , 830 epoch 61 - iter 90 / 154 - loss 0.09021203 - samples / sec : 23.49 2020 - 07 - 02 18 : 31 : 39 , 009 epoch 61 - iter 105 / 154 - loss 0.09414779 - samples / sec : 23.92 2020 - 07 - 02 18 : 31 : 57 , 965 epoch 61 - iter 120 / 154 - loss 0.09225592 - samples / sec : 25.48 2020 - 07 - 02 18 : 32 : 16 , 358 epoch 61 - iter 135 / 154 - loss 0.08870085 - samples / sec : 26.26 2020 - 07 - 02 18 : 32 : 36 , 767 epoch 61 - iter 150 / 154 - loss 0.08851716 - samples / sec : 23.65 2020 - 07 - 02 18 : 32 : 41 , 348 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 32 : 41 , 350 EPOCH 61 done : loss 0.0875 - lr 0.0020273 2020 - 07 - 02 18 : 33 : 04 , 158 DEV : loss 0.19547177851200104 - score 0.9792 2020 - 07 - 02 18 : 33 : 04 , 227 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 18 : 33 : 04 , 229 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 33 : 27 , 258 epoch 62 - iter 15 / 154 - loss 0.08993260 - samples / sec : 21.10 2020 - 07 - 02 18 : 33 : 46 , 125 epoch 62 - iter 30 / 154 - loss 0.09509450 - samples / sec : 25.61 2020 - 07 - 02 18 : 34 : 06 , 236 epoch 62 - iter 45 / 154 - loss 0.08554634 - samples / sec : 24.25 2020 - 07 - 02 18 : 34 : 26 , 283 epoch 62 - iter 60 / 154 - loss 0.08996390 - samples / sec : 24.11 2020 - 07 - 02 18 : 34 : 45 , 709 epoch 62 - iter 75 / 154 - loss 0.09672508 - samples / sec : 24.85 2020 - 07 - 02 18 : 35 : 06 , 660 epoch 62 - iter 90 / 154 - loss 0.09301235 - samples / sec : 23.18 2020 - 07 - 02 18 : 35 : 26 , 352 epoch 62 - iter 105 / 154 - loss 0.09545991 - samples / sec : 24.52 2020 - 07 - 02 18 : 35 : 46 , 774 epoch 62 - iter 120 / 154 - loss 0.09430194 - samples / sec : 23.64 2020 - 07 - 02 18 : 36 : 05 , 602 epoch 62 - iter 135 / 154 - loss 0.09519594 - samples / sec : 25.83 2020 - 07 - 02 18 : 36 : 24 , 706 epoch 62 - iter 150 / 154 - loss 0.09227619 - samples / sec : 25.27 2020 - 07 - 02 18 : 36 : 28 , 841 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 36 : 28 , 843 EPOCH 62 done : loss 0.0933 - lr 0.0020273 2020 - 07 - 02 18 : 36 : 51 , 561 DEV : loss 0.1974499672651291 - score 0.9798 2020 - 07 - 02 18 : 36 : 51 , 627 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 18 : 36 : 51 , 628 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 37 : 13 , 264 epoch 63 - iter 15 / 154 - loss 0.08186235 - samples / sec : 22.46 2020 - 07 - 02 18 : 37 : 33 , 014 epoch 63 - iter 30 / 154 - loss 0.08035179 - samples / sec : 24.49 2020 - 07 - 02 18 : 37 : 51 , 954 epoch 63 - iter 45 / 154 - loss 0.09017727 - samples / sec : 25.72 2020 - 07 - 02 18 : 38 : 11 , 628 epoch 63 - iter 60 / 154 - loss 0.08745349 - samples / sec : 24.55 2020 - 07 - 02 18 : 38 : 31 , 615 epoch 63 - iter 75 / 154 - loss 0.08307302 - samples / sec : 24.16 2020 - 07 - 02 18 : 38 : 52 , 911 epoch 63 - iter 90 / 154 - loss 0.08419990 - samples / sec : 22.67 2020 - 07 - 02 18 : 39 : 14 , 286 epoch 63 - iter 105 / 154 - loss 0.08808264 - samples / sec : 22.57 2020 - 07 - 02 18 : 39 : 33 , 426 epoch 63 - iter 120 / 154 - loss 0.08803327 - samples / sec : 25.24 2020 - 07 - 02 18 : 39 : 53 , 125 epoch 63 - iter 135 / 154 - loss 0.08664566 - samples / sec : 24.65 2020 - 07 - 02 18 : 40 : 12 , 487 epoch 63 - iter 150 / 154 - loss 0.08714323 - samples / sec : 24.93 2020 - 07 - 02 18 : 40 : 16 , 662 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 40 : 16 , 663 EPOCH 63 done : loss 0.0865 - lr 0.0020273 2020 - 07 - 02 18 : 40 : 39 , 398 DEV : loss 0.2033492624759674 - score 0.9804 2020 - 07 - 02 18 : 40 : 39 , 466 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 18 : 40 : 39 , 467 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 41 : 01 , 377 epoch 64 - iter 15 / 154 - loss 0.06153077 - samples / sec : 22.19 2020 - 07 - 02 18 : 41 : 21 , 147 epoch 64 - iter 30 / 154 - loss 0.07140551 - samples / sec : 24.44 2020 - 07 - 02 18 : 41 : 42 , 317 epoch 64 - iter 45 / 154 - loss 0.08376528 - samples / sec : 23.00 2020 - 07 - 02 18 : 42 : 01 , 321 epoch 64 - iter 60 / 154 - loss 0.08542068 - samples / sec : 25.43 2020 - 07 - 02 18 : 42 : 19 , 752 epoch 64 - iter 75 / 154 - loss 0.08507289 - samples / sec : 26.20 2020 - 07 - 02 18 : 42 : 39 , 277 epoch 64 - iter 90 / 154 - loss 0.08492970 - samples / sec : 24.89 2020 - 07 - 02 18 : 43 : 00 , 022 epoch 64 - iter 105 / 154 - loss 0.08645396 - samples / sec : 23.26 2020 - 07 - 02 18 : 43 : 20 , 185 epoch 64 - iter 120 / 154 - loss 0.08791573 - samples / sec : 23.95 2020 - 07 - 02 18 : 43 : 37 , 747 epoch 64 - iter 135 / 154 - loss 0.08794287 - samples / sec : 27.71 2020 - 07 - 02 18 : 43 : 57 , 834 epoch 64 - iter 150 / 154 - loss 0.09194450 - samples / sec : 24.06 2020 - 07 - 02 18 : 44 : 02 , 897 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 44 : 02 , 898 EPOCH 64 done : loss 0.0916 - lr 0.0020273 2020 - 07 - 02 18 : 44 : 25 , 487 DEV : loss 0.19688229262828827 - score 0.9792 2020 - 07 - 02 18 : 44 : 25 , 554 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 18 : 44 : 25 , 555 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 44 : 46 , 224 epoch 65 - iter 15 / 154 - loss 0.10502815 - samples / sec : 23.54 2020 - 07 - 02 18 : 45 : 07 , 083 epoch 65 - iter 30 / 154 - loss 0.10161052 - samples / sec : 23.14 2020 - 07 - 02 18 : 45 : 27 , 173 epoch 65 - iter 45 / 154 - loss 0.10180355 - samples / sec : 24.02 2020 - 07 - 02 18 : 45 : 46 , 943 epoch 65 - iter 60 / 154 - loss 0.10438172 - samples / sec : 24.42 2020 - 07 - 02 18 : 46 : 07 , 489 epoch 65 - iter 75 / 154 - loss 0.10476604 - samples / sec : 23.64 2020 - 07 - 02 18 : 46 : 27 , 512 epoch 65 - iter 90 / 154 - loss 0.09832354 - samples / sec : 24.11 2020 - 07 - 02 18 : 46 : 46 , 517 epoch 65 - iter 105 / 154 - loss 0.09817031 - samples / sec : 25.42 2020 - 07 - 02 18 : 47 : 07 , 649 epoch 65 - iter 120 / 154 - loss 0.09402602 - samples / sec : 23.01 2020 - 07 - 02 18 : 47 : 25 , 938 epoch 65 - iter 135 / 154 - loss 0.09160565 - samples / sec : 26.42 2020 - 07 - 02 18 : 47 : 47 , 747 epoch 65 - iter 150 / 154 - loss 0.09144586 - samples / sec : 22.14 2020 - 07 - 02 18 : 47 : 52 , 233 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 47 : 52 , 234 EPOCH 65 done : loss 0.0905 - lr 0.0020273 2020 - 07 - 02 18 : 48 : 14 , 454 DEV : loss 0.20111960172653198 - score 0.9804 2020 - 07 - 02 18 : 48 : 14 , 668 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 18 : 48 : 14 , 670 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 48 : 36 , 913 epoch 66 - iter 15 / 154 - loss 0.07612641 - samples / sec : 21.87 2020 - 07 - 02 18 : 48 : 56 , 241 epoch 66 - iter 30 / 154 - loss 0.08649506 - samples / sec : 24.99 2020 - 07 - 02 18 : 49 : 16 , 092 epoch 66 - iter 45 / 154 - loss 0.08143805 - samples / sec : 24.53 2020 - 07 - 02 18 : 49 : 36 , 784 epoch 66 - iter 60 / 154 - loss 0.08196701 - samples / sec : 23.33 2020 - 07 - 02 18 : 49 : 58 , 046 epoch 66 - iter 75 / 154 - loss 0.08096401 - samples / sec : 22.71 2020 - 07 - 02 18 : 50 : 17 , 695 epoch 66 - iter 90 / 154 - loss 0.07823467 - samples / sec : 24.58 2020 - 07 - 02 18 : 50 : 36 , 144 epoch 66 - iter 105 / 154 - loss 0.08322996 - samples / sec : 26.39 2020 - 07 - 02 18 : 50 : 55 , 814 epoch 66 - iter 120 / 154 - loss 0.08491590 - samples / sec : 24.56 2020 - 07 - 02 18 : 51 : 16 , 722 epoch 66 - iter 135 / 154 - loss 0.08727617 - samples / sec : 23.10 2020 - 07 - 02 18 : 51 : 35 , 800 epoch 66 - iter 150 / 154 - loss 0.08726892 - samples / sec : 25.32 2020 - 07 - 02 18 : 51 : 40 , 273 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 51 : 40 , 275 EPOCH 66 done : loss 0.0883 - lr 0.0020273 2020 - 07 - 02 18 : 52 : 02 , 781 DEV : loss 0.19793111085891724 - score 0.9798 Epoch 66 : reducing learning rate of group 0 to 1.0136e-03 . 2020 - 07 - 02 18 : 52 : 02 , 851 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 18 : 52 : 02 , 853 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 52 : 27 , 895 epoch 67 - iter 15 / 154 - loss 0.12642477 - samples / sec : 19.38 2020 - 07 - 02 18 : 52 : 47 , 763 epoch 67 - iter 30 / 154 - loss 0.09747878 - samples / sec : 24.54 2020 - 07 - 02 18 : 53 : 07 , 080 epoch 67 - iter 45 / 154 - loss 0.08789005 - samples / sec : 25.00 2020 - 07 - 02 18 : 53 : 26 , 735 epoch 67 - iter 60 / 154 - loss 0.08494026 - samples / sec : 24.57 2020 - 07 - 02 18 : 53 : 46 , 315 epoch 67 - iter 75 / 154 - loss 0.08639383 - samples / sec : 24.67 2020 - 07 - 02 18 : 54 : 06 , 712 epoch 67 - iter 90 / 154 - loss 0.08593876 - samples / sec : 23.81 2020 - 07 - 02 18 : 54 : 25 , 259 epoch 67 - iter 105 / 154 - loss 0.08223052 - samples / sec : 26.03 2020 - 07 - 02 18 : 54 : 44 , 372 epoch 67 - iter 120 / 154 - loss 0.07795331 - samples / sec : 25.27 2020 - 07 - 02 18 : 55 : 03 , 642 epoch 67 - iter 135 / 154 - loss 0.07723948 - samples / sec : 25.22 2020 - 07 - 02 18 : 55 : 23 , 678 epoch 67 - iter 150 / 154 - loss 0.07914580 - samples / sec : 24.12 2020 - 07 - 02 18 : 55 : 28 , 023 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 55 : 28 , 024 EPOCH 67 done : loss 0.0788 - lr 0.0010136 2020 - 07 - 02 18 : 55 : 50 , 431 DEV : loss 0.196882426738739 - score 0.9804 2020 - 07 - 02 18 : 55 : 50 , 500 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 18 : 55 : 50 , 502 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 56 : 13 , 151 epoch 68 - iter 15 / 154 - loss 0.11901739 - samples / sec : 21.44 2020 - 07 - 02 18 : 56 : 33 , 569 epoch 68 - iter 30 / 154 - loss 0.10123495 - samples / sec : 23.64 2020 - 07 - 02 18 : 56 : 53 , 516 epoch 68 - iter 45 / 154 - loss 0.10136171 - samples / sec : 24.37 2020 - 07 - 02 18 : 57 : 14 , 476 epoch 68 - iter 60 / 154 - loss 0.09211459 - samples / sec : 23.03 2020 - 07 - 02 18 : 57 : 32 , 709 epoch 68 - iter 75 / 154 - loss 0.09430514 - samples / sec : 26.50 2020 - 07 - 02 18 : 57 : 53 , 853 epoch 68 - iter 90 / 154 - loss 0.09572636 - samples / sec : 22.84 2020 - 07 - 02 18 : 58 : 13 , 391 epoch 68 - iter 105 / 154 - loss 0.09273255 - samples / sec : 24.89 2020 - 07 - 02 18 : 58 : 33 , 704 epoch 68 - iter 120 / 154 - loss 0.09239831 - samples / sec : 23.76 2020 - 07 - 02 18 : 58 : 52 , 488 epoch 68 - iter 135 / 154 - loss 0.08849469 - samples / sec : 25.92 2020 - 07 - 02 18 : 59 : 12 , 410 epoch 68 - iter 150 / 154 - loss 0.08853157 - samples / sec : 24.24 2020 - 07 - 02 18 : 59 : 16 , 879 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 59 : 16 , 880 EPOCH 68 done : loss 0.0872 - lr 0.0010136 2020 - 07 - 02 18 : 59 : 39 , 437 DEV : loss 0.1948608160018921 - score 0.9798 2020 - 07 - 02 18 : 59 : 39 , 507 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 18 : 59 : 39 , 509 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 00 : 03 , 769 epoch 69 - iter 15 / 154 - loss 0.09427100 - samples / sec : 20.23 2020 - 07 - 02 19 : 00 : 22 , 554 epoch 69 - iter 30 / 154 - loss 0.09612075 - samples / sec : 25.72 2020 - 07 - 02 19 : 00 : 45 , 090 epoch 69 - iter 45 / 154 - loss 0.09061571 - samples / sec : 21.41 2020 - 07 - 02 19 : 01 : 05 , 612 epoch 69 - iter 60 / 154 - loss 0.08845754 - samples / sec : 23.53 2020 - 07 - 02 19 : 01 : 24 , 844 epoch 69 - iter 75 / 154 - loss 0.08787593 - samples / sec : 25.11 2020 - 07 - 02 19 : 01 : 44 , 256 epoch 69 - iter 90 / 154 - loss 0.08266684 - samples / sec : 24.87 2020 - 07 - 02 19 : 02 : 05 , 028 epoch 69 - iter 105 / 154 - loss 0.08399499 - samples / sec : 23.39 2020 - 07 - 02 19 : 02 : 24 , 578 epoch 69 - iter 120 / 154 - loss 0.08212113 - samples / sec : 24.69 2020 - 07 - 02 19 : 02 : 44 , 269 epoch 69 - iter 135 / 154 - loss 0.08719511 - samples / sec : 24.51 2020 - 07 - 02 19 : 03 : 03 , 110 epoch 69 - iter 150 / 154 - loss 0.08658361 - samples / sec : 25.65 2020 - 07 - 02 19 : 03 : 07 , 068 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 03 : 07 , 069 EPOCH 69 done : loss 0.0862 - lr 0.0010136 2020 - 07 - 02 19 : 03 : 29 , 523 DEV : loss 0.2046564817428589 - score 0.9792 2020 - 07 - 02 19 : 03 : 29 , 606 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 19 : 03 : 29 , 608 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 03 : 53 , 614 epoch 70 - iter 15 / 154 - loss 0.07197015 - samples / sec : 20.42 2020 - 07 - 02 19 : 04 : 12 , 712 epoch 70 - iter 30 / 154 - loss 0.06957877 - samples / sec : 25.30 2020 - 07 - 02 19 : 04 : 32 , 036 epoch 70 - iter 45 / 154 - loss 0.07140003 - samples / sec : 24.97 2020 - 07 - 02 19 : 04 : 52 , 137 epoch 70 - iter 60 / 154 - loss 0.07865867 - samples / sec : 24.19 2020 - 07 - 02 19 : 05 : 13 , 204 epoch 70 - iter 75 / 154 - loss 0.08453413 - samples / sec : 22.91 2020 - 07 - 02 19 : 05 : 33 , 592 epoch 70 - iter 90 / 154 - loss 0.08186147 - samples / sec : 23.69 2020 - 07 - 02 19 : 05 : 52 , 073 epoch 70 - iter 105 / 154 - loss 0.08007018 - samples / sec : 26.35 2020 - 07 - 02 19 : 06 : 11 , 649 epoch 70 - iter 120 / 154 - loss 0.08037839 - samples / sec : 24.67 2020 - 07 - 02 19 : 06 : 30 , 310 epoch 70 - iter 135 / 154 - loss 0.07974631 - samples / sec : 25.87 2020 - 07 - 02 19 : 06 : 50 , 797 epoch 70 - iter 150 / 154 - loss 0.07986848 - samples / sec : 23.72 2020 - 07 - 02 19 : 06 : 55 , 307 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 06 : 55 , 308 EPOCH 70 done : loss 0.0803 - lr 0.0010136 2020 - 07 - 02 19 : 07 : 17 , 712 DEV : loss 0.20093274116516113 - score 0.9792 2020 - 07 - 02 19 : 07 : 17 , 781 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 19 : 07 : 17 , 783 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 07 : 38 , 876 epoch 71 - iter 15 / 154 - loss 0.07919160 - samples / sec : 23.06 2020 - 07 - 02 19 : 07 : 58 , 406 epoch 71 - iter 30 / 154 - loss 0.08073911 - samples / sec : 25.00 2020 - 07 - 02 19 : 08 : 18 , 014 epoch 71 - iter 45 / 154 - loss 0.07823560 - samples / sec : 24.62 2020 - 07 - 02 19 : 08 : 37 , 811 epoch 71 - iter 60 / 154 - loss 0.07837923 - samples / sec : 24.41 2020 - 07 - 02 19 : 08 : 57 , 770 epoch 71 - iter 75 / 154 - loss 0.07954901 - samples / sec : 24.36 2020 - 07 - 02 19 : 09 : 17 , 131 epoch 71 - iter 90 / 154 - loss 0.08208104 - samples / sec : 24.94 2020 - 07 - 02 19 : 09 : 36 , 642 epoch 71 - iter 105 / 154 - loss 0.08020370 - samples / sec : 24.75 2020 - 07 - 02 19 : 09 : 57 , 211 epoch 71 - iter 120 / 154 - loss 0.08218516 - samples / sec : 23.62 2020 - 07 - 02 19 : 10 : 16 , 405 epoch 71 - iter 135 / 154 - loss 0.08415195 - samples / sec : 25.17 2020 - 07 - 02 19 : 10 : 37 , 174 epoch 71 - iter 150 / 154 - loss 0.08384235 - samples / sec : 23.25 2020 - 07 - 02 19 : 10 : 43 , 228 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 10 : 43 , 229 EPOCH 71 done : loss 0.0846 - lr 0.0010136 2020 - 07 - 02 19 : 11 : 06 , 086 DEV : loss 0.1951770782470703 - score 0.9798 2020 - 07 - 02 19 : 11 : 06 , 157 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 19 : 11 : 06 , 159 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 11 : 28 , 401 epoch 72 - iter 15 / 154 - loss 0.07519316 - samples / sec : 21.84 2020 - 07 - 02 19 : 11 : 47 , 943 epoch 72 - iter 30 / 154 - loss 0.07725611 - samples / sec : 24.71 2020 - 07 - 02 19 : 12 : 08 , 292 epoch 72 - iter 45 / 154 - loss 0.08135867 - samples / sec : 23.94 2020 - 07 - 02 19 : 12 : 28 , 281 epoch 72 - iter 60 / 154 - loss 0.09061311 - samples / sec : 24.15 2020 - 07 - 02 19 : 12 : 48 , 774 epoch 72 - iter 75 / 154 - loss 0.08907134 - samples / sec : 23.56 2020 - 07 - 02 19 : 13 : 08 , 770 epoch 72 - iter 90 / 154 - loss 0.08749927 - samples / sec : 24.14 2020 - 07 - 02 19 : 13 : 27 , 640 epoch 72 - iter 105 / 154 - loss 0.08971226 - samples / sec : 25.59 2020 - 07 - 02 19 : 13 : 46 , 871 epoch 72 - iter 120 / 154 - loss 0.08774978 - samples / sec : 25.11 2020 - 07 - 02 19 : 14 : 06 , 861 epoch 72 - iter 135 / 154 - loss 0.08598488 - samples / sec : 24.31 2020 - 07 - 02 19 : 14 : 25 , 930 epoch 72 - iter 150 / 154 - loss 0.08677177 - samples / sec : 25.33 2020 - 07 - 02 19 : 14 : 30 , 812 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 14 : 30 , 813 EPOCH 72 done : loss 0.0855 - lr 0.0010136 2020 - 07 - 02 19 : 14 : 53 , 238 DEV : loss 0.19694292545318604 - score 0.9798 Epoch 72 : reducing learning rate of group 0 to 5.0682e-04 . 2020 - 07 - 02 19 : 14 : 53 , 307 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 19 : 14 : 53 , 309 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 15 : 15 , 456 epoch 73 - iter 15 / 154 - loss 0.12583278 - samples / sec : 22.17 2020 - 07 - 02 19 : 15 : 36 , 595 epoch 73 - iter 30 / 154 - loss 0.10992353 - samples / sec : 22.83 2020 - 07 - 02 19 : 15 : 57 , 197 epoch 73 - iter 45 / 154 - loss 0.09559164 - samples / sec : 23.44 2020 - 07 - 02 19 : 16 : 17 , 290 epoch 73 - iter 60 / 154 - loss 0.08711064 - samples / sec : 24.03 2020 - 07 - 02 19 : 16 : 37 , 625 epoch 73 - iter 75 / 154 - loss 0.08157262 - samples / sec : 23.75 2020 - 07 - 02 19 : 16 : 57 , 143 epoch 73 - iter 90 / 154 - loss 0.08081692 - samples / sec : 24.75 2020 - 07 - 02 19 : 17 : 17 , 445 epoch 73 - iter 105 / 154 - loss 0.08004153 - samples / sec : 23.99 2020 - 07 - 02 19 : 17 : 37 , 636 epoch 73 - iter 120 / 154 - loss 0.07894342 - samples / sec : 23.90 2020 - 07 - 02 19 : 17 : 57 , 886 epoch 73 - iter 135 / 154 - loss 0.08008806 - samples / sec : 23.84 2020 - 07 - 02 19 : 18 : 16 , 158 epoch 73 - iter 150 / 154 - loss 0.08376308 - samples / sec : 26.62 2020 - 07 - 02 19 : 18 : 20 , 767 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 18 : 20 , 769 EPOCH 73 done : loss 0.0835 - lr 0.0005068 2020 - 07 - 02 19 : 18 : 43 , 129 DEV : loss 0.19665665924549103 - score 0.9798 2020 - 07 - 02 19 : 18 : 43 , 198 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 19 : 18 : 43 , 199 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 19 : 05 , 401 epoch 74 - iter 15 / 154 - loss 0.07577754 - samples / sec : 22.09 2020 - 07 - 02 19 : 19 : 26 , 893 epoch 74 - iter 30 / 154 - loss 0.09408136 - samples / sec : 22.46 2020 - 07 - 02 19 : 19 : 47 , 188 epoch 74 - iter 45 / 154 - loss 0.08224051 - samples / sec : 23.78 2020 - 07 - 02 19 : 20 : 06 , 562 epoch 74 - iter 60 / 154 - loss 0.09164365 - samples / sec : 25.14 2020 - 07 - 02 19 : 20 : 25 , 430 epoch 74 - iter 75 / 154 - loss 0.09134524 - samples / sec : 25.58 2020 - 07 - 02 19 : 20 : 45 , 004 epoch 74 - iter 90 / 154 - loss 0.08959111 - samples / sec : 24.67 2020 - 07 - 02 19 : 21 : 03 , 477 epoch 74 - iter 105 / 154 - loss 0.08734923 - samples / sec : 26.36 2020 - 07 - 02 19 : 21 : 23 , 457 epoch 74 - iter 120 / 154 - loss 0.08620086 - samples / sec : 24.16 2020 - 07 - 02 19 : 21 : 43 , 214 epoch 74 - iter 135 / 154 - loss 0.08835269 - samples / sec : 24.61 2020 - 07 - 02 19 : 22 : 04 , 287 epoch 74 - iter 150 / 154 - loss 0.08608258 - samples / sec : 22.90 2020 - 07 - 02 19 : 22 : 08 , 952 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 22 : 08 , 954 EPOCH 74 done : loss 0.0853 - lr 0.0005068 2020 - 07 - 02 19 : 22 : 31 , 319 DEV : loss 0.19973701238632202 - score 0.9798 2020 - 07 - 02 19 : 22 : 31 , 390 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 19 : 22 : 31 , 391 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 22 : 53 , 298 epoch 75 - iter 15 / 154 - loss 0.07400509 - samples / sec : 22.44 2020 - 07 - 02 19 : 23 : 11 , 752 epoch 75 - iter 30 / 154 - loss 0.07855947 - samples / sec : 26.18 2020 - 07 - 02 19 : 23 : 32 , 338 epoch 75 - iter 45 / 154 - loss 0.08280147 - samples / sec : 23.45 2020 - 07 - 02 19 : 23 : 51 , 953 epoch 75 - iter 60 / 154 - loss 0.08387791 - samples / sec : 24.81 2020 - 07 - 02 19 : 24 : 11 , 685 epoch 75 - iter 75 / 154 - loss 0.09056895 - samples / sec : 24.47 2020 - 07 - 02 19 : 24 : 31 , 575 epoch 75 - iter 90 / 154 - loss 0.09015049 - samples / sec : 24.28 2020 - 07 - 02 19 : 24 : 51 , 849 epoch 75 - iter 105 / 154 - loss 0.08562251 - samples / sec : 23.97 2020 - 07 - 02 19 : 25 : 11 , 238 epoch 75 - iter 120 / 154 - loss 0.08479583 - samples / sec : 24.89 2020 - 07 - 02 19 : 25 : 31 , 590 epoch 75 - iter 135 / 154 - loss 0.08538203 - samples / sec : 23.87 2020 - 07 - 02 19 : 25 : 50 , 109 epoch 75 - iter 150 / 154 - loss 0.08300129 - samples / sec : 26.08 2020 - 07 - 02 19 : 25 : 54 , 984 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 25 : 54 , 986 EPOCH 75 done : loss 0.0832 - lr 0.0005068 2020 - 07 - 02 19 : 26 : 17 , 461 DEV : loss 0.19750793278217316 - score 0.9804 2020 - 07 - 02 19 : 26 : 17 , 531 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 19 : 26 : 17 , 533 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 26 : 40 , 256 epoch 76 - iter 15 / 154 - loss 0.10106792 - samples / sec : 21.59 2020 - 07 - 02 19 : 27 : 00 , 857 epoch 76 - iter 30 / 154 - loss 0.08253583 - samples / sec : 23.43 2020 - 07 - 02 19 : 27 : 21 , 793 epoch 76 - iter 45 / 154 - loss 0.09450863 - samples / sec : 23.21 2020 - 07 - 02 19 : 27 : 40 , 802 epoch 76 - iter 60 / 154 - loss 0.09367372 - samples / sec : 25.41 2020 - 07 - 02 19 : 28 : 00 , 062 epoch 76 - iter 75 / 154 - loss 0.09405521 - samples / sec : 25.08 2020 - 07 - 02 19 : 28 : 19 , 812 epoch 76 - iter 90 / 154 - loss 0.08686411 - samples / sec : 24.61 2020 - 07 - 02 19 : 28 : 39 , 040 epoch 76 - iter 105 / 154 - loss 0.08190449 - samples / sec : 25.14 2020 - 07 - 02 19 : 28 : 57 , 359 epoch 76 - iter 120 / 154 - loss 0.08770324 - samples / sec : 26.38 2020 - 07 - 02 19 : 29 : 17 , 265 epoch 76 - iter 135 / 154 - loss 0.09068281 - samples / sec : 24.39 2020 - 07 - 02 19 : 29 : 37 , 734 epoch 76 - iter 150 / 154 - loss 0.08872985 - samples / sec : 23.59 2020 - 07 - 02 19 : 29 : 43 , 601 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 29 : 43 , 603 EPOCH 76 done : loss 0.0894 - lr 0.0005068 2020 - 07 - 02 19 : 30 : 06 , 126 DEV : loss 0.1972895711660385 - score 0.9804 2020 - 07 - 02 19 : 30 : 06 , 337 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 19 : 30 : 06 , 338 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 30 : 27 , 741 epoch 77 - iter 15 / 154 - loss 0.10128506 - samples / sec : 22.72 2020 - 07 - 02 19 : 30 : 47 , 730 epoch 77 - iter 30 / 154 - loss 0.10225451 - samples / sec : 24.17 2020 - 07 - 02 19 : 31 : 08 , 978 epoch 77 - iter 45 / 154 - loss 0.09445847 - samples / sec : 22.90 2020 - 07 - 02 19 : 31 : 28 , 184 epoch 77 - iter 60 / 154 - loss 0.09758655 - samples / sec : 25.15 2020 - 07 - 02 19 : 31 : 47 , 413 epoch 77 - iter 75 / 154 - loss 0.09338443 - samples / sec : 25.11 2020 - 07 - 02 19 : 32 : 06 , 982 epoch 77 - iter 90 / 154 - loss 0.08988446 - samples / sec : 24.69 2020 - 07 - 02 19 : 32 : 26 , 392 epoch 77 - iter 105 / 154 - loss 0.08855609 - samples / sec : 24.88 2020 - 07 - 02 19 : 32 : 44 , 943 epoch 77 - iter 120 / 154 - loss 0.09002902 - samples / sec : 26.22 2020 - 07 - 02 19 : 33 : 03 , 714 epoch 77 - iter 135 / 154 - loss 0.08720513 - samples / sec : 25.72 2020 - 07 - 02 19 : 33 : 25 , 042 epoch 77 - iter 150 / 154 - loss 0.08530933 - samples / sec : 22.62 2020 - 07 - 02 19 : 33 : 29 , 991 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 33 : 29 , 992 EPOCH 77 done : loss 0.0887 - lr 0.0005068 2020 - 07 - 02 19 : 33 : 53 , 013 DEV : loss 0.19450919330120087 - score 0.9798 2020 - 07 - 02 19 : 33 : 53 , 083 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 19 : 33 : 53 , 085 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 34 : 14 , 455 epoch 78 - iter 15 / 154 - loss 0.05516994 - samples / sec : 22.75 2020 - 07 - 02 19 : 34 : 34 , 304 epoch 78 - iter 30 / 154 - loss 0.05071913 - samples / sec : 24.59 2020 - 07 - 02 19 : 34 : 54 , 316 epoch 78 - iter 45 / 154 - loss 0.06652978 - samples / sec : 24.12 2020 - 07 - 02 19 : 35 : 13 , 264 epoch 78 - iter 60 / 154 - loss 0.07676350 - samples / sec : 25.50 2020 - 07 - 02 19 : 35 : 32 , 371 epoch 78 - iter 75 / 154 - loss 0.07474116 - samples / sec : 25.28 2020 - 07 - 02 19 : 35 : 52 , 925 epoch 78 - iter 90 / 154 - loss 0.07864122 - samples / sec : 23.65 2020 - 07 - 02 19 : 36 : 12 , 549 epoch 78 - iter 105 / 154 - loss 0.08017938 - samples / sec : 24.62 2020 - 07 - 02 19 : 36 : 33 , 222 epoch 78 - iter 120 / 154 - loss 0.08172559 - samples / sec : 23.40 2020 - 07 - 02 19 : 36 : 54 , 373 epoch 78 - iter 135 / 154 - loss 0.08014307 - samples / sec : 22.96 2020 - 07 - 02 19 : 37 : 14 , 628 epoch 78 - iter 150 / 154 - loss 0.07845808 - samples / sec : 23.83 2020 - 07 - 02 19 : 37 : 19 , 890 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 37 : 19 , 891 EPOCH 78 done : loss 0.0799 - lr 0.0005068 2020 - 07 - 02 19 : 37 : 42 , 493 DEV : loss 0.19961152970790863 - score 0.9786 Epoch 78 : reducing learning rate of group 0 to 2.5341e-04 . 2020 - 07 - 02 19 : 37 : 42 , 593 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 19 : 37 : 42 , 595 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 38 : 03 , 566 epoch 79 - iter 15 / 154 - loss 0.07821662 - samples / sec : 23.50 2020 - 07 - 02 19 : 38 : 22 , 832 epoch 79 - iter 30 / 154 - loss 0.08100213 - samples / sec : 25.07 2020 - 07 - 02 19 : 38 : 42 , 538 epoch 79 - iter 45 / 154 - loss 0.08145519 - samples / sec : 24.51 2020 - 07 - 02 19 : 39 : 02 , 437 epoch 79 - iter 60 / 154 - loss 0.08334038 - samples / sec : 24.25 2020 - 07 - 02 19 : 39 : 22 , 840 epoch 79 - iter 75 / 154 - loss 0.08079175 - samples / sec : 23.67 2020 - 07 - 02 19 : 39 : 44 , 219 epoch 79 - iter 90 / 154 - loss 0.08068648 - samples / sec : 22.59 2020 - 07 - 02 19 : 40 : 04 , 445 epoch 79 - iter 105 / 154 - loss 0.08182918 - samples / sec : 23.88 2020 - 07 - 02 19 : 40 : 23 , 599 epoch 79 - iter 120 / 154 - loss 0.08141914 - samples / sec : 25.42 2020 - 07 - 02 19 : 40 : 43 , 474 epoch 79 - iter 135 / 154 - loss 0.08588830 - samples / sec : 24.30 2020 - 07 - 02 19 : 41 : 02 , 682 epoch 79 - iter 150 / 154 - loss 0.08469767 - samples / sec : 25.15 2020 - 07 - 02 19 : 41 : 07 , 627 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 41 : 07 , 628 EPOCH 79 done : loss 0.0856 - lr 0.0002534 2020 - 07 - 02 19 : 41 : 30 , 538 DEV : loss 0.19733120501041412 - score 0.9798 2020 - 07 - 02 19 : 41 : 30 , 606 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 19 : 41 : 30 , 609 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 41 : 54 , 035 epoch 80 - iter 15 / 154 - loss 0.08911656 - samples / sec : 20.74 2020 - 07 - 02 19 : 42 : 14 , 513 epoch 80 - iter 30 / 154 - loss 0.09176585 - samples / sec : 23.79 2020 - 07 - 02 19 : 42 : 33 , 470 epoch 80 - iter 45 / 154 - loss 0.08643438 - samples / sec : 25.47 2020 - 07 - 02 19 : 42 : 52 , 061 epoch 80 - iter 60 / 154 - loss 0.08399421 - samples / sec : 25.99 2020 - 07 - 02 19 : 43 : 13 , 561 epoch 80 - iter 75 / 154 - loss 0.08570782 - samples / sec : 22.60 2020 - 07 - 02 19 : 43 : 33 , 324 epoch 80 - iter 90 / 154 - loss 0.08288445 - samples / sec : 24.43 2020 - 07 - 02 19 : 43 : 52 , 847 epoch 80 - iter 105 / 154 - loss 0.08118652 - samples / sec : 24.72 2020 - 07 - 02 19 : 44 : 13 , 126 epoch 80 - iter 120 / 154 - loss 0.08139978 - samples / sec : 23.96 2020 - 07 - 02 19 : 44 : 33 , 738 epoch 80 - iter 135 / 154 - loss 0.08077319 - samples / sec : 23.42 2020 - 07 - 02 19 : 44 : 54 , 207 epoch 80 - iter 150 / 154 - loss 0.08154344 - samples / sec : 23.58 2020 - 07 - 02 19 : 44 : 58 , 843 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 44 : 58 , 845 EPOCH 80 done : loss 0.0816 - lr 0.0002534 2020 - 07 - 02 19 : 45 : 21 , 976 DEV : loss 0.19819292426109314 - score 0.9798 2020 - 07 - 02 19 : 45 : 22 , 052 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 19 : 45 : 22 , 055 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 45 : 43 , 473 epoch 81 - iter 15 / 154 - loss 0.11282083 - samples / sec : 22.71 2020 - 07 - 02 19 : 46 : 24 , 008 epoch 81 - iter 45 / 154 - loss 0.08270191 - samples / sec : 23.23 2020 - 07 - 02 19 : 46 : 42 , 832 epoch 81 - iter 60 / 154 - loss 0.08318824 - samples / sec : 25.66 2020 - 07 - 02 19 : 47 : 02 , 499 epoch 81 - iter 75 / 154 - loss 0.08471441 - samples / sec : 24.57 2020 - 07 - 02 19 : 47 : 22 , 738 epoch 81 - iter 90 / 154 - loss 0.08327459 - samples / sec : 23.85 2020 - 07 - 02 19 : 47 : 42 , 972 epoch 81 - iter 105 / 154 - loss 0.07872031 - samples / sec : 23.86 2020 - 07 - 02 19 : 48 : 01 , 928 epoch 81 - iter 120 / 154 - loss 0.07652540 - samples / sec : 25.68 2020 - 07 - 02 19 : 48 : 22 , 614 epoch 81 - iter 135 / 154 - loss 0.07975565 - samples / sec : 23.34 2020 - 07 - 02 19 : 48 : 41 , 787 epoch 81 - iter 150 / 154 - loss 0.07985378 - samples / sec : 25.18 2020 - 07 - 02 19 : 48 : 46 , 539 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 48 : 46 , 541 EPOCH 81 done : loss 0.0798 - lr 0.0002534 2020 - 07 - 02 19 : 49 : 09 , 276 DEV : loss 0.19881562888622284 - score 0.9792 2020 - 07 - 02 19 : 49 : 09 , 344 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 19 : 49 : 09 , 346 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 49 : 33 , 998 epoch 82 - iter 15 / 154 - loss 0.08336314 - samples / sec : 19.69 2020 - 07 - 02 19 : 49 : 53 , 295 epoch 82 - iter 30 / 154 - loss 0.08455563 - samples / sec : 25.27 2020 - 07 - 02 19 : 50 : 12 , 593 epoch 82 - iter 45 / 154 - loss 0.08075597 - samples / sec : 25.02 2020 - 07 - 02 19 : 50 : 31 , 678 epoch 82 - iter 60 / 154 - loss 0.07854668 - samples / sec : 25.30 2020 - 07 - 02 19 : 50 : 52 , 930 epoch 82 - iter 75 / 154 - loss 0.08441896 - samples / sec : 22.72 2020 - 07 - 02 19 : 51 : 12 , 043 epoch 82 - iter 90 / 154 - loss 0.07963300 - samples / sec : 25.26 2020 - 07 - 02 19 : 51 : 29 , 700 epoch 82 - iter 105 / 154 - loss 0.08339172 - samples / sec : 27.55 2020 - 07 - 02 19 : 51 : 49 , 879 epoch 82 - iter 120 / 154 - loss 0.08139258 - samples / sec : 23.93 2020 - 07 - 02 19 : 52 : 09 , 504 epoch 82 - iter 135 / 154 - loss 0.08187230 - samples / sec : 24.59 2020 - 07 - 02 19 : 52 : 29 , 636 epoch 82 - iter 150 / 154 - loss 0.08149730 - samples / sec : 24.17 2020 - 07 - 02 19 : 52 : 34 , 259 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 52 : 34 , 261 EPOCH 82 done : loss 0.0802 - lr 0.0002534 2020 - 07 - 02 19 : 52 : 56 , 633 DEV : loss 0.1967770755290985 - score 0.9798 2020 - 07 - 02 19 : 52 : 56 , 703 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 19 : 52 : 56 , 706 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 53 : 18 , 491 epoch 83 - iter 15 / 154 - loss 0.08312181 - samples / sec : 22.32 2020 - 07 - 02 19 : 53 : 39 , 588 epoch 83 - iter 30 / 154 - loss 0.08574174 - samples / sec : 23.12 2020 - 07 - 02 19 : 53 : 58 , 547 epoch 83 - iter 45 / 154 - loss 0.08475116 - samples / sec : 25.48 2020 - 07 - 02 19 : 54 : 19 , 207 epoch 83 - iter 60 / 154 - loss 0.07566535 - samples / sec : 23.38 2020 - 07 - 02 19 : 54 : 39 , 070 epoch 83 - iter 75 / 154 - loss 0.07675807 - samples / sec : 24.31 2020 - 07 - 02 19 : 54 : 59 , 788 epoch 83 - iter 90 / 154 - loss 0.07665076 - samples / sec : 23.43 2020 - 07 - 02 19 : 55 : 19 , 537 epoch 83 - iter 105 / 154 - loss 0.07946286 - samples / sec : 24.45 2020 - 07 - 02 19 : 55 : 39 , 566 epoch 83 - iter 120 / 154 - loss 0.07992272 - samples / sec : 24.10 2020 - 07 - 02 19 : 55 : 57 , 919 epoch 83 - iter 135 / 154 - loss 0.08126198 - samples / sec : 26.51 2020 - 07 - 02 19 : 56 : 17 , 369 epoch 83 - iter 150 / 154 - loss 0.08188783 - samples / sec : 24.81 2020 - 07 - 02 19 : 56 : 21 , 455 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 56 : 21 , 456 EPOCH 83 done : loss 0.0820 - lr 0.0002534 2020 - 07 - 02 19 : 56 : 44 , 267 DEV : loss 0.19680184125900269 - score 0.9798 2020 - 07 - 02 19 : 56 : 44 , 335 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 19 : 56 : 44 , 337 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 57 : 06 , 208 epoch 84 - iter 15 / 154 - loss 0.06290983 - samples / sec : 22.22 2020 - 07 - 02 19 : 57 : 24 , 662 epoch 84 - iter 30 / 154 - loss 0.07252258 - samples / sec : 26.19 2020 - 07 - 02 19 : 57 : 43 , 378 epoch 84 - iter 45 / 154 - loss 0.07217186 - samples / sec : 26.06 2020 - 07 - 02 19 : 58 : 03 , 579 epoch 84 - iter 60 / 154 - loss 0.07073096 - samples / sec : 23.90 2020 - 07 - 02 19 : 58 : 23 , 163 epoch 84 - iter 75 / 154 - loss 0.07663974 - samples / sec : 24.65 2020 - 07 - 02 19 : 58 : 42 , 104 epoch 84 - iter 90 / 154 - loss 0.07967136 - samples / sec : 25.69 2020 - 07 - 02 19 : 59 : 01 , 744 epoch 84 - iter 105 / 154 - loss 0.08082978 - samples / sec : 24.59 2020 - 07 - 02 19 : 59 : 23 , 133 epoch 84 - iter 120 / 154 - loss 0.08198013 - samples / sec : 22.57 2020 - 07 - 02 19 : 59 : 43 , 063 epoch 84 - iter 135 / 154 - loss 0.08208106 - samples / sec : 24.39 2020 - 07 - 02 20 : 00 : 04 , 913 epoch 84 - iter 150 / 154 - loss 0.08289737 - samples / sec : 22.07 2020 - 07 - 02 20 : 00 : 09 , 399 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 00 : 09 , 400 EPOCH 84 done : loss 0.0832 - lr 0.0002534 2020 - 07 - 02 20 : 00 : 31 , 750 DEV : loss 0.19807276129722595 - score 0.9798 Epoch 84 : reducing learning rate of group 0 to 1.2670e-04 . 2020 - 07 - 02 20 : 00 : 31 , 974 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 20 : 00 : 31 , 976 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 00 : 55 , 472 epoch 85 - iter 15 / 154 - loss 0.08761623 - samples / sec : 20.67 2020 - 07 - 02 20 : 01 : 14 , 145 epoch 85 - iter 30 / 154 - loss 0.08436980 - samples / sec : 25.88 2020 - 07 - 02 20 : 01 : 34 , 055 epoch 85 - iter 45 / 154 - loss 0.08638632 - samples / sec : 24.25 2020 - 07 - 02 20 : 01 : 53 , 168 epoch 85 - iter 60 / 154 - loss 0.09292158 - samples / sec : 25.49 2020 - 07 - 02 20 : 02 : 12 , 416 epoch 85 - iter 75 / 154 - loss 0.08979836 - samples / sec : 25.09 2020 - 07 - 02 20 : 02 : 32 , 606 epoch 85 - iter 90 / 154 - loss 0.08738348 - samples / sec : 23.91 2020 - 07 - 02 20 : 02 : 53 , 312 epoch 85 - iter 105 / 154 - loss 0.08728358 - samples / sec : 23.47 2020 - 07 - 02 20 : 03 : 13 , 522 epoch 85 - iter 120 / 154 - loss 0.08704443 - samples / sec : 23.89 2020 - 07 - 02 20 : 03 : 32 , 671 epoch 85 - iter 135 / 154 - loss 0.08533383 - samples / sec : 25.23 2020 - 07 - 02 20 : 03 : 53 , 771 epoch 85 - iter 150 / 154 - loss 0.08561399 - samples / sec : 23.03 2020 - 07 - 02 20 : 03 : 58 , 432 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 03 : 58 , 433 EPOCH 85 done : loss 0.0846 - lr 0.0001267 2020 - 07 - 02 20 : 04 : 20 , 935 DEV : loss 0.19647707045078278 - score 0.9798 2020 - 07 - 02 20 : 04 : 21 , 004 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 20 : 04 : 21 , 006 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 04 : 44 , 697 epoch 86 - iter 15 / 154 - loss 0.08141229 - samples / sec : 20.51 2020 - 07 - 02 20 : 05 : 05 , 999 epoch 86 - iter 30 / 154 - loss 0.08228939 - samples / sec : 22.67 2020 - 07 - 02 20 : 05 : 26 , 309 epoch 86 - iter 45 / 154 - loss 0.07569061 - samples / sec : 23.76 2020 - 07 - 02 20 : 05 : 45 , 276 epoch 86 - iter 60 / 154 - loss 0.07859332 - samples / sec : 25.46 2020 - 07 - 02 20 : 06 : 05 , 770 epoch 86 - iter 75 / 154 - loss 0.08294603 - samples / sec : 23.69 2020 - 07 - 02 20 : 06 : 25 , 472 epoch 86 - iter 90 / 154 - loss 0.08264545 - samples / sec : 24.51 2020 - 07 - 02 20 : 06 : 45 , 127 epoch 86 - iter 105 / 154 - loss 0.08138939 - samples / sec : 24.56 2020 - 07 - 02 20 : 07 : 03 , 601 epoch 86 - iter 120 / 154 - loss 0.08116858 - samples / sec : 26.35 2020 - 07 - 02 20 : 07 : 22 , 317 epoch 86 - iter 135 / 154 - loss 0.08432639 - samples / sec : 25.80 2020 - 07 - 02 20 : 07 : 40 , 946 epoch 86 - iter 150 / 154 - loss 0.08361728 - samples / sec : 25.93 2020 - 07 - 02 20 : 07 : 46 , 002 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 07 : 46 , 003 EPOCH 86 done : loss 0.0821 - lr 0.0001267 2020 - 07 - 02 20 : 08 : 08 , 553 DEV : loss 0.19691519439220428 - score 0.9798 2020 - 07 - 02 20 : 08 : 08 , 622 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 20 : 08 : 08 , 623 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 08 : 32 , 228 epoch 87 - iter 15 / 154 - loss 0.07756042 - samples / sec : 20.58 2020 - 07 - 02 20 : 08 : 51 , 084 epoch 87 - iter 30 / 154 - loss 0.08246949 - samples / sec : 25.92 2020 - 07 - 02 20 : 09 : 10 , 734 epoch 87 - iter 45 / 154 - loss 0.08848242 - samples / sec : 24.57 2020 - 07 - 02 20 : 09 : 30 , 149 epoch 87 - iter 60 / 154 - loss 0.09197148 - samples / sec : 24.87 2020 - 07 - 02 20 : 09 : 52 , 124 epoch 87 - iter 75 / 154 - loss 0.09220921 - samples / sec : 22.09 2020 - 07 - 02 20 : 10 : 12 , 434 epoch 87 - iter 90 / 154 - loss 0.09423689 - samples / sec : 23.77 2020 - 07 - 02 20 : 10 : 31 , 990 epoch 87 - iter 105 / 154 - loss 0.09056646 - samples / sec : 24.69 2020 - 07 - 02 20 : 10 : 51 , 277 epoch 87 - iter 120 / 154 - loss 0.08802644 - samples / sec : 25.23 2020 - 07 - 02 20 : 11 : 11 , 081 epoch 87 - iter 135 / 154 - loss 0.08470457 - samples / sec : 24.38 2020 - 07 - 02 20 : 11 : 30 , 365 epoch 87 - iter 150 / 154 - loss 0.08198533 - samples / sec : 25.05 2020 - 07 - 02 20 : 11 : 34 , 515 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 11 : 34 , 517 EPOCH 87 done : loss 0.0823 - lr 0.0001267 2020 - 07 - 02 20 : 11 : 57 , 259 DEV : loss 0.19768422842025757 - score 0.9798 2020 - 07 - 02 20 : 11 : 57 , 329 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 20 : 11 : 57 , 330 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 12 : 20 , 421 epoch 88 - iter 15 / 154 - loss 0.10425307 - samples / sec : 21.03 2020 - 07 - 02 20 : 12 : 40 , 927 epoch 88 - iter 30 / 154 - loss 0.08032743 - samples / sec : 23.56 2020 - 07 - 02 20 : 13 : 01 , 191 epoch 88 - iter 45 / 154 - loss 0.08593136 - samples / sec : 24.04 2020 - 07 - 02 20 : 13 : 19 , 940 epoch 88 - iter 60 / 154 - loss 0.08089487 - samples / sec : 25.77 2020 - 07 - 02 20 : 13 : 39 , 682 epoch 88 - iter 75 / 154 - loss 0.07908025 - samples / sec : 24.45 2020 - 07 - 02 20 : 13 : 59 , 018 epoch 88 - iter 90 / 154 - loss 0.08750883 - samples / sec : 25.15 2020 - 07 - 02 20 : 14 : 20 , 234 epoch 88 - iter 105 / 154 - loss 0.08899166 - samples / sec : 22.75 2020 - 07 - 02 20 : 14 : 39 , 067 epoch 88 - iter 120 / 154 - loss 0.08754672 - samples / sec : 25.65 2020 - 07 - 02 20 : 14 : 59 , 203 epoch 88 - iter 135 / 154 - loss 0.08932739 - samples / sec : 24.17 2020 - 07 - 02 20 : 15 : 17 , 676 epoch 88 - iter 150 / 154 - loss 0.08721906 - samples / sec : 26.15 2020 - 07 - 02 20 : 15 : 22 , 756 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 15 : 22 , 757 EPOCH 88 done : loss 0.0892 - lr 0.0001267 2020 - 07 - 02 20 : 15 : 45 , 517 DEV : loss 0.1971229910850525 - score 0.9798 2020 - 07 - 02 20 : 15 : 45 , 584 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 20 : 15 : 45 , 586 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 16 : 06 , 439 epoch 89 - iter 15 / 154 - loss 0.07559101 - samples / sec : 23.33 2020 - 07 - 02 20 : 16 : 27 , 847 epoch 89 - iter 30 / 154 - loss 0.07773159 - samples / sec : 22.54 2020 - 07 - 02 20 : 16 : 47 , 195 epoch 89 - iter 45 / 154 - loss 0.09153157 - samples / sec : 25.20 2020 - 07 - 02 20 : 17 : 09 , 002 epoch 89 - iter 60 / 154 - loss 0.08735939 - samples / sec : 22.14 2020 - 07 - 02 20 : 17 : 29 , 072 epoch 89 - iter 75 / 154 - loss 0.08408470 - samples / sec : 24.06 2020 - 07 - 02 20 : 17 : 49 , 062 epoch 89 - iter 90 / 154 - loss 0.08779174 - samples / sec : 24.17 2020 - 07 - 02 20 : 18 : 08 , 900 epoch 89 - iter 105 / 154 - loss 0.08387496 - samples / sec : 24.35 2020 - 07 - 02 20 : 18 : 28 , 500 epoch 89 - iter 120 / 154 - loss 0.08260459 - samples / sec : 24.82 2020 - 07 - 02 20 : 18 : 48 , 390 epoch 89 - iter 135 / 154 - loss 0.08210002 - samples / sec : 24.34 2020 - 07 - 02 20 : 19 : 08 , 537 epoch 89 - iter 150 / 154 - loss 0.08263975 - samples / sec : 23.97 2020 - 07 - 02 20 : 19 : 13 , 248 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 19 : 13 , 250 EPOCH 89 done : loss 0.0817 - lr 0.0001267 2020 - 07 - 02 20 : 19 : 35 , 671 DEV : loss 0.19783227145671844 - score 0.9792 2020 - 07 - 02 20 : 19 : 35 , 900 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 20 : 19 : 35 , 902 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 19 : 58 , 748 epoch 90 - iter 15 / 154 - loss 0.08422335 - samples / sec : 21.28 2020 - 07 - 02 20 : 20 : 19 , 410 epoch 90 - iter 30 / 154 - loss 0.07745877 - samples / sec : 23.37 2020 - 07 - 02 20 : 20 : 39 , 526 epoch 90 - iter 45 / 154 - loss 0.07794650 - samples / sec : 24.20 2020 - 07 - 02 20 : 20 : 58 , 368 epoch 90 - iter 60 / 154 - loss 0.07353494 - samples / sec : 25.64 2020 - 07 - 02 20 : 21 : 18 , 494 epoch 90 - iter 75 / 154 - loss 0.07113823 - samples / sec : 23.98 2020 - 07 - 02 20 : 21 : 40 , 280 epoch 90 - iter 90 / 154 - loss 0.07130840 - samples / sec : 22.29 2020 - 07 - 02 20 : 21 : 59 , 776 epoch 90 - iter 105 / 154 - loss 0.07188892 - samples / sec : 24.76 2020 - 07 - 02 20 : 22 : 17 , 909 epoch 90 - iter 120 / 154 - loss 0.07002038 - samples / sec : 26.84 2020 - 07 - 02 20 : 22 : 38 , 265 epoch 90 - iter 135 / 154 - loss 0.07641873 - samples / sec : 23.71 2020 - 07 - 02 20 : 22 : 58 , 198 epoch 90 - iter 150 / 154 - loss 0.07757820 - samples / sec : 24.24 2020 - 07 - 02 20 : 23 : 02 , 778 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 23 : 02 , 779 EPOCH 90 done : loss 0.0785 - lr 0.0001267 2020 - 07 - 02 20 : 23 : 25 , 603 DEV : loss 0.19686871767044067 - score 0.9798 Epoch 90 : reducing learning rate of group 0 to 6.3352e-05 . 2020 - 07 - 02 20 : 23 : 25 , 672 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 20 : 23 : 25 , 674 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 23 : 25 , 675 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 23 : 25 , 675 learning rate too small - quitting training ! 2020 - 07 - 02 20 : 23 : 25 , 676 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 23 : 26 , 199 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 23 : 26 , 201 Testing using best model ... 2020 - 07 - 02 20 : 23 : 26 , 204 loading file Path / to / model / output / directory / best - model . pt 2020 - 07 - 02 20 : 23 : 40 , 895 0.974 0.974 0.974 2020 - 07 - 02 20 : 23 : 40 , 897 MICRO_AVG : acc 0.9913333333333333 - f1 - score 0.974 MACRO_AVG : acc 0.9913333333333334 - f1 - score 0.9776831958334483 ABBR tp : 9 - fp : 0 - fn : 0 - tn : 491 - precision : 1.0000 - recall : 1.0000 - accuracy : 1.0000 - f1 - score : 1.0000 DESC tp : 136 - fp : 5 - fn : 2 - tn : 357 - precision : 0.9645 - recall : 0.9855 - accuracy : 0.9860 - f1 - score : 0.9749 ENTY tp : 86 - fp : 3 - fn : 8 - tn : 403 - precision : 0.9663 - recall : 0.9149 - accuracy : 0.9780 - f1 - score : 0.9399 HUM tp : 63 - fp : 1 - fn : 2 - tn : 434 - precision : 0.9844 - recall : 0.9692 - accuracy : 0.9940 - f1 - score : 0.9767 LOC tp : 80 - fp : 1 - fn : 1 - tn : 418 - precision : 0.9877 - recall : 0.9877 - accuracy : 0.9960 - f1 - score : 0.9877 NUM tp : 113 - fp : 3 - fn : 0 - tn : 384 - precision : 0.9741 - recall : 1.0000 - accuracy : 0.9940 - f1 - score : 0.9869 2020 - 07 - 02 20 : 23 : 40 , 898 ---------------------------------------------------------------------------------------------------- The model was saved in the directory OUTPUT_DIR . We can load the sequence classifier into our EasySequenceClassifier instance and start running inference. from adaptnlp import EasySequenceClassifier # Set example text and instantiate tagger instance example_text = '''Where was the Queen's wedding held? ''' classifier = EasySequenceClassifier () sentences = classifier . tag_text ( example_text , model_name_or_path = OUTPUT_DIR ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels ) Output [ LOC ( 0.9990556836128235 )]","title":"Training Sequence Classification"},{"location":"tutorial/training-sequence-classification.html#getting-started-with-sequenceclassifiertrainer","text":"We want to start by specifying three things: 1. corpus : A Flair Corpus data model object that contains train, test, and dev datasets. This can also be a path to a directory that contains train.csv , test.csv , and dev.csv files. If a path to the files is provided, you will require a column_name_map parameter that maps the indices of the text and label column headers i.e. {0: \"text\", 1: \"label\"} the colummn with text being at index 0 of the csv 2. output_dir : A path to a directory to store trainer and model files 3. doc_embeddings : The EasyDocumentEmbeddings object that has the specified key shortcut names to pretrained language models that the trainer will use as its encoder. from adaptnlp import EasyDocumentEmbeddings , SequenceClassifierTrainer from flair.datasets import TREC_6 corpus = TREC_6 () # Or path to directory of train.csv, test.csv, dev.csv files at \"Path/to/data/directory\" OUTPUT_DIR = \"Path/to/model/output/directory\" doc_embeddings = EasyDocumentEmbeddings ( \"bert-base-cased\" , methods = [ \"rnn\" ]) # We can specify to load the pool or rnn # methods to avoid loading both. Then we want to instantiate the trainer with the following parameters sc_configs = { \"corpus\" : corpus , \"encoder\" : doc_embeddings , \"column_name_map\" : { 0 : \"text\" , 1 : \"label\" }, \"corpus_in_memory\" : True , \"predictive_head\" : \"flair\" , } sc_trainer = SequenceClassifierTrainer ( ** sc_configs ) Output 2020 - 07 - 02 14 : 25 : 28 , 470 [ b 'HUM' , b 'DESC' , b 'ENTY' , b 'NUM' , b 'LOC' , b 'ABBR' ] We can then find the optimal learning rate with the help of the cyclical learning rates method by Leslie Smith. Using this along with our novel approach in automatically extracting an optimal learning rate, we can streamline training without pausing to manually extract the optimal learning rate. The built-in find_learning_rate() will automatically reinitialize the parameteres and optimizer after running the cyclical learning rates method. sc_lr_configs = { \"output_dir\" : OUTPUT_DIR , \"start_learning_rate\" : 1e-8 , \"end_learning_rate\" : 10 , \"iterations\" : 100 , \"mini_batch_size\" : 32 , \"stop_early\" : True , \"smoothing_factor\" : 0.8 , \"plot_learning_rate\" : True , } learning_rate = sc_trainer . find_learning_rate ( ** sc_lr_configs ) Output [ 1.5135612484362082e-08 ] [ 1.8620871366628676e-08 ] [ 2.2908676527677733e-08 ] [ 2.818382931264454e-08 ] [ 3.4673685045253164e-08 ] [ 4.265795188015927e-08 ] [ 5.2480746024977265e-08 ] [ 6.456542290346554e-08 ] [ 7.943282347242815e-08 ] [ 9.772372209558107e-08 ] [ 1.2022644346174127e-07 ] [ 1.4791083881682077e-07 ] [ 1.819700858609984e-07 ] [ 2.2387211385683393e-07 ] [ 2.754228703338167e-07 ] [ 3.3884415613920264e-07 ] [ 4.168693834703353e-07 ] [ 5.128613839913649e-07 ] [ 6.309573444801935e-07 ] [ 7.762471166286916e-07 ] [ 9.54992586021436e-07 ] [ 1.1748975549395298e-06 ] [ 1.4454397707459273e-06 ] [ 1.778279410038923e-06 ] [ 2.187761623949553e-06 ] [ 2.6915348039269168e-06 ] [ 3.311311214825913e-06 ] [ 4.0738027780411255e-06 ] [ 5.011872336272722e-06 ] [ 6.165950018614822e-06 ] [ 7.585775750291839e-06 ] [ 9.332543007969913e-06 ] [ 1.1481536214968832e-05 ] [ 1.4125375446227536e-05 ] [ 1.737800828749375e-05 ] [ 2.1379620895022316e-05 ] [ 2.6302679918953824e-05 ] [ 3.2359365692962836e-05 ] [ 3.981071705534974e-05 ] [ 4.8977881936844595e-05 ] [ 6.025595860743576e-05 ] [ 7.413102413009174e-05 ] [ 9.120108393559098e-05 ] [ 0.00011220184543019637 ] [ 0.00013803842646028855 ] [ 0.00016982436524617435 ] [ 0.00020892961308540387 ] [ 0.00025703957827688637 ] [ 0.00031622776601683794 ] [ 0.0003890451449942807 ] [ 0.00047863009232263854 ] [ 0.0005888436553555893 ] [ 0.0007244359600749906 ] [ 0.0008912509381337464 ] [ 0.0010964781961431862 ] [ 0.001348962882591652 ] [ 0.0016595869074375593 ] [ 0.002041737944669528 ] [ 0.002511886431509579 ] [ 0.00309029543251359 ] [ 0.003801893963205612 ] [ 0.004677351412871982 ] [ 0.005754399373371571 ] [ 0.007079457843841382 ] [ 0.008709635899560813 ] [ 0.010715193052376074 ] [ 0.013182567385564083 ] [ 0.016218100973589285 ] [ 0.019952623149688778 ] [ 0.024547089156850287 ] [ 0.030199517204020147 ] [ 0.03715352290971724 ] [ 0.0457088189614875 ] [ 0.05623413251903491 ] [ 0.06918309709189367 ] [ 0.08511380382023769 ] [ 0.10471285480509002 ] [ 0.1288249551693135 ] [ 0.1584893192461115 ] [ 0.19498445997580477 ] [ 0.2398832919019488 ] [ 0.29512092266663836 ] [ 0.3630780547701011 ] [ 0.446683592150963 ] [ 0.5495408738576244 ] [ 0.6760829753919818 ] [ 0.8317637711026711 ] [ 1.0232929922807545 ] 2020 - 07 - 02 14 : 31 : 22 , 204 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 31 : 22 , 205 loss diverged - stopping early ! 2020 - 07 - 02 14 : 31 : 22 , 364 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 31 : 22 , 365 learning rate finder finished - plot Path / to / model / output / directory / learning_rate . tsv 2020 - 07 - 02 14 : 31 : 22 , 366 ---------------------------------------------------------------------------------------------------- Learning_rate plots are saved in Path / to / model / output / directory / learning_rate . png Recommended Learning Rate 0.016218100973589285 We can then kick off training below. sc_train_configs = { \"output_dir\" : OUTPUT_DIR , \"learning_rate\" : learning_rate , \"mini_batch_size\" : 32 , \"anneal_factor\" : 0.5 , \"patience\" : 5 , \"max_epochs\" : 150 , \"plot_weights\" : False , \"batch_growth_annealing\" : False , } sc_trainer . train ( ** sc_train_configs ) Output 2020 - 07 - 02 14 : 39 : 15 , 191 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 194 Model : \"TextClassifier( ( document_embeddings ): DocumentRNNEmbeddings ( ( embeddings ): StackedEmbeddings ( ( list_embedding_0 ): BertEmbeddings ( ( model ): BertModel ( ( embeddings ): BertEmbeddings ( ( word_embeddings ): Embedding ( 28996 , 768 , padding_idx = 0 ) ( position_embeddings ): Embedding ( 512 , 768 ) ( token_type_embeddings ): Embedding ( 2 , 768 ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( encoder ): BertEncoder ( ( layer ): ModuleList ( ( 0 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 1 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 2 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 3 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 4 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 5 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 6 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 7 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 8 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 9 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 10 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( 11 ): BertLayer ( ( attention ): BertAttention ( ( self ): BertSelfAttention ( ( query ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( key ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( value ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ( output ): BertSelfOutput ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ( intermediate ): BertIntermediate ( ( dense ): Linear ( in_features = 768 , out_features = 3072 , bias = True ) ) ( output ): BertOutput ( ( dense ): Linear ( in_features = 3072 , out_features = 768 , bias = True ) ( LayerNorm ): LayerNorm (( 768 ,), eps = 1e-12 , elementwise_affine = True ) ( dropout ): Dropout ( p = 0.1 , inplace = False ) ) ) ) ) ( pooler ): BertPooler ( ( dense ): Linear ( in_features = 768 , out_features = 768 , bias = True ) ( activation ): Tanh () ) ) ) ) ( word_reprojection_map ): Linear ( in_features = 3072 , out_features = 256 , bias = True ) ( rnn ): GRU ( 256 , 512 , batch_first = True ) ( dropout ): Dropout ( p = 0.5 , inplace = False ) ) ( decoder ): Linear ( in_features = 512 , out_features = 6 , bias = True ) ( loss_function ): CrossEntropyLoss () ( beta ): 1.0 ( weights ): None ( weight_tensor ) None ) \" 2020 - 07 - 02 14 : 39 : 15 , 196 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 197 Corpus : \"Corpus: 4907 train + 545 dev + 500 test sentences\" 2020 - 07 - 02 14 : 39 : 15 , 198 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 199 Parameters : 2020 - 07 - 02 14 : 39 : 15 , 199 - learning_rate : \"0.016218100973589285\" 2020 - 07 - 02 14 : 39 : 15 , 200 - mini_batch_size : \"32\" 2020 - 07 - 02 14 : 39 : 15 , 201 - patience : \"5\" 2020 - 07 - 02 14 : 39 : 15 , 201 - anneal_factor : \"0.5\" 2020 - 07 - 02 14 : 39 : 15 , 202 - max_epochs : \"150\" 2020 - 07 - 02 14 : 39 : 15 , 203 - shuffle : \"True\" 2020 - 07 - 02 14 : 39 : 15 , 203 - train_with_dev : \"False\" 2020 - 07 - 02 14 : 39 : 15 , 204 - batch_growth_annealing : \"False\" 2020 - 07 - 02 14 : 39 : 15 , 205 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 206 Model training base path : \"Path/to/model/output/directory\" 2020 - 07 - 02 14 : 39 : 15 , 206 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 207 Device : cpu 2020 - 07 - 02 14 : 39 : 15 , 208 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 15 , 209 Embeddings storage mode : cpu 2020 - 07 - 02 14 : 39 : 15 , 214 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 39 : 36 , 964 epoch 1 - iter 15 / 154 - loss 2.12653748 - samples / sec : 22.31 2020 - 07 - 02 14 : 39 : 55 , 974 epoch 1 - iter 30 / 154 - loss 2.03583712 - samples / sec : 25.44 2020 - 07 - 02 14 : 40 : 14 , 865 epoch 1 - iter 45 / 154 - loss 2.02372188 - samples / sec : 25.81 2020 - 07 - 02 14 : 40 : 35 , 062 epoch 1 - iter 60 / 154 - loss 2.03083786 - samples / sec : 23.91 2020 - 07 - 02 14 : 40 : 56 , 537 epoch 1 - iter 75 / 154 - loss 2.00187496 - samples / sec : 22.48 2020 - 07 - 02 14 : 41 : 15 , 410 epoch 1 - iter 90 / 154 - loss 1.98854279 - samples / sec : 25.77 2020 - 07 - 02 14 : 41 : 34 , 838 epoch 1 - iter 105 / 154 - loss 1.97349383 - samples / sec : 24.85 2020 - 07 - 02 14 : 41 : 55 , 114 epoch 1 - iter 120 / 154 - loss 1.96310420 - samples / sec : 23.81 2020 - 07 - 02 14 : 42 : 15 , 951 epoch 1 - iter 135 / 154 - loss 1.94268769 - samples / sec : 23.17 2020 - 07 - 02 14 : 42 : 35 , 886 epoch 1 - iter 150 / 154 - loss 1.92316744 - samples / sec : 24.23 2020 - 07 - 02 14 : 42 : 39 , 888 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 42 : 39 , 890 EPOCH 1 done : loss 1.9191 - lr 0.0162181 2020 - 07 - 02 14 : 43 : 02 , 558 DEV : loss 1.538955569267273 - score 0.7994 2020 - 07 - 02 14 : 43 : 02 , 626 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 14 : 43 : 03 , 149 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 43 : 26 , 692 epoch 2 - iter 15 / 154 - loss 1.69178906 - samples / sec : 20.60 2020 - 07 - 02 14 : 43 : 46 , 382 epoch 2 - iter 30 / 154 - loss 1.70241214 - samples / sec : 24.53 2020 - 07 - 02 14 : 44 : 06 , 135 epoch 2 - iter 45 / 154 - loss 1.69157395 - samples / sec : 24.69 2020 - 07 - 02 14 : 44 : 25 , 998 epoch 2 - iter 60 / 154 - loss 1.68283709 - samples / sec : 24.31 2020 - 07 - 02 14 : 44 : 45 , 498 epoch 2 - iter 75 / 154 - loss 1.65560424 - samples / sec : 24.78 2020 - 07 - 02 14 : 45 : 07 , 466 epoch 2 - iter 90 / 154 - loss 1.64676977 - samples / sec : 21.97 2020 - 07 - 02 14 : 45 : 27 , 106 epoch 2 - iter 105 / 154 - loss 1.63899740 - samples / sec : 24.59 2020 - 07 - 02 14 : 45 : 47 , 150 epoch 2 - iter 120 / 154 - loss 1.62948714 - samples / sec : 24.08 2020 - 07 - 02 14 : 46 : 06 , 680 epoch 2 - iter 135 / 154 - loss 1.61551479 - samples / sec : 24.88 2020 - 07 - 02 14 : 46 : 25 , 444 epoch 2 - iter 150 / 154 - loss 1.59960103 - samples / sec : 25.74 2020 - 07 - 02 14 : 46 : 30 , 454 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 46 : 30 , 455 EPOCH 2 done : loss 1.5982 - lr 0.0162181 2020 - 07 - 02 14 : 46 : 53 , 208 DEV : loss 1.5088865756988525 - score 0.8012 2020 - 07 - 02 14 : 46 : 53 , 278 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 14 : 46 : 57 , 546 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 47 : 19 , 428 epoch 3 - iter 15 / 154 - loss 1.42914403 - samples / sec : 22.43 2020 - 07 - 02 14 : 47 : 39 , 206 epoch 3 - iter 30 / 154 - loss 1.39760986 - samples / sec : 24.41 2020 - 07 - 02 14 : 47 : 58 , 944 epoch 3 - iter 45 / 154 - loss 1.38527177 - samples / sec : 24.45 2020 - 07 - 02 14 : 48 : 17 , 976 epoch 3 - iter 60 / 154 - loss 1.37054651 - samples / sec : 25.58 2020 - 07 - 02 14 : 48 : 37 , 911 epoch 3 - iter 75 / 154 - loss 1.35359440 - samples / sec : 24.21 2020 - 07 - 02 14 : 48 : 56 , 977 epoch 3 - iter 90 / 154 - loss 1.34525723 - samples / sec : 25.52 2020 - 07 - 02 14 : 49 : 15 , 853 epoch 3 - iter 105 / 154 - loss 1.33801149 - samples / sec : 25.58 2020 - 07 - 02 14 : 49 : 36 , 253 epoch 3 - iter 120 / 154 - loss 1.33194426 - samples / sec : 23.66 2020 - 07 - 02 14 : 49 : 56 , 601 epoch 3 - iter 135 / 154 - loss 1.32245981 - samples / sec : 23.89 2020 - 07 - 02 14 : 50 : 16 , 942 epoch 3 - iter 150 / 154 - loss 1.31203588 - samples / sec : 23.73 2020 - 07 - 02 14 : 50 : 21 , 354 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 50 : 21 , 355 EPOCH 3 done : loss 1.3032 - lr 0.0162181 2020 - 07 - 02 14 : 50 : 43 , 683 DEV : loss 1.2037978172302246 - score 0.8544 2020 - 07 - 02 14 : 50 : 43 , 903 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 14 : 50 : 48 , 297 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 51 : 09 , 072 epoch 4 - iter 15 / 154 - loss 1.21338186 - samples / sec : 23.39 2020 - 07 - 02 14 : 51 : 28 , 841 epoch 4 - iter 30 / 154 - loss 1.22687368 - samples / sec : 24.42 2020 - 07 - 02 14 : 51 : 48 , 861 epoch 4 - iter 45 / 154 - loss 1.18060581 - samples / sec : 24.12 2020 - 07 - 02 14 : 52 : 10 , 066 epoch 4 - iter 60 / 154 - loss 1.16962456 - samples / sec : 22.76 2020 - 07 - 02 14 : 52 : 30 , 187 epoch 4 - iter 75 / 154 - loss 1.14393969 - samples / sec : 23.99 2020 - 07 - 02 14 : 52 : 50 , 295 epoch 4 - iter 90 / 154 - loss 1.13386970 - samples / sec : 24.18 2020 - 07 - 02 14 : 53 : 09 , 576 epoch 4 - iter 105 / 154 - loss 1.12137398 - samples / sec : 25.06 2020 - 07 - 02 14 : 53 : 28 , 453 epoch 4 - iter 120 / 154 - loss 1.10854916 - samples / sec : 25.60 2020 - 07 - 02 14 : 53 : 48 , 347 epoch 4 - iter 135 / 154 - loss 1.10391057 - samples / sec : 24.27 2020 - 07 - 02 14 : 54 : 08 , 560 epoch 4 - iter 150 / 154 - loss 1.09810837 - samples / sec : 24.06 2020 - 07 - 02 14 : 54 : 14 , 154 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 54 : 14 , 156 EPOCH 4 done : loss 1.0947 - lr 0.0162181 2020 - 07 - 02 14 : 54 : 36 , 541 DEV : loss 0.9538484215736389 - score 0.8979 2020 - 07 - 02 14 : 54 : 36 , 611 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 14 : 54 : 40 , 893 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 55 : 03 , 151 epoch 5 - iter 15 / 154 - loss 1.02293763 - samples / sec : 22.02 2020 - 07 - 02 14 : 55 : 24 , 535 epoch 5 - iter 30 / 154 - loss 1.01685485 - samples / sec : 22.57 2020 - 07 - 02 14 : 55 : 44 , 082 epoch 5 - iter 45 / 154 - loss 1.00741227 - samples / sec : 24.70 2020 - 07 - 02 14 : 56 : 04 , 053 epoch 5 - iter 60 / 154 - loss 0.99412741 - samples / sec : 24.35 2020 - 07 - 02 14 : 56 : 24 , 697 epoch 5 - iter 75 / 154 - loss 0.97703705 - samples / sec : 23.38 2020 - 07 - 02 14 : 56 : 44 , 511 epoch 5 - iter 90 / 154 - loss 0.95509407 - samples / sec : 24.36 2020 - 07 - 02 14 : 57 : 04 , 272 epoch 5 - iter 105 / 154 - loss 0.95036031 - samples / sec : 24.61 2020 - 07 - 02 14 : 57 : 23 , 543 epoch 5 - iter 120 / 154 - loss 0.94678519 - samples / sec : 25.06 2020 - 07 - 02 14 : 57 : 42 , 375 epoch 5 - iter 135 / 154 - loss 0.93587750 - samples / sec : 25.64 2020 - 07 - 02 14 : 58 : 01 , 328 epoch 5 - iter 150 / 154 - loss 0.93406403 - samples / sec : 25.66 2020 - 07 - 02 14 : 58 : 05 , 957 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 58 : 05 , 959 EPOCH 5 done : loss 0.9277 - lr 0.0162181 2020 - 07 - 02 14 : 58 : 28 , 230 DEV : loss 0.8651217818260193 - score 0.8972 2020 - 07 - 02 14 : 58 : 28 , 297 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 14 : 58 : 28 , 299 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 14 : 58 : 50 , 917 epoch 6 - iter 15 / 154 - loss 0.87874780 - samples / sec : 21.69 2020 - 07 - 02 14 : 59 : 10 , 424 epoch 6 - iter 30 / 154 - loss 0.84258909 - samples / sec : 24.75 2020 - 07 - 02 14 : 59 : 30 , 557 epoch 6 - iter 45 / 154 - loss 0.83004284 - samples / sec : 23.97 2020 - 07 - 02 14 : 59 : 50 , 051 epoch 6 - iter 60 / 154 - loss 0.82869032 - samples / sec : 24.99 2020 - 07 - 02 15 : 00 : 09 , 318 epoch 6 - iter 75 / 154 - loss 0.82800252 - samples / sec : 25.06 2020 - 07 - 02 15 : 00 : 28 , 879 epoch 6 - iter 90 / 154 - loss 0.82355009 - samples / sec : 24.69 2020 - 07 - 02 15 : 00 : 49 , 260 epoch 6 - iter 105 / 154 - loss 0.80982284 - samples / sec : 23.85 2020 - 07 - 02 15 : 01 : 10 , 265 epoch 6 - iter 120 / 154 - loss 0.80053075 - samples / sec : 22.98 2020 - 07 - 02 15 : 01 : 30 , 165 epoch 6 - iter 135 / 154 - loss 0.78381255 - samples / sec : 24.43 2020 - 07 - 02 15 : 01 : 49 , 998 epoch 6 - iter 150 / 154 - loss 0.77709739 - samples / sec : 24.35 2020 - 07 - 02 15 : 01 : 54 , 600 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 01 : 54 , 601 EPOCH 6 done : loss 0.7798 - lr 0.0162181 2020 - 07 - 02 15 : 02 : 16 , 924 DEV : loss 0.5837535262107849 - score 0.9315 2020 - 07 - 02 15 : 02 : 16 , 994 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 02 : 21 , 257 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 02 : 42 , 864 epoch 7 - iter 15 / 154 - loss 0.60823991 - samples / sec : 22.71 2020 - 07 - 02 15 : 03 : 03 , 054 epoch 7 - iter 30 / 154 - loss 0.64749694 - samples / sec : 23.92 2020 - 07 - 02 15 : 03 : 23 , 289 epoch 7 - iter 45 / 154 - loss 0.67237021 - samples / sec : 23.86 2020 - 07 - 02 15 : 03 : 44 , 783 epoch 7 - iter 60 / 154 - loss 0.67263686 - samples / sec : 22.61 2020 - 07 - 02 15 : 04 : 04 , 331 epoch 7 - iter 75 / 154 - loss 0.67410455 - samples / sec : 24.71 2020 - 07 - 02 15 : 04 : 25 , 483 epoch 7 - iter 90 / 154 - loss 0.67185280 - samples / sec : 22.82 2020 - 07 - 02 15 : 04 : 44 , 808 epoch 7 - iter 105 / 154 - loss 0.66940188 - samples / sec : 25.16 2020 - 07 - 02 15 : 05 : 03 , 509 epoch 7 - iter 120 / 154 - loss 0.67044823 - samples / sec : 25.81 2020 - 07 - 02 15 : 05 : 21 , 901 epoch 7 - iter 135 / 154 - loss 0.67026268 - samples / sec : 26.26 2020 - 07 - 02 15 : 05 : 40 , 996 epoch 7 - iter 150 / 154 - loss 0.66505048 - samples / sec : 25.45 2020 - 07 - 02 15 : 05 : 45 , 685 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 05 : 45 , 686 EPOCH 7 done : loss 0.6625 - lr 0.0162181 2020 - 07 - 02 15 : 06 : 07 , 975 DEV : loss 0.532750129699707 - score 0.9346 2020 - 07 - 02 15 : 06 : 08 , 045 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 06 : 12 , 337 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 06 : 32 , 560 epoch 8 - iter 15 / 154 - loss 0.61121991 - samples / sec : 24.32 2020 - 07 - 02 15 : 06 : 52 , 919 epoch 8 - iter 30 / 154 - loss 0.59228445 - samples / sec : 23.71 2020 - 07 - 02 15 : 07 : 12 , 810 epoch 8 - iter 45 / 154 - loss 0.57655472 - samples / sec : 24.28 2020 - 07 - 02 15 : 07 : 32 , 711 epoch 8 - iter 60 / 154 - loss 0.57155969 - samples / sec : 24.27 2020 - 07 - 02 15 : 07 : 52 , 405 epoch 8 - iter 75 / 154 - loss 0.56132450 - samples / sec : 24.51 2020 - 07 - 02 15 : 08 : 13 , 561 epoch 8 - iter 90 / 154 - loss 0.55938630 - samples / sec : 22.81 2020 - 07 - 02 15 : 08 : 35 , 125 epoch 8 - iter 105 / 154 - loss 0.55755164 - samples / sec : 22.51 2020 - 07 - 02 15 : 08 : 54 , 527 epoch 8 - iter 120 / 154 - loss 0.56773651 - samples / sec : 24.89 2020 - 07 - 02 15 : 09 : 14 , 350 epoch 8 - iter 135 / 154 - loss 0.56791281 - samples / sec : 24.35 2020 - 07 - 02 15 : 09 : 34 , 306 epoch 8 - iter 150 / 154 - loss 0.56556819 - samples / sec : 24.36 2020 - 07 - 02 15 : 09 : 39 , 031 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 09 : 39 , 033 EPOCH 8 done : loss 0.5619 - lr 0.0162181 2020 - 07 - 02 15 : 10 : 01 , 373 DEV : loss 0.6124246716499329 - score 0.9138 2020 - 07 - 02 15 : 10 : 01 , 443 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 15 : 10 : 01 , 444 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 10 : 22 , 741 epoch 9 - iter 15 / 154 - loss 0.52073216 - samples / sec : 23.09 2020 - 07 - 02 15 : 10 : 41 , 808 epoch 9 - iter 30 / 154 - loss 0.51747889 - samples / sec : 25.35 2020 - 07 - 02 15 : 11 : 03 , 530 epoch 9 - iter 45 / 154 - loss 0.50766587 - samples / sec : 22.22 2020 - 07 - 02 15 : 11 : 22 , 831 epoch 9 - iter 60 / 154 - loss 0.49717654 - samples / sec : 25.19 2020 - 07 - 02 15 : 11 : 43 , 282 epoch 9 - iter 75 / 154 - loss 0.48794214 - samples / sec : 23.61 2020 - 07 - 02 15 : 12 : 03 , 162 epoch 9 - iter 90 / 154 - loss 0.48554325 - samples / sec : 24.28 2020 - 07 - 02 15 : 12 : 22 , 939 epoch 9 - iter 105 / 154 - loss 0.48294531 - samples / sec : 24.58 2020 - 07 - 02 15 : 12 : 42 , 470 epoch 9 - iter 120 / 154 - loss 0.47932543 - samples / sec : 24.72 2020 - 07 - 02 15 : 13 : 02 , 512 epoch 9 - iter 135 / 154 - loss 0.48639485 - samples / sec : 24.09 2020 - 07 - 02 15 : 13 : 22 , 715 epoch 9 - iter 150 / 154 - loss 0.48241082 - samples / sec : 24.10 2020 - 07 - 02 15 : 13 : 27 , 354 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 13 : 27 , 356 EPOCH 9 done : loss 0.4791 - lr 0.0162181 2020 - 07 - 02 15 : 13 : 49 , 788 DEV : loss 0.39634451270103455 - score 0.9511 2020 - 07 - 02 15 : 13 : 49 , 893 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 13 : 54 , 198 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 14 : 15 , 385 epoch 10 - iter 15 / 154 - loss 0.39700798 - samples / sec : 22.97 2020 - 07 - 02 15 : 14 : 35 , 579 epoch 10 - iter 30 / 154 - loss 0.44900593 - samples / sec : 24.16 2020 - 07 - 02 15 : 14 : 55 , 026 epoch 10 - iter 45 / 154 - loss 0.43003887 - samples / sec : 24.84 2020 - 07 - 02 15 : 15 : 14 , 794 epoch 10 - iter 60 / 154 - loss 0.44525786 - samples / sec : 24.45 2020 - 07 - 02 15 : 15 : 34 , 512 epoch 10 - iter 75 / 154 - loss 0.45256295 - samples / sec : 24.49 2020 - 07 - 02 15 : 15 : 53 , 671 epoch 10 - iter 90 / 154 - loss 0.45120489 - samples / sec : 25.21 2020 - 07 - 02 15 : 16 : 15 , 627 epoch 10 - iter 105 / 154 - loss 0.44198098 - samples / sec : 22.12 2020 - 07 - 02 15 : 16 : 36 , 052 epoch 10 - iter 120 / 154 - loss 0.43900539 - samples / sec : 23.63 2020 - 07 - 02 15 : 16 : 54 , 831 epoch 10 - iter 135 / 154 - loss 0.44348369 - samples / sec : 25.71 2020 - 07 - 02 15 : 17 : 14 , 348 epoch 10 - iter 150 / 154 - loss 0.44872592 - samples / sec : 24.91 2020 - 07 - 02 15 : 17 : 19 , 719 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 17 : 19 , 721 EPOCH 10 done : loss 0.4485 - lr 0.0162181 2020 - 07 - 02 15 : 17 : 42 , 071 DEV : loss 0.37473350763320923 - score 0.9554 2020 - 07 - 02 15 : 17 : 42 , 141 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 17 : 46 , 432 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 18 : 07 , 509 epoch 11 - iter 15 / 154 - loss 0.42587440 - samples / sec : 23.07 2020 - 07 - 02 15 : 18 : 26 , 425 epoch 11 - iter 30 / 154 - loss 0.41586999 - samples / sec : 25.55 2020 - 07 - 02 15 : 18 : 45 , 758 epoch 11 - iter 45 / 154 - loss 0.40622304 - samples / sec : 24.98 2020 - 07 - 02 15 : 19 : 08 , 078 epoch 11 - iter 60 / 154 - loss 0.41316052 - samples / sec : 21.62 2020 - 07 - 02 15 : 19 : 27 , 885 epoch 11 - iter 75 / 154 - loss 0.42014157 - samples / sec : 24.55 2020 - 07 - 02 15 : 19 : 46 , 742 epoch 11 - iter 90 / 154 - loss 0.40332305 - samples / sec : 25.61 2020 - 07 - 02 15 : 20 : 06 , 936 epoch 11 - iter 105 / 154 - loss 0.40566851 - samples / sec : 24.06 2020 - 07 - 02 15 : 20 : 27 , 452 epoch 11 - iter 120 / 154 - loss 0.40743910 - samples / sec : 23.53 2020 - 07 - 02 15 : 20 : 47 , 670 epoch 11 - iter 135 / 154 - loss 0.40461053 - samples / sec : 23.88 2020 - 07 - 02 15 : 21 : 07 , 230 epoch 11 - iter 150 / 154 - loss 0.40773223 - samples / sec : 24.69 2020 - 07 - 02 15 : 21 : 12 , 274 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 21 : 12 , 275 EPOCH 11 done : loss 0.4066 - lr 0.0162181 2020 - 07 - 02 15 : 21 : 34 , 988 DEV : loss 0.37664657831192017 - score 0.9498 2020 - 07 - 02 15 : 21 : 35 , 056 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 15 : 21 : 35 , 057 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 21 : 55 , 878 epoch 12 - iter 15 / 154 - loss 0.35145220 - samples / sec : 23.35 2020 - 07 - 02 15 : 22 : 15 , 511 epoch 12 - iter 30 / 154 - loss 0.35834764 - samples / sec : 24.87 2020 - 07 - 02 15 : 22 : 34 , 140 epoch 12 - iter 45 / 154 - loss 0.35214746 - samples / sec : 25.95 2020 - 07 - 02 15 : 22 : 53 , 836 epoch 12 - iter 60 / 154 - loss 0.35563465 - samples / sec : 24.53 2020 - 07 - 02 15 : 23 : 14 , 561 epoch 12 - iter 75 / 154 - loss 0.35541137 - samples / sec : 23.29 2020 - 07 - 02 15 : 23 : 35 , 306 epoch 12 - iter 90 / 154 - loss 0.35891361 - samples / sec : 23.46 2020 - 07 - 02 15 : 23 : 55 , 845 epoch 12 - iter 105 / 154 - loss 0.36141122 - samples / sec : 23.50 2020 - 07 - 02 15 : 24 : 16 , 115 epoch 12 - iter 120 / 154 - loss 0.36918869 - samples / sec : 23.83 2020 - 07 - 02 15 : 24 : 36 , 956 epoch 12 - iter 135 / 154 - loss 0.37309432 - samples / sec : 23.30 2020 - 07 - 02 15 : 24 : 55 , 936 epoch 12 - iter 150 / 154 - loss 0.37402713 - samples / sec : 25.45 2020 - 07 - 02 15 : 25 : 01 , 049 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 25 : 01 , 051 EPOCH 12 done : loss 0.3721 - lr 0.0162181 2020 - 07 - 02 15 : 25 : 24 , 063 DEV : loss 0.3611939251422882 - score 0.9523 2020 - 07 - 02 15 : 25 : 24 , 131 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 15 : 25 : 24 , 132 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 25 : 46 , 995 epoch 13 - iter 15 / 154 - loss 0.38793649 - samples / sec : 21.25 2020 - 07 - 02 15 : 26 : 08 , 016 epoch 13 - iter 30 / 154 - loss 0.38421851 - samples / sec : 22.98 2020 - 07 - 02 15 : 26 : 28 , 932 epoch 13 - iter 45 / 154 - loss 0.36525546 - samples / sec : 23.28 2020 - 07 - 02 15 : 26 : 49 , 413 epoch 13 - iter 60 / 154 - loss 0.36084372 - samples / sec : 23.57 2020 - 07 - 02 15 : 27 : 08 , 492 epoch 13 - iter 75 / 154 - loss 0.35162010 - samples / sec : 25.47 2020 - 07 - 02 15 : 27 : 28 , 847 epoch 13 - iter 90 / 154 - loss 0.35324366 - samples / sec : 23.76 2020 - 07 - 02 15 : 27 : 48 , 163 epoch 13 - iter 105 / 154 - loss 0.35290401 - samples / sec : 25.01 2020 - 07 - 02 15 : 28 : 06 , 650 epoch 13 - iter 120 / 154 - loss 0.35728267 - samples / sec : 26.34 2020 - 07 - 02 15 : 28 : 26 , 045 epoch 13 - iter 135 / 154 - loss 0.35024357 - samples / sec : 24.89 2020 - 07 - 02 15 : 28 : 46 , 154 epoch 13 - iter 150 / 154 - loss 0.34566470 - samples / sec : 24.00 2020 - 07 - 02 15 : 28 : 50 , 959 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 28 : 50 , 960 EPOCH 13 done : loss 0.3445 - lr 0.0162181 2020 - 07 - 02 15 : 29 : 14 , 173 DEV : loss 0.33947789669036865 - score 0.9578 2020 - 07 - 02 15 : 29 : 14 , 240 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 29 : 18 , 531 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 29 : 40 , 891 epoch 14 - iter 15 / 154 - loss 0.38821750 - samples / sec : 21.75 2020 - 07 - 02 15 : 29 : 59 , 401 epoch 14 - iter 30 / 154 - loss 0.35584508 - samples / sec : 26.38 2020 - 07 - 02 15 : 30 : 20 , 485 epoch 14 - iter 45 / 154 - loss 0.33287027 - samples / sec : 22.90 2020 - 07 - 02 15 : 30 : 40 , 526 epoch 14 - iter 60 / 154 - loss 0.33028220 - samples / sec : 24.09 2020 - 07 - 02 15 : 30 : 59 , 343 epoch 14 - iter 75 / 154 - loss 0.33653424 - samples / sec : 25.67 2020 - 07 - 02 15 : 31 : 19 , 277 epoch 14 - iter 90 / 154 - loss 0.33223337 - samples / sec : 24.23 2020 - 07 - 02 15 : 31 : 38 , 786 epoch 14 - iter 105 / 154 - loss 0.32999784 - samples / sec : 24.76 2020 - 07 - 02 15 : 31 : 58 , 166 epoch 14 - iter 120 / 154 - loss 0.32390204 - samples / sec : 24.94 2020 - 07 - 02 15 : 32 : 18 , 669 epoch 14 - iter 135 / 154 - loss 0.31847246 - samples / sec : 23.70 2020 - 07 - 02 15 : 32 : 38 , 980 epoch 14 - iter 150 / 154 - loss 0.31818390 - samples / sec : 23.78 2020 - 07 - 02 15 : 32 : 43 , 782 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 32 : 43 , 784 EPOCH 14 done : loss 0.3160 - lr 0.0162181 2020 - 07 - 02 15 : 33 : 06 , 216 DEV : loss 0.33674922585487366 - score 0.9517 2020 - 07 - 02 15 : 33 : 06 , 286 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 15 : 33 : 06 , 288 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 33 : 28 , 131 epoch 15 - iter 15 / 154 - loss 0.32382143 - samples / sec : 22.46 2020 - 07 - 02 15 : 33 : 47 , 269 epoch 15 - iter 30 / 154 - loss 0.32938266 - samples / sec : 25.24 2020 - 07 - 02 15 : 34 : 08 , 013 epoch 15 - iter 45 / 154 - loss 0.35580096 - samples / sec : 23.27 2020 - 07 - 02 15 : 34 : 28 , 451 epoch 15 - iter 60 / 154 - loss 0.33785067 - samples / sec : 23.79 2020 - 07 - 02 15 : 34 : 47 , 864 epoch 15 - iter 75 / 154 - loss 0.32975845 - samples / sec : 24.87 2020 - 07 - 02 15 : 35 : 08 , 211 epoch 15 - iter 90 / 154 - loss 0.31067502 - samples / sec : 23.74 2020 - 07 - 02 15 : 35 : 27 , 686 epoch 15 - iter 105 / 154 - loss 0.30810503 - samples / sec : 24.79 2020 - 07 - 02 15 : 35 : 48 , 510 epoch 15 - iter 120 / 154 - loss 0.30373972 - samples / sec : 23.17 2020 - 07 - 02 15 : 36 : 09 , 118 epoch 15 - iter 135 / 154 - loss 0.30006551 - samples / sec : 23.44 2020 - 07 - 02 15 : 36 : 28 , 709 epoch 15 - iter 150 / 154 - loss 0.29886991 - samples / sec : 24.64 2020 - 07 - 02 15 : 36 : 33 , 721 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 36 : 33 , 723 EPOCH 15 done : loss 0.3044 - lr 0.0162181 2020 - 07 - 02 15 : 36 : 56 , 211 DEV : loss 0.45606178045272827 - score 0.9517 2020 - 07 - 02 15 : 36 : 56 , 282 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 15 : 36 : 56 , 284 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 37 : 16 , 899 epoch 16 - iter 15 / 154 - loss 0.26865751 - samples / sec : 23.84 2020 - 07 - 02 15 : 37 : 38 , 328 epoch 16 - iter 30 / 154 - loss 0.25540573 - samples / sec : 22.51 2020 - 07 - 02 15 : 37 : 57 , 159 epoch 16 - iter 45 / 154 - loss 0.26173169 - samples / sec : 25.65 2020 - 07 - 02 15 : 38 : 17 , 127 epoch 16 - iter 60 / 154 - loss 0.26441356 - samples / sec : 24.17 2020 - 07 - 02 15 : 38 : 37 , 460 epoch 16 - iter 75 / 154 - loss 0.27951374 - samples / sec : 23.75 2020 - 07 - 02 15 : 38 : 57 , 341 epoch 16 - iter 90 / 154 - loss 0.28452394 - samples / sec : 24.43 2020 - 07 - 02 15 : 39 : 17 , 755 epoch 16 - iter 105 / 154 - loss 0.28827544 - samples / sec : 23.65 2020 - 07 - 02 15 : 39 : 37 , 319 epoch 16 - iter 120 / 154 - loss 0.29456732 - samples / sec : 24.69 2020 - 07 - 02 15 : 39 : 57 , 241 epoch 16 - iter 135 / 154 - loss 0.29623859 - samples / sec : 24.40 2020 - 07 - 02 15 : 40 : 17 , 446 epoch 16 - iter 150 / 154 - loss 0.29836056 - samples / sec : 23.89 2020 - 07 - 02 15 : 40 : 22 , 068 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 40 : 22 , 069 EPOCH 16 done : loss 0.3000 - lr 0.0162181 2020 - 07 - 02 15 : 40 : 44 , 528 DEV : loss 0.36227965354919434 - score 0.956 2020 - 07 - 02 15 : 40 : 44 , 597 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 15 : 40 : 44 , 598 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 41 : 06 , 680 epoch 17 - iter 15 / 154 - loss 0.32204509 - samples / sec : 22.22 2020 - 07 - 02 15 : 41 : 27 , 155 epoch 17 - iter 30 / 154 - loss 0.28370522 - samples / sec : 23.60 2020 - 07 - 02 15 : 41 : 47 , 382 epoch 17 - iter 45 / 154 - loss 0.28329503 - samples / sec : 23.87 2020 - 07 - 02 15 : 42 : 06 , 345 epoch 17 - iter 60 / 154 - loss 0.28177566 - samples / sec : 25.66 2020 - 07 - 02 15 : 42 : 26 , 573 epoch 17 - iter 75 / 154 - loss 0.28397036 - samples / sec : 23.86 2020 - 07 - 02 15 : 42 : 47 , 908 epoch 17 - iter 90 / 154 - loss 0.28389345 - samples / sec : 22.63 2020 - 07 - 02 15 : 43 : 08 , 229 epoch 17 - iter 105 / 154 - loss 0.27705963 - samples / sec : 23.90 2020 - 07 - 02 15 : 43 : 28 , 245 epoch 17 - iter 120 / 154 - loss 0.27206560 - samples / sec : 24.13 2020 - 07 - 02 15 : 43 : 47 , 708 epoch 17 - iter 135 / 154 - loss 0.27306891 - samples / sec : 24.80 2020 - 07 - 02 15 : 44 : 06 , 403 epoch 17 - iter 150 / 154 - loss 0.27053858 - samples / sec : 25.85 2020 - 07 - 02 15 : 44 : 10 , 964 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 44 : 10 , 965 EPOCH 17 done : loss 0.2706 - lr 0.0162181 2020 - 07 - 02 15 : 44 : 33 , 212 DEV : loss 0.3132312595844269 - score 0.9682 2020 - 07 - 02 15 : 44 : 33 , 281 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 44 : 37 , 552 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 45 : 00 , 030 epoch 18 - iter 15 / 154 - loss 0.25208958 - samples / sec : 21.83 2020 - 07 - 02 15 : 45 : 19 , 063 epoch 18 - iter 30 / 154 - loss 0.27032827 - samples / sec : 25.37 2020 - 07 - 02 15 : 45 : 39 , 871 epoch 18 - iter 45 / 154 - loss 0.26369591 - samples / sec : 23.35 2020 - 07 - 02 15 : 46 : 00 , 460 epoch 18 - iter 60 / 154 - loss 0.27755907 - samples / sec : 23.44 2020 - 07 - 02 15 : 46 : 21 , 331 epoch 18 - iter 75 / 154 - loss 0.26805330 - samples / sec : 23.14 2020 - 07 - 02 15 : 46 : 39 , 926 epoch 18 - iter 90 / 154 - loss 0.26553340 - samples / sec : 25.97 2020 - 07 - 02 15 : 47 : 00 , 419 epoch 18 - iter 105 / 154 - loss 0.26726629 - samples / sec : 23.71 2020 - 07 - 02 15 : 47 : 19 , 530 epoch 18 - iter 120 / 154 - loss 0.26267663 - samples / sec : 25.27 2020 - 07 - 02 15 : 47 : 38 , 403 epoch 18 - iter 135 / 154 - loss 0.26108754 - samples / sec : 25.76 2020 - 07 - 02 15 : 47 : 58 , 105 epoch 18 - iter 150 / 154 - loss 0.26345694 - samples / sec : 24.51 2020 - 07 - 02 15 : 48 : 02 , 615 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 48 : 02 , 617 EPOCH 18 done : loss 0.2690 - lr 0.0162181 2020 - 07 - 02 15 : 48 : 24 , 979 DEV : loss 0.24188274145126343 - score 0.9706 2020 - 07 - 02 15 : 48 : 25 , 049 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 48 : 29 , 343 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 48 : 52 , 980 epoch 19 - iter 15 / 154 - loss 0.26782284 - samples / sec : 20.73 2020 - 07 - 02 15 : 49 : 12 , 800 epoch 19 - iter 30 / 154 - loss 0.25746150 - samples / sec : 24.36 2020 - 07 - 02 15 : 49 : 32 , 595 epoch 19 - iter 45 / 154 - loss 0.25379602 - samples / sec : 24.38 2020 - 07 - 02 15 : 49 : 51 , 373 epoch 19 - iter 60 / 154 - loss 0.25453721 - samples / sec : 25.73 2020 - 07 - 02 15 : 50 : 12 , 023 epoch 19 - iter 75 / 154 - loss 0.24971705 - samples / sec : 23.39 2020 - 07 - 02 15 : 50 : 33 , 769 epoch 19 - iter 90 / 154 - loss 0.25132714 - samples / sec : 22.20 2020 - 07 - 02 15 : 50 : 52 , 815 epoch 19 - iter 105 / 154 - loss 0.24608561 - samples / sec : 25.34 2020 - 07 - 02 15 : 51 : 12 , 130 epoch 19 - iter 120 / 154 - loss 0.24540012 - samples / sec : 25.01 2020 - 07 - 02 15 : 51 : 30 , 127 epoch 19 - iter 135 / 154 - loss 0.24441875 - samples / sec : 26.82 2020 - 07 - 02 15 : 51 : 50 , 334 epoch 19 - iter 150 / 154 - loss 0.24105846 - samples / sec : 24.05 2020 - 07 - 02 15 : 51 : 55 , 900 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 51 : 55 , 902 EPOCH 19 done : loss 0.2417 - lr 0.0162181 2020 - 07 - 02 15 : 52 : 18 , 249 DEV : loss 0.2675461173057556 - score 0.9645 2020 - 07 - 02 15 : 52 : 18 , 315 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 15 : 52 : 18 , 317 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 52 : 40 , 145 epoch 20 - iter 15 / 154 - loss 0.22299275 - samples / sec : 22.48 2020 - 07 - 02 15 : 52 : 59 , 619 epoch 20 - iter 30 / 154 - loss 0.23901086 - samples / sec : 24.79 2020 - 07 - 02 15 : 53 : 20 , 568 epoch 20 - iter 45 / 154 - loss 0.22591903 - samples / sec : 23.04 2020 - 07 - 02 15 : 53 : 40 , 829 epoch 20 - iter 60 / 154 - loss 0.21634991 - samples / sec : 23.98 2020 - 07 - 02 15 : 54 : 01 , 041 epoch 20 - iter 75 / 154 - loss 0.22712179 - samples / sec : 23.89 2020 - 07 - 02 15 : 54 : 21 , 699 epoch 20 - iter 90 / 154 - loss 0.22611159 - samples / sec : 23.37 2020 - 07 - 02 15 : 54 : 41 , 296 epoch 20 - iter 105 / 154 - loss 0.22637414 - samples / sec : 24.81 2020 - 07 - 02 15 : 55 : 01 , 411 epoch 20 - iter 120 / 154 - loss 0.22813549 - samples / sec : 24.00 2020 - 07 - 02 15 : 55 : 21 , 366 epoch 20 - iter 135 / 154 - loss 0.22931698 - samples / sec : 24.20 2020 - 07 - 02 15 : 55 : 41 , 199 epoch 20 - iter 150 / 154 - loss 0.23357049 - samples / sec : 24.54 2020 - 07 - 02 15 : 55 : 45 , 334 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 55 : 45 , 335 EPOCH 20 done : loss 0.2297 - lr 0.0162181 2020 - 07 - 02 15 : 56 : 07 , 628 DEV : loss 0.23396362364292145 - score 0.9719 2020 - 07 - 02 15 : 56 : 07 , 700 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 15 : 56 : 11 , 976 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 56 : 33 , 755 epoch 21 - iter 15 / 154 - loss 0.24369793 - samples / sec : 22.53 2020 - 07 - 02 15 : 56 : 53 , 590 epoch 21 - iter 30 / 154 - loss 0.22235053 - samples / sec : 24.33 2020 - 07 - 02 15 : 57 : 13 , 517 epoch 21 - iter 45 / 154 - loss 0.22550796 - samples / sec : 24.23 2020 - 07 - 02 15 : 57 : 33 , 431 epoch 21 - iter 60 / 154 - loss 0.22605099 - samples / sec : 24.22 2020 - 07 - 02 15 : 57 : 53 , 093 epoch 21 - iter 75 / 154 - loss 0.21876556 - samples / sec : 24.57 2020 - 07 - 02 15 : 58 : 12 , 986 epoch 21 - iter 90 / 154 - loss 0.22181167 - samples / sec : 24.28 2020 - 07 - 02 15 : 58 : 33 , 326 epoch 21 - iter 105 / 154 - loss 0.22137965 - samples / sec : 23.75 2020 - 07 - 02 15 : 58 : 53 , 510 epoch 21 - iter 120 / 154 - loss 0.22005988 - samples / sec : 24.07 2020 - 07 - 02 15 : 59 : 14 , 691 epoch 21 - iter 135 / 154 - loss 0.22334215 - samples / sec : 22.80 2020 - 07 - 02 15 : 59 : 33 , 247 epoch 21 - iter 150 / 154 - loss 0.22272228 - samples / sec : 26.02 2020 - 07 - 02 15 : 59 : 38 , 154 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 15 : 59 : 38 , 156 EPOCH 21 done : loss 0.2209 - lr 0.0162181 2020 - 07 - 02 16 : 00 : 00 , 536 DEV : loss 0.2306308150291443 - score 0.9737 2020 - 07 - 02 16 : 00 : 00 , 619 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 16 : 00 : 04 , 925 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 00 : 27 , 080 epoch 22 - iter 15 / 154 - loss 0.15147622 - samples / sec : 21.95 2020 - 07 - 02 16 : 00 : 48 , 794 epoch 22 - iter 30 / 154 - loss 0.23374797 - samples / sec : 22.23 2020 - 07 - 02 16 : 01 : 07 , 535 epoch 22 - iter 45 / 154 - loss 0.23072744 - samples / sec : 25.76 2020 - 07 - 02 16 : 01 : 28 , 454 epoch 22 - iter 60 / 154 - loss 0.23073106 - samples / sec : 23.22 2020 - 07 - 02 16 : 01 : 47 , 677 epoch 22 - iter 75 / 154 - loss 0.22344845 - samples / sec : 25.13 2020 - 07 - 02 16 : 02 : 06 , 080 epoch 22 - iter 90 / 154 - loss 0.21383200 - samples / sec : 26.24 2020 - 07 - 02 16 : 02 : 25 , 456 epoch 22 - iter 105 / 154 - loss 0.21821867 - samples / sec : 25.10 2020 - 07 - 02 16 : 02 : 46 , 316 epoch 22 - iter 120 / 154 - loss 0.21940228 - samples / sec : 23.14 2020 - 07 - 02 16 : 03 : 06 , 127 epoch 22 - iter 135 / 154 - loss 0.21804290 - samples / sec : 24.56 2020 - 07 - 02 16 : 03 : 25 , 057 epoch 22 - iter 150 / 154 - loss 0.21798164 - samples / sec : 25.51 2020 - 07 - 02 16 : 03 : 29 , 479 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 03 : 29 , 481 EPOCH 22 done : loss 0.2170 - lr 0.0162181 2020 - 07 - 02 16 : 03 : 51 , 825 DEV : loss 0.2819221615791321 - score 0.9651 2020 - 07 - 02 16 : 03 : 51 , 897 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 16 : 03 : 51 , 900 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 04 : 14 , 615 epoch 23 - iter 15 / 154 - loss 0.20381826 - samples / sec : 21.38 2020 - 07 - 02 16 : 04 : 36 , 216 epoch 23 - iter 30 / 154 - loss 0.21659263 - samples / sec : 22.34 2020 - 07 - 02 16 : 04 : 55 , 141 epoch 23 - iter 45 / 154 - loss 0.20468760 - samples / sec : 25.52 2020 - 07 - 02 16 : 05 : 14 , 993 epoch 23 - iter 60 / 154 - loss 0.20271637 - samples / sec : 24.48 2020 - 07 - 02 16 : 05 : 34 , 130 epoch 23 - iter 75 / 154 - loss 0.19821025 - samples / sec : 25.23 2020 - 07 - 02 16 : 05 : 54 , 292 epoch 23 - iter 90 / 154 - loss 0.20070277 - samples / sec : 24.09 2020 - 07 - 02 16 : 06 : 14 , 437 epoch 23 - iter 105 / 154 - loss 0.20031097 - samples / sec : 23.97 2020 - 07 - 02 16 : 06 : 34 , 083 epoch 23 - iter 120 / 154 - loss 0.20813754 - samples / sec : 24.58 2020 - 07 - 02 16 : 06 : 52 , 903 epoch 23 - iter 135 / 154 - loss 0.21487906 - samples / sec : 25.88 2020 - 07 - 02 16 : 07 : 13 , 310 epoch 23 - iter 150 / 154 - loss 0.20735793 - samples / sec : 23.65 2020 - 07 - 02 16 : 07 : 17 , 500 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 07 : 17 , 501 EPOCH 23 done : loss 0.2128 - lr 0.0162181 2020 - 07 - 02 16 : 07 : 39 , 965 DEV : loss 0.23975355923175812 - score 0.9719 2020 - 07 - 02 16 : 07 : 40 , 036 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 16 : 07 : 40 , 037 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 08 : 02 , 215 epoch 24 - iter 15 / 154 - loss 0.24556303 - samples / sec : 22.19 2020 - 07 - 02 16 : 08 : 21 , 804 epoch 24 - iter 30 / 154 - loss 0.21485928 - samples / sec : 24.66 2020 - 07 - 02 16 : 08 : 42 , 513 epoch 24 - iter 45 / 154 - loss 0.20294512 - samples / sec : 23.31 2020 - 07 - 02 16 : 09 : 02 , 210 epoch 24 - iter 60 / 154 - loss 0.20490214 - samples / sec : 24.55 2020 - 07 - 02 16 : 09 : 23 , 391 epoch 24 - iter 75 / 154 - loss 0.21047717 - samples / sec : 22.93 2020 - 07 - 02 16 : 09 : 43 , 197 epoch 24 - iter 90 / 154 - loss 0.20396163 - samples / sec : 24.38 2020 - 07 - 02 16 : 10 : 04 , 245 epoch 24 - iter 105 / 154 - loss 0.20865799 - samples / sec : 22.94 2020 - 07 - 02 16 : 10 : 22 , 417 epoch 24 - iter 120 / 154 - loss 0.21269711 - samples / sec : 26.59 2020 - 07 - 02 16 : 10 : 41 , 693 epoch 24 - iter 135 / 154 - loss 0.20936885 - samples / sec : 25.06 2020 - 07 - 02 16 : 11 : 01 , 235 epoch 24 - iter 150 / 154 - loss 0.21083501 - samples / sec : 24.71 2020 - 07 - 02 16 : 11 : 05 , 323 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 11 : 05 , 324 EPOCH 24 done : loss 0.2130 - lr 0.0162181 2020 - 07 - 02 16 : 11 : 27 , 865 DEV : loss 0.2661949396133423 - score 0.9737 2020 - 07 - 02 16 : 11 : 27 , 934 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 16 : 11 : 27 , 936 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 11 : 50 , 567 epoch 25 - iter 15 / 154 - loss 0.24687227 - samples / sec : 21.46 2020 - 07 - 02 16 : 12 : 08 , 728 epoch 25 - iter 30 / 154 - loss 0.21076790 - samples / sec : 26.60 2020 - 07 - 02 16 : 12 : 29 , 997 epoch 25 - iter 45 / 154 - loss 0.18616831 - samples / sec : 22.90 2020 - 07 - 02 16 : 12 : 48 , 632 epoch 25 - iter 60 / 154 - loss 0.18038774 - samples / sec : 25.93 2020 - 07 - 02 16 : 13 : 09 , 459 epoch 25 - iter 75 / 154 - loss 0.18513621 - samples / sec : 23.33 2020 - 07 - 02 16 : 13 : 30 , 101 epoch 25 - iter 90 / 154 - loss 0.18450534 - samples / sec : 23.39 2020 - 07 - 02 16 : 13 : 49 , 169 epoch 25 - iter 105 / 154 - loss 0.18219731 - samples / sec : 25.32 2020 - 07 - 02 16 : 14 : 09 , 165 epoch 25 - iter 120 / 154 - loss 0.18160356 - samples / sec : 24.31 2020 - 07 - 02 16 : 14 : 29 , 648 epoch 25 - iter 135 / 154 - loss 0.18727879 - samples / sec : 23.57 2020 - 07 - 02 16 : 14 : 49 , 182 epoch 25 - iter 150 / 154 - loss 0.18395177 - samples / sec : 24.71 2020 - 07 - 02 16 : 14 : 54 , 658 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 14 : 54 , 660 EPOCH 25 done : loss 0.1870 - lr 0.0162181 2020 - 07 - 02 16 : 15 : 17 , 417 DEV : loss 0.24831292033195496 - score 0.9725 2020 - 07 - 02 16 : 15 : 17 , 486 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 16 : 15 : 17 , 487 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 15 : 40 , 274 epoch 26 - iter 15 / 154 - loss 0.15373853 - samples / sec : 21.31 2020 - 07 - 02 16 : 16 : 00 , 506 epoch 26 - iter 30 / 154 - loss 0.16712674 - samples / sec : 23.88 2020 - 07 - 02 16 : 16 : 21 , 136 epoch 26 - iter 45 / 154 - loss 0.17038985 - samples / sec : 23.63 2020 - 07 - 02 16 : 16 : 40 , 021 epoch 26 - iter 60 / 154 - loss 0.17567901 - samples / sec : 25.57 2020 - 07 - 02 16 : 16 : 59 , 197 epoch 26 - iter 75 / 154 - loss 0.18035345 - samples / sec : 25.19 2020 - 07 - 02 16 : 17 : 18 , 590 epoch 26 - iter 90 / 154 - loss 0.18694772 - samples / sec : 25.07 2020 - 07 - 02 16 : 17 : 38 , 407 epoch 26 - iter 105 / 154 - loss 0.19046586 - samples / sec : 24.37 2020 - 07 - 02 16 : 17 : 59 , 785 epoch 26 - iter 120 / 154 - loss 0.18908963 - samples / sec : 22.58 2020 - 07 - 02 16 : 18 : 19 , 241 epoch 26 - iter 135 / 154 - loss 0.18909395 - samples / sec : 24.82 2020 - 07 - 02 16 : 18 : 39 , 838 epoch 26 - iter 150 / 154 - loss 0.18379643 - samples / sec : 23.59 2020 - 07 - 02 16 : 18 : 44 , 586 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 18 : 44 , 588 EPOCH 26 done : loss 0.1831 - lr 0.0162181 2020 - 07 - 02 16 : 19 : 07 , 027 DEV : loss 0.23407800495624542 - score 0.9719 2020 - 07 - 02 16 : 19 : 07 , 096 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 16 : 19 : 07 , 097 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 19 : 28 , 706 epoch 27 - iter 15 / 154 - loss 0.20348703 - samples / sec : 22.48 2020 - 07 - 02 16 : 19 : 48 , 120 epoch 27 - iter 30 / 154 - loss 0.16796032 - samples / sec : 24.88 2020 - 07 - 02 16 : 20 : 07 , 942 epoch 27 - iter 45 / 154 - loss 0.17932437 - samples / sec : 24.36 2020 - 07 - 02 16 : 20 : 27 , 121 epoch 27 - iter 60 / 154 - loss 0.17203238 - samples / sec : 25.18 2020 - 07 - 02 16 : 20 : 48 , 312 epoch 27 - iter 75 / 154 - loss 0.16742311 - samples / sec : 22.94 2020 - 07 - 02 16 : 21 : 08 , 425 epoch 27 - iter 90 / 154 - loss 0.17116617 - samples / sec : 24.00 2020 - 07 - 02 16 : 21 : 28 , 036 epoch 27 - iter 105 / 154 - loss 0.17072401 - samples / sec : 24.79 2020 - 07 - 02 16 : 21 : 47 , 868 epoch 27 - iter 120 / 154 - loss 0.17486551 - samples / sec : 24.34 2020 - 07 - 02 16 : 22 : 08 , 240 epoch 27 - iter 135 / 154 - loss 0.17548364 - samples / sec : 23.70 2020 - 07 - 02 16 : 22 : 28 , 509 epoch 27 - iter 150 / 154 - loss 0.17920635 - samples / sec : 24.04 2020 - 07 - 02 16 : 22 : 32 , 904 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 22 : 32 , 906 EPOCH 27 done : loss 0.1777 - lr 0.0162181 2020 - 07 - 02 16 : 22 : 55 , 495 DEV : loss 0.3410264253616333 - score 0.9554 Epoch 27 : reducing learning rate of group 0 to 8.1091e-03 . 2020 - 07 - 02 16 : 22 : 55 , 577 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 16 : 22 : 55 , 578 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 23 : 16 , 576 epoch 28 - iter 15 / 154 - loss 0.18262969 - samples / sec : 23.17 2020 - 07 - 02 16 : 23 : 37 , 276 epoch 28 - iter 30 / 154 - loss 0.17605033 - samples / sec : 23.57 2020 - 07 - 02 16 : 23 : 57 , 641 epoch 28 - iter 45 / 154 - loss 0.16287563 - samples / sec : 23.70 2020 - 07 - 02 16 : 24 : 17 , 352 epoch 28 - iter 60 / 154 - loss 0.17845349 - samples / sec : 24.50 2020 - 07 - 02 16 : 24 : 37 , 712 epoch 28 - iter 75 / 154 - loss 0.16782649 - samples / sec : 23.86 2020 - 07 - 02 16 : 24 : 57 , 625 epoch 28 - iter 90 / 154 - loss 0.16389592 - samples / sec : 24.25 2020 - 07 - 02 16 : 25 : 16 , 025 epoch 28 - iter 105 / 154 - loss 0.15795042 - samples / sec : 26.24 2020 - 07 - 02 16 : 25 : 35 , 831 epoch 28 - iter 120 / 154 - loss 0.16067890 - samples / sec : 24.53 2020 - 07 - 02 16 : 25 : 54 , 702 epoch 28 - iter 135 / 154 - loss 0.16030627 - samples / sec : 25.58 2020 - 07 - 02 16 : 26 : 14 , 707 epoch 28 - iter 150 / 154 - loss 0.15914360 - samples / sec : 24.29 2020 - 07 - 02 16 : 26 : 18 , 949 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 26 : 18 , 950 EPOCH 28 done : loss 0.1580 - lr 0.0081091 2020 - 07 - 02 16 : 26 : 41 , 249 DEV : loss 0.2035386562347412 - score 0.9755 2020 - 07 - 02 16 : 26 : 41 , 318 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 16 : 26 : 45 , 583 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 27 : 07 , 154 epoch 29 - iter 15 / 154 - loss 0.14639383 - samples / sec : 22.52 2020 - 07 - 02 16 : 27 : 26 , 806 epoch 29 - iter 30 / 154 - loss 0.12783547 - samples / sec : 24.58 2020 - 07 - 02 16 : 27 : 48 , 187 epoch 29 - iter 45 / 154 - loss 0.13536035 - samples / sec : 22.56 2020 - 07 - 02 16 : 28 : 07 , 762 epoch 29 - iter 60 / 154 - loss 0.14223794 - samples / sec : 24.68 2020 - 07 - 02 16 : 28 : 26 , 724 epoch 29 - iter 75 / 154 - loss 0.14081989 - samples / sec : 25.65 2020 - 07 - 02 16 : 28 : 48 , 403 epoch 29 - iter 90 / 154 - loss 0.13874355 - samples / sec : 22.27 2020 - 07 - 02 16 : 29 : 09 , 293 epoch 29 - iter 105 / 154 - loss 0.14219119 - samples / sec : 23.12 2020 - 07 - 02 16 : 29 : 28 , 855 epoch 29 - iter 120 / 154 - loss 0.14479751 - samples / sec : 24.69 2020 - 07 - 02 16 : 29 : 48 , 408 epoch 29 - iter 135 / 154 - loss 0.14921473 - samples / sec : 24.70 2020 - 07 - 02 16 : 30 : 08 , 553 epoch 29 - iter 150 / 154 - loss 0.14897868 - samples / sec : 23.97 2020 - 07 - 02 16 : 30 : 12 , 911 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 30 : 12 , 913 EPOCH 29 done : loss 0.1476 - lr 0.0081091 2020 - 07 - 02 16 : 30 : 35 , 283 DEV : loss 0.21832378208637238 - score 0.9755 2020 - 07 - 02 16 : 30 : 35 , 493 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 16 : 30 : 35 , 495 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 30 : 56 , 765 epoch 30 - iter 15 / 154 - loss 0.18098603 - samples / sec : 22.84 2020 - 07 - 02 16 : 31 : 15 , 369 epoch 30 - iter 30 / 154 - loss 0.16143890 - samples / sec : 25.97 2020 - 07 - 02 16 : 31 : 35 , 237 epoch 30 - iter 45 / 154 - loss 0.14935495 - samples / sec : 24.50 2020 - 07 - 02 16 : 31 : 55 , 399 epoch 30 - iter 60 / 154 - loss 0.14397024 - samples / sec : 23.94 2020 - 07 - 02 16 : 32 : 14 , 663 epoch 30 - iter 75 / 154 - loss 0.14854784 - samples / sec : 25.07 2020 - 07 - 02 16 : 32 : 35 , 534 epoch 30 - iter 90 / 154 - loss 0.14540661 - samples / sec : 23.14 2020 - 07 - 02 16 : 32 : 55 , 656 epoch 30 - iter 105 / 154 - loss 0.14210871 - samples / sec : 24.00 2020 - 07 - 02 16 : 33 : 15 , 355 epoch 30 - iter 120 / 154 - loss 0.14353256 - samples / sec : 24.50 2020 - 07 - 02 16 : 33 : 36 , 347 epoch 30 - iter 135 / 154 - loss 0.14563753 - samples / sec : 23.14 2020 - 07 - 02 16 : 33 : 57 , 331 epoch 30 - iter 150 / 154 - loss 0.14559265 - samples / sec : 22.99 2020 - 07 - 02 16 : 34 : 01 , 855 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 34 : 01 , 856 EPOCH 30 done : loss 0.1459 - lr 0.0081091 2020 - 07 - 02 16 : 34 : 24 , 512 DEV : loss 0.24465428292751312 - score 0.9719 2020 - 07 - 02 16 : 34 : 24 , 579 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 16 : 34 : 24 , 580 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 34 : 46 , 219 epoch 31 - iter 15 / 154 - loss 0.15976830 - samples / sec : 22.46 2020 - 07 - 02 16 : 35 : 04 , 746 epoch 31 - iter 30 / 154 - loss 0.13884736 - samples / sec : 26.08 2020 - 07 - 02 16 : 35 : 25 , 418 epoch 31 - iter 45 / 154 - loss 0.13285553 - samples / sec : 23.56 2020 - 07 - 02 16 : 35 : 46 , 085 epoch 31 - iter 60 / 154 - loss 0.13884976 - samples / sec : 23.36 2020 - 07 - 02 16 : 36 : 06 , 922 epoch 31 - iter 75 / 154 - loss 0.14738866 - samples / sec : 23.17 2020 - 07 - 02 16 : 36 : 26 , 604 epoch 31 - iter 90 / 154 - loss 0.14702798 - samples / sec : 24.71 2020 - 07 - 02 16 : 36 : 46 , 280 epoch 31 - iter 105 / 154 - loss 0.14182458 - samples / sec : 24.55 2020 - 07 - 02 16 : 37 : 05 , 761 epoch 31 - iter 120 / 154 - loss 0.14055530 - samples / sec : 24.78 2020 - 07 - 02 16 : 37 : 26 , 799 epoch 31 - iter 135 / 154 - loss 0.13758830 - samples / sec : 23.09 2020 - 07 - 02 16 : 37 : 46 , 408 epoch 31 - iter 150 / 154 - loss 0.13833731 - samples / sec : 24.62 2020 - 07 - 02 16 : 37 : 50 , 524 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 37 : 50 , 525 EPOCH 31 done : loss 0.1374 - lr 0.0081091 2020 - 07 - 02 16 : 38 : 12 , 884 DEV : loss 0.20527389645576477 - score 0.9743 2020 - 07 - 02 16 : 38 : 12 , 956 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 16 : 38 : 12 , 957 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 38 : 34 , 427 epoch 32 - iter 15 / 154 - loss 0.11888420 - samples / sec : 23.19 2020 - 07 - 02 16 : 38 : 55 , 683 epoch 32 - iter 30 / 154 - loss 0.13450191 - samples / sec : 22.71 2020 - 07 - 02 16 : 39 : 14 , 237 epoch 32 - iter 45 / 154 - loss 0.13657612 - samples / sec : 26.21 2020 - 07 - 02 16 : 39 : 34 , 635 epoch 32 - iter 60 / 154 - loss 0.13090624 - samples / sec : 23.67 2020 - 07 - 02 16 : 39 : 53 , 688 epoch 32 - iter 75 / 154 - loss 0.13154434 - samples / sec : 25.34 2020 - 07 - 02 16 : 40 : 13 , 250 epoch 32 - iter 90 / 154 - loss 0.13931910 - samples / sec : 24.69 2020 - 07 - 02 16 : 40 : 35 , 482 epoch 32 - iter 105 / 154 - loss 0.14349992 - samples / sec : 21.70 2020 - 07 - 02 16 : 40 : 55 , 671 epoch 32 - iter 120 / 154 - loss 0.14045170 - samples / sec : 24.08 2020 - 07 - 02 16 : 41 : 14 , 900 epoch 32 - iter 135 / 154 - loss 0.14271810 - samples / sec : 25.12 2020 - 07 - 02 16 : 41 : 33 , 122 epoch 32 - iter 150 / 154 - loss 0.14134367 - samples / sec : 26.51 2020 - 07 - 02 16 : 41 : 37 , 990 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 41 : 37 , 992 EPOCH 32 done : loss 0.1423 - lr 0.0081091 2020 - 07 - 02 16 : 42 : 00 , 731 DEV : loss 0.21198561787605286 - score 0.9755 2020 - 07 - 02 16 : 42 : 00 , 803 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 16 : 42 : 00 , 805 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 42 : 23 , 967 epoch 33 - iter 15 / 154 - loss 0.17858952 - samples / sec : 20.97 2020 - 07 - 02 16 : 42 : 44 , 312 epoch 33 - iter 30 / 154 - loss 0.14277964 - samples / sec : 23.75 2020 - 07 - 02 16 : 43 : 04 , 676 epoch 33 - iter 45 / 154 - loss 0.14020735 - samples / sec : 23.93 2020 - 07 - 02 16 : 43 : 25 , 173 epoch 33 - iter 60 / 154 - loss 0.13840914 - samples / sec : 23.56 2020 - 07 - 02 16 : 43 : 44 , 300 epoch 33 - iter 75 / 154 - loss 0.13643146 - samples / sec : 25.25 2020 - 07 - 02 16 : 44 : 04 , 640 epoch 33 - iter 90 / 154 - loss 0.13610004 - samples / sec : 23.90 2020 - 07 - 02 16 : 44 : 24 , 217 epoch 33 - iter 105 / 154 - loss 0.13951315 - samples / sec : 24.65 2020 - 07 - 02 16 : 44 : 44 , 125 epoch 33 - iter 120 / 154 - loss 0.13773997 - samples / sec : 24.25 2020 - 07 - 02 16 : 45 : 02 , 697 epoch 33 - iter 135 / 154 - loss 0.13444212 - samples / sec : 26.22 2020 - 07 - 02 16 : 45 : 22 , 033 epoch 33 - iter 150 / 154 - loss 0.13648703 - samples / sec : 24.97 2020 - 07 - 02 16 : 45 : 26 , 452 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 45 : 26 , 454 EPOCH 33 done : loss 0.1352 - lr 0.0081091 2020 - 07 - 02 16 : 45 : 49 , 301 DEV : loss 0.2025325894355774 - score 0.978 2020 - 07 - 02 16 : 45 : 49 , 370 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 16 : 45 : 53 , 635 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 46 : 16 , 117 epoch 34 - iter 15 / 154 - loss 0.13611445 - samples / sec : 21.61 2020 - 07 - 02 16 : 46 : 36 , 415 epoch 34 - iter 30 / 154 - loss 0.14471161 - samples / sec : 23.79 2020 - 07 - 02 16 : 46 : 57 , 806 epoch 34 - iter 45 / 154 - loss 0.13999367 - samples / sec : 22.56 2020 - 07 - 02 16 : 47 : 17 , 926 epoch 34 - iter 60 / 154 - loss 0.13826550 - samples / sec : 24.01 2020 - 07 - 02 16 : 47 : 35 , 555 epoch 34 - iter 75 / 154 - loss 0.13903977 - samples / sec : 27.41 2020 - 07 - 02 16 : 47 : 54 , 789 epoch 34 - iter 90 / 154 - loss 0.13380337 - samples / sec : 25.11 2020 - 07 - 02 16 : 48 : 14 , 552 epoch 34 - iter 105 / 154 - loss 0.13171350 - samples / sec : 24.42 2020 - 07 - 02 16 : 48 : 34 , 064 epoch 34 - iter 120 / 154 - loss 0.13369191 - samples / sec : 24.75 2020 - 07 - 02 16 : 48 : 54 , 128 epoch 34 - iter 135 / 154 - loss 0.13411653 - samples / sec : 24.23 2020 - 07 - 02 16 : 49 : 14 , 657 epoch 34 - iter 150 / 154 - loss 0.13264349 - samples / sec : 23.51 2020 - 07 - 02 16 : 49 : 19 , 762 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 49 : 19 , 764 EPOCH 34 done : loss 0.1323 - lr 0.0081091 2020 - 07 - 02 16 : 49 : 42 , 706 DEV : loss 0.21952836215496063 - score 0.9761 2020 - 07 - 02 16 : 49 : 42 , 773 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 16 : 49 : 42 , 775 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 50 : 04 , 741 epoch 35 - iter 15 / 154 - loss 0.14532092 - samples / sec : 22.13 2020 - 07 - 02 16 : 50 : 24 , 638 epoch 35 - iter 30 / 154 - loss 0.14132146 - samples / sec : 24.27 2020 - 07 - 02 16 : 50 : 44 , 619 epoch 35 - iter 45 / 154 - loss 0.13029910 - samples / sec : 24.36 2020 - 07 - 02 16 : 51 : 04 , 282 epoch 35 - iter 60 / 154 - loss 0.13496286 - samples / sec : 24.56 2020 - 07 - 02 16 : 51 : 25 , 266 epoch 35 - iter 75 / 154 - loss 0.12704580 - samples / sec : 22.99 2020 - 07 - 02 16 : 51 : 45 , 668 epoch 35 - iter 90 / 154 - loss 0.13263493 - samples / sec : 23.66 2020 - 07 - 02 16 : 52 : 04 , 516 epoch 35 - iter 105 / 154 - loss 0.13433218 - samples / sec : 25.62 2020 - 07 - 02 16 : 52 : 25 , 365 epoch 35 - iter 120 / 154 - loss 0.12975193 - samples / sec : 23.16 2020 - 07 - 02 16 : 52 : 43 , 700 epoch 35 - iter 135 / 154 - loss 0.12926159 - samples / sec : 26.36 2020 - 07 - 02 16 : 53 : 04 , 549 epoch 35 - iter 150 / 154 - loss 0.13078938 - samples / sec : 23.15 2020 - 07 - 02 16 : 53 : 09 , 340 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 53 : 09 , 341 EPOCH 35 done : loss 0.1326 - lr 0.0081091 2020 - 07 - 02 16 : 53 : 31 , 992 DEV : loss 0.21245123445987701 - score 0.9749 2020 - 07 - 02 16 : 53 : 32 , 061 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 16 : 53 : 32 , 063 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 53 : 55 , 029 epoch 36 - iter 15 / 154 - loss 0.11573464 - samples / sec : 21.14 2020 - 07 - 02 16 : 54 : 14 , 451 epoch 36 - iter 30 / 154 - loss 0.13536664 - samples / sec : 25.16 2020 - 07 - 02 16 : 54 : 34 , 936 epoch 36 - iter 45 / 154 - loss 0.14638091 - samples / sec : 23.55 2020 - 07 - 02 16 : 54 : 54 , 032 epoch 36 - iter 60 / 154 - loss 0.14207099 - samples / sec : 25.29 2020 - 07 - 02 16 : 55 : 13 , 615 epoch 36 - iter 75 / 154 - loss 0.14031379 - samples / sec : 24.65 2020 - 07 - 02 16 : 55 : 32 , 985 epoch 36 - iter 90 / 154 - loss 0.13803298 - samples / sec : 24.92 2020 - 07 - 02 16 : 55 : 53 , 148 epoch 36 - iter 105 / 154 - loss 0.14540687 - samples / sec : 23.96 2020 - 07 - 02 16 : 56 : 12 , 803 epoch 36 - iter 120 / 154 - loss 0.14509310 - samples / sec : 24.57 2020 - 07 - 02 16 : 56 : 34 , 448 epoch 36 - iter 135 / 154 - loss 0.14601868 - samples / sec : 22.44 2020 - 07 - 02 16 : 56 : 53 , 871 epoch 36 - iter 150 / 154 - loss 0.13876525 - samples / sec : 24.84 2020 - 07 - 02 16 : 56 : 58 , 371 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 56 : 58 , 372 EPOCH 36 done : loss 0.1369 - lr 0.0081091 2020 - 07 - 02 16 : 57 : 21 , 055 DEV : loss 0.2004116028547287 - score 0.9774 2020 - 07 - 02 16 : 57 : 21 , 129 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 16 : 57 : 21 , 132 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 16 : 57 : 43 , 702 epoch 37 - iter 15 / 154 - loss 0.15859565 - samples / sec : 21.53 2020 - 07 - 02 16 : 58 : 03 , 547 epoch 37 - iter 30 / 154 - loss 0.13821206 - samples / sec : 24.33 2020 - 07 - 02 16 : 58 : 23 , 015 epoch 37 - iter 45 / 154 - loss 0.14161196 - samples / sec : 25.03 2020 - 07 - 02 16 : 58 : 42 , 188 epoch 37 - iter 60 / 154 - loss 0.14231336 - samples / sec : 25.19 2020 - 07 - 02 16 : 59 : 01 , 640 epoch 37 - iter 75 / 154 - loss 0.14150818 - samples / sec : 24.82 2020 - 07 - 02 16 : 59 : 22 , 858 epoch 37 - iter 90 / 154 - loss 0.13713271 - samples / sec : 22.75 2020 - 07 - 02 16 : 59 : 44 , 526 epoch 37 - iter 105 / 154 - loss 0.13910145 - samples / sec : 22.28 2020 - 07 - 02 17 : 00 : 03 , 844 epoch 37 - iter 120 / 154 - loss 0.13740354 - samples / sec : 25.18 2020 - 07 - 02 17 : 00 : 23 , 183 epoch 37 - iter 135 / 154 - loss 0.13456342 - samples / sec : 24.97 2020 - 07 - 02 17 : 00 : 42 , 278 epoch 37 - iter 150 / 154 - loss 0.13265983 - samples / sec : 25.29 2020 - 07 - 02 17 : 00 : 46 , 742 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 00 : 46 , 743 EPOCH 37 done : loss 0.1338 - lr 0.0081091 2020 - 07 - 02 17 : 01 : 09 , 404 DEV : loss 0.22566574811935425 - score 0.978 2020 - 07 - 02 17 : 01 : 09 , 481 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 17 : 01 : 09 , 482 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 01 : 31 , 189 epoch 38 - iter 15 / 154 - loss 0.11563111 - samples / sec : 22.40 2020 - 07 - 02 17 : 01 : 53 , 327 epoch 38 - iter 30 / 154 - loss 0.12145271 - samples / sec : 21.80 2020 - 07 - 02 17 : 02 : 13 , 138 epoch 38 - iter 45 / 154 - loss 0.13173554 - samples / sec : 24.61 2020 - 07 - 02 17 : 02 : 33 , 470 epoch 38 - iter 60 / 154 - loss 0.13281630 - samples / sec : 23.74 2020 - 07 - 02 17 : 02 : 51 , 911 epoch 38 - iter 75 / 154 - loss 0.13535262 - samples / sec : 26.19 2020 - 07 - 02 17 : 03 : 11 , 871 epoch 38 - iter 90 / 154 - loss 0.13950271 - samples / sec : 24.35 2020 - 07 - 02 17 : 03 : 30 , 440 epoch 38 - iter 105 / 154 - loss 0.13672152 - samples / sec : 26.02 2020 - 07 - 02 17 : 03 : 51 , 450 epoch 38 - iter 120 / 154 - loss 0.13120697 - samples / sec : 22.97 2020 - 07 - 02 17 : 04 : 11 , 038 epoch 38 - iter 135 / 154 - loss 0.13313321 - samples / sec : 24.81 2020 - 07 - 02 17 : 04 : 32 , 278 epoch 38 - iter 150 / 154 - loss 0.13108277 - samples / sec : 22.72 2020 - 07 - 02 17 : 04 : 36 , 919 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 04 : 36 , 920 EPOCH 38 done : loss 0.1316 - lr 0.0081091 2020 - 07 - 02 17 : 04 : 59 , 607 DEV : loss 0.19655393064022064 - score 0.978 2020 - 07 - 02 17 : 04 : 59 , 675 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 17 : 05 : 03 , 953 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 05 : 27 , 355 epoch 39 - iter 15 / 154 - loss 0.08606811 - samples / sec : 20.77 2020 - 07 - 02 17 : 05 : 47 , 467 epoch 39 - iter 30 / 154 - loss 0.09626995 - samples / sec : 24.00 2020 - 07 - 02 17 : 06 : 08 , 121 epoch 39 - iter 45 / 154 - loss 0.09990381 - samples / sec : 23.60 2020 - 07 - 02 17 : 06 : 28 , 420 epoch 39 - iter 60 / 154 - loss 0.10682175 - samples / sec : 23.77 2020 - 07 - 02 17 : 06 : 49 , 090 epoch 39 - iter 75 / 154 - loss 0.10827231 - samples / sec : 23.50 2020 - 07 - 02 17 : 07 : 07 , 984 epoch 39 - iter 90 / 154 - loss 0.11429365 - samples / sec : 25.57 2020 - 07 - 02 17 : 07 : 27 , 829 epoch 39 - iter 105 / 154 - loss 0.11071864 - samples / sec : 24.34 2020 - 07 - 02 17 : 07 : 47 , 350 epoch 39 - iter 120 / 154 - loss 0.11326766 - samples / sec : 24.92 2020 - 07 - 02 17 : 08 : 06 , 989 epoch 39 - iter 135 / 154 - loss 0.11420242 - samples / sec : 24.60 2020 - 07 - 02 17 : 08 : 24 , 776 epoch 39 - iter 150 / 154 - loss 0.11605036 - samples / sec : 27.14 2020 - 07 - 02 17 : 08 : 30 , 076 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 08 : 30 , 077 EPOCH 39 done : loss 0.1189 - lr 0.0081091 2020 - 07 - 02 17 : 08 : 52 , 666 DEV : loss 0.23231680691242218 - score 0.978 2020 - 07 - 02 17 : 08 : 52 , 733 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 17 : 08 : 52 , 734 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 09 : 14 , 407 epoch 40 - iter 15 / 154 - loss 0.10929688 - samples / sec : 22.45 2020 - 07 - 02 17 : 09 : 35 , 102 epoch 40 - iter 30 / 154 - loss 0.12555539 - samples / sec : 23.57 2020 - 07 - 02 17 : 09 : 54 , 695 epoch 40 - iter 45 / 154 - loss 0.13723706 - samples / sec : 24.64 2020 - 07 - 02 17 : 10 : 15 , 206 epoch 40 - iter 60 / 154 - loss 0.13665709 - samples / sec : 23.55 2020 - 07 - 02 17 : 10 : 34 , 944 epoch 40 - iter 75 / 154 - loss 0.13865776 - samples / sec : 24.62 2020 - 07 - 02 17 : 10 : 54 , 804 epoch 40 - iter 90 / 154 - loss 0.13386259 - samples / sec : 24.31 2020 - 07 - 02 17 : 11 : 14 , 413 epoch 40 - iter 105 / 154 - loss 0.12282358 - samples / sec : 24.61 2020 - 07 - 02 17 : 11 : 34 , 419 epoch 40 - iter 120 / 154 - loss 0.12237809 - samples / sec : 24.30 2020 - 07 - 02 17 : 11 : 53 , 691 epoch 40 - iter 135 / 154 - loss 0.12271550 - samples / sec : 25.05 2020 - 07 - 02 17 : 12 : 14 , 398 epoch 40 - iter 150 / 154 - loss 0.12171200 - samples / sec : 23.32 2020 - 07 - 02 17 : 12 : 18 , 591 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 12 : 18 , 592 EPOCH 40 done : loss 0.1226 - lr 0.0081091 2020 - 07 - 02 17 : 12 : 41 , 415 DEV : loss 0.19786295294761658 - score 0.9786 2020 - 07 - 02 17 : 12 : 41 , 486 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 17 : 12 : 45 , 767 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 13 : 08 , 933 epoch 41 - iter 15 / 154 - loss 0.10291993 - samples / sec : 20.97 2020 - 07 - 02 17 : 13 : 28 , 070 epoch 41 - iter 30 / 154 - loss 0.11291885 - samples / sec : 25.51 2020 - 07 - 02 17 : 13 : 49 , 148 epoch 41 - iter 45 / 154 - loss 0.12107014 - samples / sec : 22.90 2020 - 07 - 02 17 : 14 : 07 , 473 epoch 41 - iter 60 / 154 - loss 0.12080107 - samples / sec : 26.35 2020 - 07 - 02 17 : 14 : 26 , 414 epoch 41 - iter 75 / 154 - loss 0.12976694 - samples / sec : 25.67 2020 - 07 - 02 17 : 14 : 46 , 749 epoch 41 - iter 90 / 154 - loss 0.12766570 - samples / sec : 23.75 2020 - 07 - 02 17 : 15 : 07 , 591 epoch 41 - iter 105 / 154 - loss 0.12382418 - samples / sec : 23.16 2020 - 07 - 02 17 : 15 : 27 , 330 epoch 41 - iter 120 / 154 - loss 0.12289315 - samples / sec : 24.47 2020 - 07 - 02 17 : 15 : 46 , 626 epoch 41 - iter 135 / 154 - loss 0.12151943 - samples / sec : 25.03 2020 - 07 - 02 17 : 16 : 07 , 237 epoch 41 - iter 150 / 154 - loss 0.11910739 - samples / sec : 23.41 2020 - 07 - 02 17 : 16 : 11 , 979 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 16 : 11 , 980 EPOCH 41 done : loss 0.1180 - lr 0.0081091 2020 - 07 - 02 17 : 16 : 34 , 749 DEV : loss 0.20476281642913818 - score 0.9786 2020 - 07 - 02 17 : 16 : 34 , 818 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 17 : 16 : 34 , 819 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 16 : 58 , 607 epoch 42 - iter 15 / 154 - loss 0.11515100 - samples / sec : 20.43 2020 - 07 - 02 17 : 17 : 19 , 701 epoch 42 - iter 30 / 154 - loss 0.11943454 - samples / sec : 22.89 2020 - 07 - 02 17 : 17 : 39 , 395 epoch 42 - iter 45 / 154 - loss 0.11384823 - samples / sec : 24.79 2020 - 07 - 02 17 : 17 : 58 , 643 epoch 42 - iter 60 / 154 - loss 0.11443645 - samples / sec : 25.09 2020 - 07 - 02 17 : 18 : 18 , 648 epoch 42 - iter 75 / 154 - loss 0.11680269 - samples / sec : 24.13 2020 - 07 - 02 17 : 18 : 38 , 267 epoch 42 - iter 90 / 154 - loss 0.12064875 - samples / sec : 24.82 2020 - 07 - 02 17 : 18 : 58 , 371 epoch 42 - iter 105 / 154 - loss 0.12379217 - samples / sec : 24.01 2020 - 07 - 02 17 : 19 : 19 , 252 epoch 42 - iter 120 / 154 - loss 0.12142454 - samples / sec : 23.12 2020 - 07 - 02 17 : 19 : 37 , 911 epoch 42 - iter 135 / 154 - loss 0.12138695 - samples / sec : 25.90 2020 - 07 - 02 17 : 19 : 57 , 683 epoch 42 - iter 150 / 154 - loss 0.12124144 - samples / sec : 24.42 2020 - 07 - 02 17 : 20 : 02 , 818 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 20 : 02 , 819 EPOCH 42 done : loss 0.1232 - lr 0.0081091 2020 - 07 - 02 17 : 20 : 25 , 643 DEV : loss 0.228939026594162 - score 0.9749 2020 - 07 - 02 17 : 20 : 25 , 711 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 17 : 20 : 25 , 712 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 20 : 50 , 520 epoch 43 - iter 15 / 154 - loss 0.11688481 - samples / sec : 19.56 2020 - 07 - 02 17 : 21 : 09 , 983 epoch 43 - iter 30 / 154 - loss 0.11299946 - samples / sec : 24.82 2020 - 07 - 02 17 : 21 : 31 , 073 epoch 43 - iter 45 / 154 - loss 0.10471270 - samples / sec : 23.10 2020 - 07 - 02 17 : 21 : 50 , 293 epoch 43 - iter 60 / 154 - loss 0.11333607 - samples / sec : 25.13 2020 - 07 - 02 17 : 22 : 09 , 299 epoch 43 - iter 75 / 154 - loss 0.10945214 - samples / sec : 25.40 2020 - 07 - 02 17 : 22 : 29 , 846 epoch 43 - iter 90 / 154 - loss 0.10823468 - samples / sec : 23.49 2020 - 07 - 02 17 : 22 : 48 , 260 epoch 43 - iter 105 / 154 - loss 0.11234212 - samples / sec : 26.24 2020 - 07 - 02 17 : 23 : 08 , 351 epoch 43 - iter 120 / 154 - loss 0.11211128 - samples / sec : 24.03 2020 - 07 - 02 17 : 23 : 28 , 918 epoch 43 - iter 135 / 154 - loss 0.11339033 - samples / sec : 23.64 2020 - 07 - 02 17 : 23 : 48 , 849 epoch 43 - iter 150 / 154 - loss 0.11390336 - samples / sec : 24.22 2020 - 07 - 02 17 : 23 : 53 , 484 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 23 : 53 , 486 EPOCH 43 done : loss 0.1160 - lr 0.0081091 2020 - 07 - 02 17 : 24 : 16 , 355 DEV : loss 0.20396070182323456 - score 0.978 2020 - 07 - 02 17 : 24 : 16 , 423 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 17 : 24 : 16 , 424 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 24 : 39 , 845 epoch 44 - iter 15 / 154 - loss 0.07580647 - samples / sec : 20.76 2020 - 07 - 02 17 : 24 : 58 , 766 epoch 44 - iter 30 / 154 - loss 0.08805038 - samples / sec : 25.54 2020 - 07 - 02 17 : 25 : 19 , 081 epoch 44 - iter 45 / 154 - loss 0.08704735 - samples / sec : 23.96 2020 - 07 - 02 17 : 25 : 38 , 923 epoch 44 - iter 60 / 154 - loss 0.10597141 - samples / sec : 24.35 2020 - 07 - 02 17 : 25 : 57 , 992 epoch 44 - iter 75 / 154 - loss 0.10900353 - samples / sec : 25.31 2020 - 07 - 02 17 : 26 : 18 , 083 epoch 44 - iter 90 / 154 - loss 0.11724408 - samples / sec : 24.22 2020 - 07 - 02 17 : 26 : 36 , 721 epoch 44 - iter 105 / 154 - loss 0.11579082 - samples / sec : 25.90 2020 - 07 - 02 17 : 26 : 57 , 300 epoch 44 - iter 120 / 154 - loss 0.11354713 - samples / sec : 23.63 2020 - 07 - 02 17 : 27 : 17 , 545 epoch 44 - iter 135 / 154 - loss 0.11238661 - samples / sec : 23.84 2020 - 07 - 02 17 : 27 : 37 , 533 epoch 44 - iter 150 / 154 - loss 0.11010026 - samples / sec : 24.15 2020 - 07 - 02 17 : 27 : 41 , 777 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 27 : 41 , 778 EPOCH 44 done : loss 0.1084 - lr 0.0081091 2020 - 07 - 02 17 : 28 : 04 , 480 DEV : loss 0.2126724123954773 - score 0.9786 2020 - 07 - 02 17 : 28 : 04 , 547 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 17 : 28 : 04 , 549 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 28 : 26 , 022 epoch 45 - iter 15 / 154 - loss 0.10441768 - samples / sec : 22.64 2020 - 07 - 02 17 : 28 : 46 , 731 epoch 45 - iter 30 / 154 - loss 0.11024101 - samples / sec : 23.31 2020 - 07 - 02 17 : 29 : 07 , 281 epoch 45 - iter 45 / 154 - loss 0.11836660 - samples / sec : 23.73 2020 - 07 - 02 17 : 29 : 26 , 386 epoch 45 - iter 60 / 154 - loss 0.11498991 - samples / sec : 25.27 2020 - 07 - 02 17 : 29 : 46 , 118 epoch 45 - iter 75 / 154 - loss 0.11378261 - samples / sec : 24.47 2020 - 07 - 02 17 : 30 : 05 , 930 epoch 45 - iter 90 / 154 - loss 0.11051767 - samples / sec : 24.54 2020 - 07 - 02 17 : 30 : 24 , 450 epoch 45 - iter 105 / 154 - loss 0.11882560 - samples / sec : 26.09 2020 - 07 - 02 17 : 30 : 43 , 922 epoch 45 - iter 120 / 154 - loss 0.11922019 - samples / sec : 24.79 2020 - 07 - 02 17 : 31 : 05 , 345 epoch 45 - iter 135 / 154 - loss 0.11827121 - samples / sec : 22.69 2020 - 07 - 02 17 : 31 : 24 , 900 epoch 45 - iter 150 / 154 - loss 0.11522523 - samples / sec : 24.68 2020 - 07 - 02 17 : 31 : 29 , 527 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 31 : 29 , 529 EPOCH 45 done : loss 0.1151 - lr 0.0081091 2020 - 07 - 02 17 : 31 : 52 , 285 DEV : loss 0.21013282239437103 - score 0.9774 2020 - 07 - 02 17 : 31 : 52 , 353 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 17 : 31 : 52 , 354 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 32 : 15 , 570 epoch 46 - iter 15 / 154 - loss 0.11027370 - samples / sec : 20.92 2020 - 07 - 02 17 : 32 : 35 , 288 epoch 46 - iter 30 / 154 - loss 0.10980776 - samples / sec : 24.50 2020 - 07 - 02 17 : 32 : 55 , 630 epoch 46 - iter 45 / 154 - loss 0.11028625 - samples / sec : 23.73 2020 - 07 - 02 17 : 33 : 16 , 437 epoch 46 - iter 60 / 154 - loss 0.11294564 - samples / sec : 23.20 2020 - 07 - 02 17 : 33 : 36 , 764 epoch 46 - iter 75 / 154 - loss 0.11088956 - samples / sec : 23.90 2020 - 07 - 02 17 : 33 : 56 , 137 epoch 46 - iter 90 / 154 - loss 0.11293679 - samples / sec : 24.93 2020 - 07 - 02 17 : 34 : 16 , 271 epoch 46 - iter 105 / 154 - loss 0.11380949 - samples / sec : 23.98 2020 - 07 - 02 17 : 34 : 36 , 209 epoch 46 - iter 120 / 154 - loss 0.11566317 - samples / sec : 24.22 2020 - 07 - 02 17 : 34 : 55 , 594 epoch 46 - iter 135 / 154 - loss 0.11391440 - samples / sec : 24.91 2020 - 07 - 02 17 : 35 : 14 , 821 epoch 46 - iter 150 / 154 - loss 0.11119683 - samples / sec : 25.29 2020 - 07 - 02 17 : 35 : 19 , 264 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 35 : 19 , 265 EPOCH 46 done : loss 0.1132 - lr 0.0081091 2020 - 07 - 02 17 : 35 : 41 , 738 DEV : loss 0.2473410964012146 - score 0.9768 Epoch 46 : reducing learning rate of group 0 to 4.0545e-03 . 2020 - 07 - 02 17 : 35 : 41 , 808 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 17 : 35 : 41 , 811 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 36 : 04 , 483 epoch 47 - iter 15 / 154 - loss 0.13017201 - samples / sec : 21.44 2020 - 07 - 02 17 : 36 : 25 , 431 epoch 47 - iter 30 / 154 - loss 0.11781174 - samples / sec : 23.27 2020 - 07 - 02 17 : 36 : 44 , 855 epoch 47 - iter 45 / 154 - loss 0.10631655 - samples / sec : 24.86 2020 - 07 - 02 17 : 37 : 03 , 628 epoch 47 - iter 60 / 154 - loss 0.09923212 - samples / sec : 25.72 2020 - 07 - 02 17 : 37 : 24 , 523 epoch 47 - iter 75 / 154 - loss 0.09882059 - samples / sec : 23.12 2020 - 07 - 02 17 : 37 : 44 , 449 epoch 47 - iter 90 / 154 - loss 0.10180322 - samples / sec : 24.21 2020 - 07 - 02 17 : 38 : 04 , 677 epoch 47 - iter 105 / 154 - loss 0.10014918 - samples / sec : 23.86 2020 - 07 - 02 17 : 38 : 24 , 080 epoch 47 - iter 120 / 154 - loss 0.10079072 - samples / sec : 25.07 2020 - 07 - 02 17 : 38 : 43 , 268 epoch 47 - iter 135 / 154 - loss 0.10352145 - samples / sec : 25.17 2020 - 07 - 02 17 : 39 : 03 , 680 epoch 47 - iter 150 / 154 - loss 0.10257754 - samples / sec : 23.81 2020 - 07 - 02 17 : 39 : 08 , 929 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 39 : 08 , 930 EPOCH 47 done : loss 0.1019 - lr 0.0040545 2020 - 07 - 02 17 : 39 : 31 , 341 DEV : loss 0.1972641497850418 - score 0.9798 2020 - 07 - 02 17 : 39 : 31 , 410 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 17 : 39 : 35 , 701 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 39 : 56 , 986 epoch 48 - iter 15 / 154 - loss 0.09316256 - samples / sec : 22.84 2020 - 07 - 02 17 : 40 : 18 , 519 epoch 48 - iter 30 / 154 - loss 0.09207635 - samples / sec : 22.64 2020 - 07 - 02 17 : 40 : 38 , 501 epoch 48 - iter 45 / 154 - loss 0.09080028 - samples / sec : 24.16 2020 - 07 - 02 17 : 40 : 59 , 896 epoch 48 - iter 60 / 154 - loss 0.09034919 - samples / sec : 22.56 2020 - 07 - 02 17 : 41 : 21 , 390 epoch 48 - iter 75 / 154 - loss 0.09863393 - samples / sec : 22.45 2020 - 07 - 02 17 : 41 : 40 , 885 epoch 48 - iter 90 / 154 - loss 0.09842956 - samples / sec : 24.96 2020 - 07 - 02 17 : 41 : 59 , 314 epoch 48 - iter 105 / 154 - loss 0.09847930 - samples / sec : 26.23 2020 - 07 - 02 17 : 42 : 18 , 382 epoch 48 - iter 120 / 154 - loss 0.10359679 - samples / sec : 25.32 2020 - 07 - 02 17 : 42 : 36 , 737 epoch 48 - iter 135 / 154 - loss 0.10533498 - samples / sec : 26.51 2020 - 07 - 02 17 : 42 : 56 , 401 epoch 48 - iter 150 / 154 - loss 0.10285178 - samples / sec : 24.54 2020 - 07 - 02 17 : 43 : 01 , 894 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 43 : 01 , 896 EPOCH 48 done : loss 0.1029 - lr 0.0040545 2020 - 07 - 02 17 : 43 : 24 , 565 DEV : loss 0.20908032357692719 - score 0.9774 2020 - 07 - 02 17 : 43 : 24 , 634 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 17 : 43 : 24 , 636 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 43 : 46 , 260 epoch 49 - iter 15 / 154 - loss 0.12452545 - samples / sec : 22.74 2020 - 07 - 02 17 : 44 : 06 , 872 epoch 49 - iter 30 / 154 - loss 0.11373051 - samples / sec : 23.41 2020 - 07 - 02 17 : 44 : 25 , 356 epoch 49 - iter 45 / 154 - loss 0.11127935 - samples / sec : 26.39 2020 - 07 - 02 17 : 44 : 45 , 722 epoch 49 - iter 60 / 154 - loss 0.10167709 - samples / sec : 23.70 2020 - 07 - 02 17 : 45 : 04 , 808 epoch 49 - iter 75 / 154 - loss 0.10067866 - samples / sec : 25.31 2020 - 07 - 02 17 : 45 : 25 , 368 epoch 49 - iter 90 / 154 - loss 0.10211510 - samples / sec : 23.65 2020 - 07 - 02 17 : 45 : 45 , 284 epoch 49 - iter 105 / 154 - loss 0.09828637 - samples / sec : 24.24 2020 - 07 - 02 17 : 46 : 07 , 360 epoch 49 - iter 120 / 154 - loss 0.10255460 - samples / sec : 21.85 2020 - 07 - 02 17 : 46 : 28 , 038 epoch 49 - iter 135 / 154 - loss 0.10150222 - samples / sec : 23.51 2020 - 07 - 02 17 : 46 : 47 , 340 epoch 49 - iter 150 / 154 - loss 0.10494432 - samples / sec : 25.01 2020 - 07 - 02 17 : 46 : 51 , 762 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 46 : 51 , 764 EPOCH 49 done : loss 0.1036 - lr 0.0040545 2020 - 07 - 02 17 : 47 : 14 , 698 DEV : loss 0.21484945714473724 - score 0.978 2020 - 07 - 02 17 : 47 : 14 , 767 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 17 : 47 : 14 , 768 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 47 : 37 , 461 epoch 50 - iter 15 / 154 - loss 0.07441427 - samples / sec : 21.41 2020 - 07 - 02 17 : 47 : 57 , 684 epoch 50 - iter 30 / 154 - loss 0.09981060 - samples / sec : 23.88 2020 - 07 - 02 17 : 48 : 17 , 556 epoch 50 - iter 45 / 154 - loss 0.09582326 - samples / sec : 24.54 2020 - 07 - 02 17 : 48 : 37 , 096 epoch 50 - iter 60 / 154 - loss 0.09657856 - samples / sec : 24.71 2020 - 07 - 02 17 : 48 : 57 , 920 epoch 50 - iter 75 / 154 - loss 0.09897010 - samples / sec : 23.19 2020 - 07 - 02 17 : 49 : 17 , 314 epoch 50 - iter 90 / 154 - loss 0.09652073 - samples / sec : 25.10 2020 - 07 - 02 17 : 49 : 36 , 071 epoch 50 - iter 105 / 154 - loss 0.09323545 - samples / sec : 25.75 2020 - 07 - 02 17 : 49 : 56 , 835 epoch 50 - iter 120 / 154 - loss 0.09363949 - samples / sec : 23.24 2020 - 07 - 02 17 : 50 : 16 , 600 epoch 50 - iter 135 / 154 - loss 0.09351738 - samples / sec : 24.60 2020 - 07 - 02 17 : 50 : 37 , 072 epoch 50 - iter 150 / 154 - loss 0.09631807 - samples / sec : 23.58 2020 - 07 - 02 17 : 50 : 41 , 535 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 50 : 41 , 536 EPOCH 50 done : loss 0.0959 - lr 0.0040545 2020 - 07 - 02 17 : 51 : 04 , 378 DEV : loss 0.22444875538349152 - score 0.9792 2020 - 07 - 02 17 : 51 : 04 , 446 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 17 : 51 : 04 , 448 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 51 : 26 , 503 epoch 51 - iter 15 / 154 - loss 0.08688969 - samples / sec : 22.04 2020 - 07 - 02 17 : 51 : 45 , 925 epoch 51 - iter 30 / 154 - loss 0.09203181 - samples / sec : 24.88 2020 - 07 - 02 17 : 52 : 06 , 814 epoch 51 - iter 45 / 154 - loss 0.10646763 - samples / sec : 23.31 2020 - 07 - 02 17 : 52 : 26 , 057 epoch 51 - iter 60 / 154 - loss 0.11192265 - samples / sec : 25.09 2020 - 07 - 02 17 : 52 : 45 , 627 epoch 51 - iter 75 / 154 - loss 0.11106363 - samples / sec : 24.69 2020 - 07 - 02 17 : 53 : 06 , 167 epoch 51 - iter 90 / 154 - loss 0.11052981 - samples / sec : 23.63 2020 - 07 - 02 17 : 53 : 26 , 731 epoch 51 - iter 105 / 154 - loss 0.10707935 - samples / sec : 23.47 2020 - 07 - 02 17 : 53 : 46 , 864 epoch 51 - iter 120 / 154 - loss 0.10218134 - samples / sec : 23.97 2020 - 07 - 02 17 : 54 : 05 , 931 epoch 51 - iter 135 / 154 - loss 0.10484334 - samples / sec : 25.52 2020 - 07 - 02 17 : 54 : 25 , 902 epoch 51 - iter 150 / 154 - loss 0.10450326 - samples / sec : 24.18 2020 - 07 - 02 17 : 54 : 30 , 340 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 54 : 30 , 341 EPOCH 51 done : loss 0.1029 - lr 0.0040545 2020 - 07 - 02 17 : 54 : 53 , 014 DEV : loss 0.2026711255311966 - score 0.9792 2020 - 07 - 02 17 : 54 : 53 , 084 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 17 : 54 : 53 , 085 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 55 : 16 , 198 epoch 52 - iter 15 / 154 - loss 0.09530740 - samples / sec : 21.03 2020 - 07 - 02 17 : 55 : 38 , 136 epoch 52 - iter 30 / 154 - loss 0.10926712 - samples / sec : 22.01 2020 - 07 - 02 17 : 55 : 59 , 339 epoch 52 - iter 45 / 154 - loss 0.10953697 - samples / sec : 22.97 2020 - 07 - 02 17 : 56 : 18 , 647 epoch 52 - iter 60 / 154 - loss 0.11040190 - samples / sec : 25.01 2020 - 07 - 02 17 : 56 : 36 , 716 epoch 52 - iter 75 / 154 - loss 0.10400084 - samples / sec : 26.74 2020 - 07 - 02 17 : 56 : 56 , 401 epoch 52 - iter 90 / 154 - loss 0.10083394 - samples / sec : 24.69 2020 - 07 - 02 17 : 57 : 15 , 759 epoch 52 - iter 105 / 154 - loss 0.10321290 - samples / sec : 24.94 2020 - 07 - 02 17 : 57 : 35 , 739 epoch 52 - iter 120 / 154 - loss 0.09946122 - samples / sec : 24.33 2020 - 07 - 02 17 : 57 : 54 , 736 epoch 52 - iter 135 / 154 - loss 0.09763378 - samples / sec : 25.41 2020 - 07 - 02 17 : 58 : 13 , 939 epoch 52 - iter 150 / 154 - loss 0.09765992 - samples / sec : 25.15 2020 - 07 - 02 17 : 58 : 18 , 133 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 58 : 18 , 135 EPOCH 52 done : loss 0.0961 - lr 0.0040545 2020 - 07 - 02 17 : 58 : 40 , 895 DEV : loss 0.1978662759065628 - score 0.9792 2020 - 07 - 02 17 : 58 : 40 , 965 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 17 : 58 : 40 , 968 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 17 : 59 : 03 , 263 epoch 53 - iter 15 / 154 - loss 0.10081433 - samples / sec : 21.80 2020 - 07 - 02 17 : 59 : 23 , 122 epoch 53 - iter 30 / 154 - loss 0.12010567 - samples / sec : 24.33 2020 - 07 - 02 17 : 59 : 43 , 761 epoch 53 - iter 45 / 154 - loss 0.11009379 - samples / sec : 23.41 2020 - 07 - 02 18 : 00 : 03 , 389 epoch 53 - iter 60 / 154 - loss 0.11138892 - samples / sec : 24.61 2020 - 07 - 02 18 : 00 : 22 , 303 epoch 53 - iter 75 / 154 - loss 0.10890932 - samples / sec : 25.56 2020 - 07 - 02 18 : 00 : 42 , 664 epoch 53 - iter 90 / 154 - loss 0.10678213 - samples / sec : 23.72 2020 - 07 - 02 18 : 01 : 04 , 707 epoch 53 - iter 105 / 154 - loss 0.10528137 - samples / sec : 22.06 2020 - 07 - 02 18 : 01 : 25 , 323 epoch 53 - iter 120 / 154 - loss 0.10567519 - samples / sec : 23.42 2020 - 07 - 02 18 : 01 : 43 , 823 epoch 53 - iter 135 / 154 - loss 0.10170225 - samples / sec : 26.11 2020 - 07 - 02 18 : 02 : 03 , 223 epoch 53 - iter 150 / 154 - loss 0.09834855 - samples / sec : 25.06 2020 - 07 - 02 18 : 02 : 07 , 660 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 02 : 07 , 661 EPOCH 53 done : loss 0.0981 - lr 0.0040545 2020 - 07 - 02 18 : 02 : 30 , 200 DEV : loss 0.20592159032821655 - score 0.9774 Epoch 53 : reducing learning rate of group 0 to 2.0273e-03 . 2020 - 07 - 02 18 : 02 : 30 , 275 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 18 : 02 : 30 , 277 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 02 : 50 , 800 epoch 54 - iter 15 / 154 - loss 0.08718331 - samples / sec : 23.70 2020 - 07 - 02 18 : 03 : 10 , 995 epoch 54 - iter 30 / 154 - loss 0.09210472 - samples / sec : 24.14 2020 - 07 - 02 18 : 03 : 31 , 577 epoch 54 - iter 45 / 154 - loss 0.10180214 - samples / sec : 23.46 2020 - 07 - 02 18 : 03 : 51 , 411 epoch 54 - iter 60 / 154 - loss 0.09472846 - samples / sec : 24.33 2020 - 07 - 02 18 : 04 : 11 , 247 epoch 54 - iter 75 / 154 - loss 0.08983106 - samples / sec : 24.35 2020 - 07 - 02 18 : 04 : 30 , 488 epoch 54 - iter 90 / 154 - loss 0.09004916 - samples / sec : 25.09 2020 - 07 - 02 18 : 04 : 49 , 388 epoch 54 - iter 105 / 154 - loss 0.08870099 - samples / sec : 25.73 2020 - 07 - 02 18 : 05 : 09 , 785 epoch 54 - iter 120 / 154 - loss 0.08760914 - samples / sec : 23.68 2020 - 07 - 02 18 : 05 : 30 , 333 epoch 54 - iter 135 / 154 - loss 0.08641136 - samples / sec : 23.48 2020 - 07 - 02 18 : 05 : 49 , 789 epoch 54 - iter 150 / 154 - loss 0.08918157 - samples / sec : 24.99 2020 - 07 - 02 18 : 05 : 54 , 518 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 05 : 54 , 520 EPOCH 54 done : loss 0.0910 - lr 0.0020273 2020 - 07 - 02 18 : 06 : 16 , 972 DEV : loss 0.19774390757083893 - score 0.9798 2020 - 07 - 02 18 : 06 : 17 , 041 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 18 : 06 : 17 , 042 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 06 : 39 , 573 epoch 55 - iter 15 / 154 - loss 0.08008183 - samples / sec : 21.79 2020 - 07 - 02 18 : 06 : 59 , 164 epoch 55 - iter 30 / 154 - loss 0.08933909 - samples / sec : 24.64 2020 - 07 - 02 18 : 07 : 18 , 806 epoch 55 - iter 45 / 154 - loss 0.09008065 - samples / sec : 24.58 2020 - 07 - 02 18 : 07 : 40 , 535 epoch 55 - iter 60 / 154 - loss 0.08874504 - samples / sec : 22.36 2020 - 07 - 02 18 : 08 : 00 , 201 epoch 55 - iter 75 / 154 - loss 0.08941119 - samples / sec : 24.56 2020 - 07 - 02 18 : 08 : 20 , 983 epoch 55 - iter 90 / 154 - loss 0.08812096 - samples / sec : 23.22 2020 - 07 - 02 18 : 08 : 40 , 361 epoch 55 - iter 105 / 154 - loss 0.08716224 - samples / sec : 25.10 2020 - 07 - 02 18 : 09 : 00 , 385 epoch 55 - iter 120 / 154 - loss 0.08778634 - samples / sec : 24.11 2020 - 07 - 02 18 : 09 : 19 , 225 epoch 55 - iter 135 / 154 - loss 0.08927797 - samples / sec : 25.64 2020 - 07 - 02 18 : 09 : 38 , 592 epoch 55 - iter 150 / 154 - loss 0.09444317 - samples / sec : 25.09 2020 - 07 - 02 18 : 09 : 43 , 560 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 09 : 43 , 561 EPOCH 55 done : loss 0.0934 - lr 0.0020273 2020 - 07 - 02 18 : 10 : 05 , 988 DEV : loss 0.19739288091659546 - score 0.978 2020 - 07 - 02 18 : 10 : 06 , 059 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 18 : 10 : 06 , 061 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 10 : 28 , 412 epoch 56 - iter 15 / 154 - loss 0.08788449 - samples / sec : 21.74 2020 - 07 - 02 18 : 10 : 46 , 877 epoch 56 - iter 30 / 154 - loss 0.08437797 - samples / sec : 26.16 2020 - 07 - 02 18 : 11 : 06 , 048 epoch 56 - iter 45 / 154 - loss 0.08514273 - samples / sec : 25.19 2020 - 07 - 02 18 : 11 : 25 , 647 epoch 56 - iter 60 / 154 - loss 0.08036039 - samples / sec : 24.79 2020 - 07 - 02 18 : 11 : 45 , 286 epoch 56 - iter 75 / 154 - loss 0.08442916 - samples / sec : 24.59 2020 - 07 - 02 18 : 12 : 06 , 783 epoch 56 - iter 90 / 154 - loss 0.08954436 - samples / sec : 22.44 2020 - 07 - 02 18 : 12 : 27 , 649 epoch 56 - iter 105 / 154 - loss 0.08976238 - samples / sec : 23.30 2020 - 07 - 02 18 : 12 : 46 , 342 epoch 56 - iter 120 / 154 - loss 0.09411574 - samples / sec : 25.84 2020 - 07 - 02 18 : 13 : 05 , 596 epoch 56 - iter 135 / 154 - loss 0.09723895 - samples / sec : 25.09 2020 - 07 - 02 18 : 13 : 24 , 877 epoch 56 - iter 150 / 154 - loss 0.09649601 - samples / sec : 25.22 2020 - 07 - 02 18 : 13 : 29 , 371 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 13 : 29 , 373 EPOCH 56 done : loss 0.0952 - lr 0.0020273 2020 - 07 - 02 18 : 13 : 51 , 797 DEV : loss 0.20011773705482483 - score 0.9786 2020 - 07 - 02 18 : 13 : 51 , 866 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 18 : 13 : 51 , 867 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 14 : 12 , 911 epoch 57 - iter 15 / 154 - loss 0.08282442 - samples / sec : 23.12 2020 - 07 - 02 18 : 14 : 32 , 947 epoch 57 - iter 30 / 154 - loss 0.09025696 - samples / sec : 24.10 2020 - 07 - 02 18 : 14 : 53 , 386 epoch 57 - iter 45 / 154 - loss 0.09394956 - samples / sec : 23.61 2020 - 07 - 02 18 : 15 : 13 , 913 epoch 57 - iter 60 / 154 - loss 0.09126146 - samples / sec : 23.66 2020 - 07 - 02 18 : 15 : 32 , 907 epoch 57 - iter 75 / 154 - loss 0.09113265 - samples / sec : 25.43 2020 - 07 - 02 18 : 15 : 52 , 413 epoch 57 - iter 90 / 154 - loss 0.08752083 - samples / sec : 24.77 2020 - 07 - 02 18 : 16 : 14 , 762 epoch 57 - iter 105 / 154 - loss 0.09105406 - samples / sec : 21.73 2020 - 07 - 02 18 : 16 : 34 , 872 epoch 57 - iter 120 / 154 - loss 0.08909690 - samples / sec : 24.00 2020 - 07 - 02 18 : 16 : 54 , 106 epoch 57 - iter 135 / 154 - loss 0.08767471 - samples / sec : 25.10 2020 - 07 - 02 18 : 17 : 13 , 265 epoch 57 - iter 150 / 154 - loss 0.08853321 - samples / sec : 25.40 2020 - 07 - 02 18 : 17 : 17 , 660 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 17 : 17 , 661 EPOCH 57 done : loss 0.0898 - lr 0.0020273 2020 - 07 - 02 18 : 17 : 40 , 137 DEV : loss 0.1950114369392395 - score 0.9804 2020 - 07 - 02 18 : 17 : 40 , 206 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 18 : 17 : 44 , 473 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 18 : 05 , 641 epoch 58 - iter 15 / 154 - loss 0.09854108 - samples / sec : 23.25 2020 - 07 - 02 18 : 18 : 26 , 107 epoch 58 - iter 30 / 154 - loss 0.09823707 - samples / sec : 23.59 2020 - 07 - 02 18 : 18 : 44 , 691 epoch 58 - iter 45 / 154 - loss 0.09728588 - samples / sec : 26.01 2020 - 07 - 02 18 : 19 : 05 , 368 epoch 58 - iter 60 / 154 - loss 0.09077503 - samples / sec : 23.50 2020 - 07 - 02 18 : 19 : 25 , 551 epoch 58 - iter 75 / 154 - loss 0.09195299 - samples / sec : 23.93 2020 - 07 - 02 18 : 19 : 46 , 168 epoch 58 - iter 90 / 154 - loss 0.08965277 - samples / sec : 23.41 2020 - 07 - 02 18 : 20 : 07 , 091 epoch 58 - iter 105 / 154 - loss 0.08772397 - samples / sec : 23.22 2020 - 07 - 02 18 : 20 : 25 , 531 epoch 58 - iter 120 / 154 - loss 0.08932425 - samples / sec : 26.19 2020 - 07 - 02 18 : 20 : 44 , 199 epoch 58 - iter 135 / 154 - loss 0.09028912 - samples / sec : 25.88 2020 - 07 - 02 18 : 21 : 04 , 364 epoch 58 - iter 150 / 154 - loss 0.09111484 - samples / sec : 24.11 2020 - 07 - 02 18 : 21 : 09 , 689 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 21 : 09 , 691 EPOCH 58 done : loss 0.0919 - lr 0.0020273 2020 - 07 - 02 18 : 21 : 32 , 360 DEV : loss 0.19827120006084442 - score 0.9798 2020 - 07 - 02 18 : 21 : 32 , 428 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 18 : 21 : 32 , 429 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 21 : 53 , 186 epoch 59 - iter 15 / 154 - loss 0.07434669 - samples / sec : 23.43 2020 - 07 - 02 18 : 22 : 13 , 121 epoch 59 - iter 30 / 154 - loss 0.08210114 - samples / sec : 24.22 2020 - 07 - 02 18 : 22 : 32 , 288 epoch 59 - iter 45 / 154 - loss 0.08505506 - samples / sec : 25.19 2020 - 07 - 02 18 : 22 : 52 , 595 epoch 59 - iter 60 / 154 - loss 0.08686025 - samples / sec : 23.79 2020 - 07 - 02 18 : 23 : 11 , 985 epoch 59 - iter 75 / 154 - loss 0.08708367 - samples / sec : 25.06 2020 - 07 - 02 18 : 23 : 32 , 762 epoch 59 - iter 90 / 154 - loss 0.09318458 - samples / sec : 23.24 2020 - 07 - 02 18 : 23 : 54 , 143 epoch 59 - iter 105 / 154 - loss 0.09390901 - samples / sec : 22.56 2020 - 07 - 02 18 : 24 : 15 , 226 epoch 59 - iter 120 / 154 - loss 0.09243909 - samples / sec : 23.06 2020 - 07 - 02 18 : 24 : 34 , 418 epoch 59 - iter 135 / 154 - loss 0.09128275 - samples / sec : 25.17 2020 - 07 - 02 18 : 24 : 54 , 375 epoch 59 - iter 150 / 154 - loss 0.09075951 - samples / sec : 24.19 2020 - 07 - 02 18 : 24 : 58 , 631 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 24 : 58 , 632 EPOCH 59 done : loss 0.0899 - lr 0.0020273 2020 - 07 - 02 18 : 25 : 21 , 438 DEV : loss 0.2027978152036667 - score 0.9804 2020 - 07 - 02 18 : 25 : 21 , 509 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 18 : 25 : 21 , 511 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 25 : 43 , 241 epoch 60 - iter 15 / 154 - loss 0.08677354 - samples / sec : 22.39 2020 - 07 - 02 18 : 26 : 02 , 464 epoch 60 - iter 30 / 154 - loss 0.08324124 - samples / sec : 25.13 2020 - 07 - 02 18 : 26 : 22 , 590 epoch 60 - iter 45 / 154 - loss 0.08803943 - samples / sec : 23.99 2020 - 07 - 02 18 : 26 : 43 , 159 epoch 60 - iter 60 / 154 - loss 0.09347135 - samples / sec : 23.47 2020 - 07 - 02 18 : 27 : 03 , 283 epoch 60 - iter 75 / 154 - loss 0.09846938 - samples / sec : 23.98 2020 - 07 - 02 18 : 27 : 23 , 185 epoch 60 - iter 90 / 154 - loss 0.09680327 - samples / sec : 24.42 2020 - 07 - 02 18 : 27 : 42 , 460 epoch 60 - iter 105 / 154 - loss 0.09433226 - samples / sec : 25.05 2020 - 07 - 02 18 : 28 : 02 , 370 epoch 60 - iter 120 / 154 - loss 0.09501102 - samples / sec : 24.25 2020 - 07 - 02 18 : 28 : 22 , 500 epoch 60 - iter 135 / 154 - loss 0.09191312 - samples / sec : 24.15 2020 - 07 - 02 18 : 28 : 43 , 973 epoch 60 - iter 150 / 154 - loss 0.09032161 - samples / sec : 22.47 2020 - 07 - 02 18 : 28 : 48 , 579 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 28 : 48 , 581 EPOCH 60 done : loss 0.0893 - lr 0.0020273 2020 - 07 - 02 18 : 29 : 11 , 275 DEV : loss 0.20585915446281433 - score 0.981 2020 - 07 - 02 18 : 29 : 11 , 343 BAD EPOCHS ( no improvement ): 0 saving best model 2020 - 07 - 02 18 : 29 : 15 , 614 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 29 : 38 , 156 epoch 61 - iter 15 / 154 - loss 0.09983344 - samples / sec : 21.55 2020 - 07 - 02 18 : 29 : 58 , 238 epoch 61 - iter 30 / 154 - loss 0.08600884 - samples / sec : 24.05 2020 - 07 - 02 18 : 30 : 17 , 840 epoch 61 - iter 45 / 154 - loss 0.08722653 - samples / sec : 24.88 2020 - 07 - 02 18 : 30 : 38 , 254 epoch 61 - iter 60 / 154 - loss 0.09154642 - samples / sec : 23.66 2020 - 07 - 02 18 : 30 : 58 , 140 epoch 61 - iter 75 / 154 - loss 0.09060006 - samples / sec : 24.27 2020 - 07 - 02 18 : 31 : 18 , 830 epoch 61 - iter 90 / 154 - loss 0.09021203 - samples / sec : 23.49 2020 - 07 - 02 18 : 31 : 39 , 009 epoch 61 - iter 105 / 154 - loss 0.09414779 - samples / sec : 23.92 2020 - 07 - 02 18 : 31 : 57 , 965 epoch 61 - iter 120 / 154 - loss 0.09225592 - samples / sec : 25.48 2020 - 07 - 02 18 : 32 : 16 , 358 epoch 61 - iter 135 / 154 - loss 0.08870085 - samples / sec : 26.26 2020 - 07 - 02 18 : 32 : 36 , 767 epoch 61 - iter 150 / 154 - loss 0.08851716 - samples / sec : 23.65 2020 - 07 - 02 18 : 32 : 41 , 348 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 32 : 41 , 350 EPOCH 61 done : loss 0.0875 - lr 0.0020273 2020 - 07 - 02 18 : 33 : 04 , 158 DEV : loss 0.19547177851200104 - score 0.9792 2020 - 07 - 02 18 : 33 : 04 , 227 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 18 : 33 : 04 , 229 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 33 : 27 , 258 epoch 62 - iter 15 / 154 - loss 0.08993260 - samples / sec : 21.10 2020 - 07 - 02 18 : 33 : 46 , 125 epoch 62 - iter 30 / 154 - loss 0.09509450 - samples / sec : 25.61 2020 - 07 - 02 18 : 34 : 06 , 236 epoch 62 - iter 45 / 154 - loss 0.08554634 - samples / sec : 24.25 2020 - 07 - 02 18 : 34 : 26 , 283 epoch 62 - iter 60 / 154 - loss 0.08996390 - samples / sec : 24.11 2020 - 07 - 02 18 : 34 : 45 , 709 epoch 62 - iter 75 / 154 - loss 0.09672508 - samples / sec : 24.85 2020 - 07 - 02 18 : 35 : 06 , 660 epoch 62 - iter 90 / 154 - loss 0.09301235 - samples / sec : 23.18 2020 - 07 - 02 18 : 35 : 26 , 352 epoch 62 - iter 105 / 154 - loss 0.09545991 - samples / sec : 24.52 2020 - 07 - 02 18 : 35 : 46 , 774 epoch 62 - iter 120 / 154 - loss 0.09430194 - samples / sec : 23.64 2020 - 07 - 02 18 : 36 : 05 , 602 epoch 62 - iter 135 / 154 - loss 0.09519594 - samples / sec : 25.83 2020 - 07 - 02 18 : 36 : 24 , 706 epoch 62 - iter 150 / 154 - loss 0.09227619 - samples / sec : 25.27 2020 - 07 - 02 18 : 36 : 28 , 841 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 36 : 28 , 843 EPOCH 62 done : loss 0.0933 - lr 0.0020273 2020 - 07 - 02 18 : 36 : 51 , 561 DEV : loss 0.1974499672651291 - score 0.9798 2020 - 07 - 02 18 : 36 : 51 , 627 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 18 : 36 : 51 , 628 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 37 : 13 , 264 epoch 63 - iter 15 / 154 - loss 0.08186235 - samples / sec : 22.46 2020 - 07 - 02 18 : 37 : 33 , 014 epoch 63 - iter 30 / 154 - loss 0.08035179 - samples / sec : 24.49 2020 - 07 - 02 18 : 37 : 51 , 954 epoch 63 - iter 45 / 154 - loss 0.09017727 - samples / sec : 25.72 2020 - 07 - 02 18 : 38 : 11 , 628 epoch 63 - iter 60 / 154 - loss 0.08745349 - samples / sec : 24.55 2020 - 07 - 02 18 : 38 : 31 , 615 epoch 63 - iter 75 / 154 - loss 0.08307302 - samples / sec : 24.16 2020 - 07 - 02 18 : 38 : 52 , 911 epoch 63 - iter 90 / 154 - loss 0.08419990 - samples / sec : 22.67 2020 - 07 - 02 18 : 39 : 14 , 286 epoch 63 - iter 105 / 154 - loss 0.08808264 - samples / sec : 22.57 2020 - 07 - 02 18 : 39 : 33 , 426 epoch 63 - iter 120 / 154 - loss 0.08803327 - samples / sec : 25.24 2020 - 07 - 02 18 : 39 : 53 , 125 epoch 63 - iter 135 / 154 - loss 0.08664566 - samples / sec : 24.65 2020 - 07 - 02 18 : 40 : 12 , 487 epoch 63 - iter 150 / 154 - loss 0.08714323 - samples / sec : 24.93 2020 - 07 - 02 18 : 40 : 16 , 662 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 40 : 16 , 663 EPOCH 63 done : loss 0.0865 - lr 0.0020273 2020 - 07 - 02 18 : 40 : 39 , 398 DEV : loss 0.2033492624759674 - score 0.9804 2020 - 07 - 02 18 : 40 : 39 , 466 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 18 : 40 : 39 , 467 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 41 : 01 , 377 epoch 64 - iter 15 / 154 - loss 0.06153077 - samples / sec : 22.19 2020 - 07 - 02 18 : 41 : 21 , 147 epoch 64 - iter 30 / 154 - loss 0.07140551 - samples / sec : 24.44 2020 - 07 - 02 18 : 41 : 42 , 317 epoch 64 - iter 45 / 154 - loss 0.08376528 - samples / sec : 23.00 2020 - 07 - 02 18 : 42 : 01 , 321 epoch 64 - iter 60 / 154 - loss 0.08542068 - samples / sec : 25.43 2020 - 07 - 02 18 : 42 : 19 , 752 epoch 64 - iter 75 / 154 - loss 0.08507289 - samples / sec : 26.20 2020 - 07 - 02 18 : 42 : 39 , 277 epoch 64 - iter 90 / 154 - loss 0.08492970 - samples / sec : 24.89 2020 - 07 - 02 18 : 43 : 00 , 022 epoch 64 - iter 105 / 154 - loss 0.08645396 - samples / sec : 23.26 2020 - 07 - 02 18 : 43 : 20 , 185 epoch 64 - iter 120 / 154 - loss 0.08791573 - samples / sec : 23.95 2020 - 07 - 02 18 : 43 : 37 , 747 epoch 64 - iter 135 / 154 - loss 0.08794287 - samples / sec : 27.71 2020 - 07 - 02 18 : 43 : 57 , 834 epoch 64 - iter 150 / 154 - loss 0.09194450 - samples / sec : 24.06 2020 - 07 - 02 18 : 44 : 02 , 897 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 44 : 02 , 898 EPOCH 64 done : loss 0.0916 - lr 0.0020273 2020 - 07 - 02 18 : 44 : 25 , 487 DEV : loss 0.19688229262828827 - score 0.9792 2020 - 07 - 02 18 : 44 : 25 , 554 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 18 : 44 : 25 , 555 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 44 : 46 , 224 epoch 65 - iter 15 / 154 - loss 0.10502815 - samples / sec : 23.54 2020 - 07 - 02 18 : 45 : 07 , 083 epoch 65 - iter 30 / 154 - loss 0.10161052 - samples / sec : 23.14 2020 - 07 - 02 18 : 45 : 27 , 173 epoch 65 - iter 45 / 154 - loss 0.10180355 - samples / sec : 24.02 2020 - 07 - 02 18 : 45 : 46 , 943 epoch 65 - iter 60 / 154 - loss 0.10438172 - samples / sec : 24.42 2020 - 07 - 02 18 : 46 : 07 , 489 epoch 65 - iter 75 / 154 - loss 0.10476604 - samples / sec : 23.64 2020 - 07 - 02 18 : 46 : 27 , 512 epoch 65 - iter 90 / 154 - loss 0.09832354 - samples / sec : 24.11 2020 - 07 - 02 18 : 46 : 46 , 517 epoch 65 - iter 105 / 154 - loss 0.09817031 - samples / sec : 25.42 2020 - 07 - 02 18 : 47 : 07 , 649 epoch 65 - iter 120 / 154 - loss 0.09402602 - samples / sec : 23.01 2020 - 07 - 02 18 : 47 : 25 , 938 epoch 65 - iter 135 / 154 - loss 0.09160565 - samples / sec : 26.42 2020 - 07 - 02 18 : 47 : 47 , 747 epoch 65 - iter 150 / 154 - loss 0.09144586 - samples / sec : 22.14 2020 - 07 - 02 18 : 47 : 52 , 233 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 47 : 52 , 234 EPOCH 65 done : loss 0.0905 - lr 0.0020273 2020 - 07 - 02 18 : 48 : 14 , 454 DEV : loss 0.20111960172653198 - score 0.9804 2020 - 07 - 02 18 : 48 : 14 , 668 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 18 : 48 : 14 , 670 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 48 : 36 , 913 epoch 66 - iter 15 / 154 - loss 0.07612641 - samples / sec : 21.87 2020 - 07 - 02 18 : 48 : 56 , 241 epoch 66 - iter 30 / 154 - loss 0.08649506 - samples / sec : 24.99 2020 - 07 - 02 18 : 49 : 16 , 092 epoch 66 - iter 45 / 154 - loss 0.08143805 - samples / sec : 24.53 2020 - 07 - 02 18 : 49 : 36 , 784 epoch 66 - iter 60 / 154 - loss 0.08196701 - samples / sec : 23.33 2020 - 07 - 02 18 : 49 : 58 , 046 epoch 66 - iter 75 / 154 - loss 0.08096401 - samples / sec : 22.71 2020 - 07 - 02 18 : 50 : 17 , 695 epoch 66 - iter 90 / 154 - loss 0.07823467 - samples / sec : 24.58 2020 - 07 - 02 18 : 50 : 36 , 144 epoch 66 - iter 105 / 154 - loss 0.08322996 - samples / sec : 26.39 2020 - 07 - 02 18 : 50 : 55 , 814 epoch 66 - iter 120 / 154 - loss 0.08491590 - samples / sec : 24.56 2020 - 07 - 02 18 : 51 : 16 , 722 epoch 66 - iter 135 / 154 - loss 0.08727617 - samples / sec : 23.10 2020 - 07 - 02 18 : 51 : 35 , 800 epoch 66 - iter 150 / 154 - loss 0.08726892 - samples / sec : 25.32 2020 - 07 - 02 18 : 51 : 40 , 273 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 51 : 40 , 275 EPOCH 66 done : loss 0.0883 - lr 0.0020273 2020 - 07 - 02 18 : 52 : 02 , 781 DEV : loss 0.19793111085891724 - score 0.9798 Epoch 66 : reducing learning rate of group 0 to 1.0136e-03 . 2020 - 07 - 02 18 : 52 : 02 , 851 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 18 : 52 : 02 , 853 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 52 : 27 , 895 epoch 67 - iter 15 / 154 - loss 0.12642477 - samples / sec : 19.38 2020 - 07 - 02 18 : 52 : 47 , 763 epoch 67 - iter 30 / 154 - loss 0.09747878 - samples / sec : 24.54 2020 - 07 - 02 18 : 53 : 07 , 080 epoch 67 - iter 45 / 154 - loss 0.08789005 - samples / sec : 25.00 2020 - 07 - 02 18 : 53 : 26 , 735 epoch 67 - iter 60 / 154 - loss 0.08494026 - samples / sec : 24.57 2020 - 07 - 02 18 : 53 : 46 , 315 epoch 67 - iter 75 / 154 - loss 0.08639383 - samples / sec : 24.67 2020 - 07 - 02 18 : 54 : 06 , 712 epoch 67 - iter 90 / 154 - loss 0.08593876 - samples / sec : 23.81 2020 - 07 - 02 18 : 54 : 25 , 259 epoch 67 - iter 105 / 154 - loss 0.08223052 - samples / sec : 26.03 2020 - 07 - 02 18 : 54 : 44 , 372 epoch 67 - iter 120 / 154 - loss 0.07795331 - samples / sec : 25.27 2020 - 07 - 02 18 : 55 : 03 , 642 epoch 67 - iter 135 / 154 - loss 0.07723948 - samples / sec : 25.22 2020 - 07 - 02 18 : 55 : 23 , 678 epoch 67 - iter 150 / 154 - loss 0.07914580 - samples / sec : 24.12 2020 - 07 - 02 18 : 55 : 28 , 023 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 55 : 28 , 024 EPOCH 67 done : loss 0.0788 - lr 0.0010136 2020 - 07 - 02 18 : 55 : 50 , 431 DEV : loss 0.196882426738739 - score 0.9804 2020 - 07 - 02 18 : 55 : 50 , 500 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 18 : 55 : 50 , 502 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 56 : 13 , 151 epoch 68 - iter 15 / 154 - loss 0.11901739 - samples / sec : 21.44 2020 - 07 - 02 18 : 56 : 33 , 569 epoch 68 - iter 30 / 154 - loss 0.10123495 - samples / sec : 23.64 2020 - 07 - 02 18 : 56 : 53 , 516 epoch 68 - iter 45 / 154 - loss 0.10136171 - samples / sec : 24.37 2020 - 07 - 02 18 : 57 : 14 , 476 epoch 68 - iter 60 / 154 - loss 0.09211459 - samples / sec : 23.03 2020 - 07 - 02 18 : 57 : 32 , 709 epoch 68 - iter 75 / 154 - loss 0.09430514 - samples / sec : 26.50 2020 - 07 - 02 18 : 57 : 53 , 853 epoch 68 - iter 90 / 154 - loss 0.09572636 - samples / sec : 22.84 2020 - 07 - 02 18 : 58 : 13 , 391 epoch 68 - iter 105 / 154 - loss 0.09273255 - samples / sec : 24.89 2020 - 07 - 02 18 : 58 : 33 , 704 epoch 68 - iter 120 / 154 - loss 0.09239831 - samples / sec : 23.76 2020 - 07 - 02 18 : 58 : 52 , 488 epoch 68 - iter 135 / 154 - loss 0.08849469 - samples / sec : 25.92 2020 - 07 - 02 18 : 59 : 12 , 410 epoch 68 - iter 150 / 154 - loss 0.08853157 - samples / sec : 24.24 2020 - 07 - 02 18 : 59 : 16 , 879 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 18 : 59 : 16 , 880 EPOCH 68 done : loss 0.0872 - lr 0.0010136 2020 - 07 - 02 18 : 59 : 39 , 437 DEV : loss 0.1948608160018921 - score 0.9798 2020 - 07 - 02 18 : 59 : 39 , 507 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 18 : 59 : 39 , 509 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 00 : 03 , 769 epoch 69 - iter 15 / 154 - loss 0.09427100 - samples / sec : 20.23 2020 - 07 - 02 19 : 00 : 22 , 554 epoch 69 - iter 30 / 154 - loss 0.09612075 - samples / sec : 25.72 2020 - 07 - 02 19 : 00 : 45 , 090 epoch 69 - iter 45 / 154 - loss 0.09061571 - samples / sec : 21.41 2020 - 07 - 02 19 : 01 : 05 , 612 epoch 69 - iter 60 / 154 - loss 0.08845754 - samples / sec : 23.53 2020 - 07 - 02 19 : 01 : 24 , 844 epoch 69 - iter 75 / 154 - loss 0.08787593 - samples / sec : 25.11 2020 - 07 - 02 19 : 01 : 44 , 256 epoch 69 - iter 90 / 154 - loss 0.08266684 - samples / sec : 24.87 2020 - 07 - 02 19 : 02 : 05 , 028 epoch 69 - iter 105 / 154 - loss 0.08399499 - samples / sec : 23.39 2020 - 07 - 02 19 : 02 : 24 , 578 epoch 69 - iter 120 / 154 - loss 0.08212113 - samples / sec : 24.69 2020 - 07 - 02 19 : 02 : 44 , 269 epoch 69 - iter 135 / 154 - loss 0.08719511 - samples / sec : 24.51 2020 - 07 - 02 19 : 03 : 03 , 110 epoch 69 - iter 150 / 154 - loss 0.08658361 - samples / sec : 25.65 2020 - 07 - 02 19 : 03 : 07 , 068 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 03 : 07 , 069 EPOCH 69 done : loss 0.0862 - lr 0.0010136 2020 - 07 - 02 19 : 03 : 29 , 523 DEV : loss 0.2046564817428589 - score 0.9792 2020 - 07 - 02 19 : 03 : 29 , 606 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 19 : 03 : 29 , 608 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 03 : 53 , 614 epoch 70 - iter 15 / 154 - loss 0.07197015 - samples / sec : 20.42 2020 - 07 - 02 19 : 04 : 12 , 712 epoch 70 - iter 30 / 154 - loss 0.06957877 - samples / sec : 25.30 2020 - 07 - 02 19 : 04 : 32 , 036 epoch 70 - iter 45 / 154 - loss 0.07140003 - samples / sec : 24.97 2020 - 07 - 02 19 : 04 : 52 , 137 epoch 70 - iter 60 / 154 - loss 0.07865867 - samples / sec : 24.19 2020 - 07 - 02 19 : 05 : 13 , 204 epoch 70 - iter 75 / 154 - loss 0.08453413 - samples / sec : 22.91 2020 - 07 - 02 19 : 05 : 33 , 592 epoch 70 - iter 90 / 154 - loss 0.08186147 - samples / sec : 23.69 2020 - 07 - 02 19 : 05 : 52 , 073 epoch 70 - iter 105 / 154 - loss 0.08007018 - samples / sec : 26.35 2020 - 07 - 02 19 : 06 : 11 , 649 epoch 70 - iter 120 / 154 - loss 0.08037839 - samples / sec : 24.67 2020 - 07 - 02 19 : 06 : 30 , 310 epoch 70 - iter 135 / 154 - loss 0.07974631 - samples / sec : 25.87 2020 - 07 - 02 19 : 06 : 50 , 797 epoch 70 - iter 150 / 154 - loss 0.07986848 - samples / sec : 23.72 2020 - 07 - 02 19 : 06 : 55 , 307 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 06 : 55 , 308 EPOCH 70 done : loss 0.0803 - lr 0.0010136 2020 - 07 - 02 19 : 07 : 17 , 712 DEV : loss 0.20093274116516113 - score 0.9792 2020 - 07 - 02 19 : 07 : 17 , 781 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 19 : 07 : 17 , 783 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 07 : 38 , 876 epoch 71 - iter 15 / 154 - loss 0.07919160 - samples / sec : 23.06 2020 - 07 - 02 19 : 07 : 58 , 406 epoch 71 - iter 30 / 154 - loss 0.08073911 - samples / sec : 25.00 2020 - 07 - 02 19 : 08 : 18 , 014 epoch 71 - iter 45 / 154 - loss 0.07823560 - samples / sec : 24.62 2020 - 07 - 02 19 : 08 : 37 , 811 epoch 71 - iter 60 / 154 - loss 0.07837923 - samples / sec : 24.41 2020 - 07 - 02 19 : 08 : 57 , 770 epoch 71 - iter 75 / 154 - loss 0.07954901 - samples / sec : 24.36 2020 - 07 - 02 19 : 09 : 17 , 131 epoch 71 - iter 90 / 154 - loss 0.08208104 - samples / sec : 24.94 2020 - 07 - 02 19 : 09 : 36 , 642 epoch 71 - iter 105 / 154 - loss 0.08020370 - samples / sec : 24.75 2020 - 07 - 02 19 : 09 : 57 , 211 epoch 71 - iter 120 / 154 - loss 0.08218516 - samples / sec : 23.62 2020 - 07 - 02 19 : 10 : 16 , 405 epoch 71 - iter 135 / 154 - loss 0.08415195 - samples / sec : 25.17 2020 - 07 - 02 19 : 10 : 37 , 174 epoch 71 - iter 150 / 154 - loss 0.08384235 - samples / sec : 23.25 2020 - 07 - 02 19 : 10 : 43 , 228 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 10 : 43 , 229 EPOCH 71 done : loss 0.0846 - lr 0.0010136 2020 - 07 - 02 19 : 11 : 06 , 086 DEV : loss 0.1951770782470703 - score 0.9798 2020 - 07 - 02 19 : 11 : 06 , 157 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 19 : 11 : 06 , 159 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 11 : 28 , 401 epoch 72 - iter 15 / 154 - loss 0.07519316 - samples / sec : 21.84 2020 - 07 - 02 19 : 11 : 47 , 943 epoch 72 - iter 30 / 154 - loss 0.07725611 - samples / sec : 24.71 2020 - 07 - 02 19 : 12 : 08 , 292 epoch 72 - iter 45 / 154 - loss 0.08135867 - samples / sec : 23.94 2020 - 07 - 02 19 : 12 : 28 , 281 epoch 72 - iter 60 / 154 - loss 0.09061311 - samples / sec : 24.15 2020 - 07 - 02 19 : 12 : 48 , 774 epoch 72 - iter 75 / 154 - loss 0.08907134 - samples / sec : 23.56 2020 - 07 - 02 19 : 13 : 08 , 770 epoch 72 - iter 90 / 154 - loss 0.08749927 - samples / sec : 24.14 2020 - 07 - 02 19 : 13 : 27 , 640 epoch 72 - iter 105 / 154 - loss 0.08971226 - samples / sec : 25.59 2020 - 07 - 02 19 : 13 : 46 , 871 epoch 72 - iter 120 / 154 - loss 0.08774978 - samples / sec : 25.11 2020 - 07 - 02 19 : 14 : 06 , 861 epoch 72 - iter 135 / 154 - loss 0.08598488 - samples / sec : 24.31 2020 - 07 - 02 19 : 14 : 25 , 930 epoch 72 - iter 150 / 154 - loss 0.08677177 - samples / sec : 25.33 2020 - 07 - 02 19 : 14 : 30 , 812 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 14 : 30 , 813 EPOCH 72 done : loss 0.0855 - lr 0.0010136 2020 - 07 - 02 19 : 14 : 53 , 238 DEV : loss 0.19694292545318604 - score 0.9798 Epoch 72 : reducing learning rate of group 0 to 5.0682e-04 . 2020 - 07 - 02 19 : 14 : 53 , 307 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 19 : 14 : 53 , 309 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 15 : 15 , 456 epoch 73 - iter 15 / 154 - loss 0.12583278 - samples / sec : 22.17 2020 - 07 - 02 19 : 15 : 36 , 595 epoch 73 - iter 30 / 154 - loss 0.10992353 - samples / sec : 22.83 2020 - 07 - 02 19 : 15 : 57 , 197 epoch 73 - iter 45 / 154 - loss 0.09559164 - samples / sec : 23.44 2020 - 07 - 02 19 : 16 : 17 , 290 epoch 73 - iter 60 / 154 - loss 0.08711064 - samples / sec : 24.03 2020 - 07 - 02 19 : 16 : 37 , 625 epoch 73 - iter 75 / 154 - loss 0.08157262 - samples / sec : 23.75 2020 - 07 - 02 19 : 16 : 57 , 143 epoch 73 - iter 90 / 154 - loss 0.08081692 - samples / sec : 24.75 2020 - 07 - 02 19 : 17 : 17 , 445 epoch 73 - iter 105 / 154 - loss 0.08004153 - samples / sec : 23.99 2020 - 07 - 02 19 : 17 : 37 , 636 epoch 73 - iter 120 / 154 - loss 0.07894342 - samples / sec : 23.90 2020 - 07 - 02 19 : 17 : 57 , 886 epoch 73 - iter 135 / 154 - loss 0.08008806 - samples / sec : 23.84 2020 - 07 - 02 19 : 18 : 16 , 158 epoch 73 - iter 150 / 154 - loss 0.08376308 - samples / sec : 26.62 2020 - 07 - 02 19 : 18 : 20 , 767 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 18 : 20 , 769 EPOCH 73 done : loss 0.0835 - lr 0.0005068 2020 - 07 - 02 19 : 18 : 43 , 129 DEV : loss 0.19665665924549103 - score 0.9798 2020 - 07 - 02 19 : 18 : 43 , 198 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 19 : 18 : 43 , 199 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 19 : 05 , 401 epoch 74 - iter 15 / 154 - loss 0.07577754 - samples / sec : 22.09 2020 - 07 - 02 19 : 19 : 26 , 893 epoch 74 - iter 30 / 154 - loss 0.09408136 - samples / sec : 22.46 2020 - 07 - 02 19 : 19 : 47 , 188 epoch 74 - iter 45 / 154 - loss 0.08224051 - samples / sec : 23.78 2020 - 07 - 02 19 : 20 : 06 , 562 epoch 74 - iter 60 / 154 - loss 0.09164365 - samples / sec : 25.14 2020 - 07 - 02 19 : 20 : 25 , 430 epoch 74 - iter 75 / 154 - loss 0.09134524 - samples / sec : 25.58 2020 - 07 - 02 19 : 20 : 45 , 004 epoch 74 - iter 90 / 154 - loss 0.08959111 - samples / sec : 24.67 2020 - 07 - 02 19 : 21 : 03 , 477 epoch 74 - iter 105 / 154 - loss 0.08734923 - samples / sec : 26.36 2020 - 07 - 02 19 : 21 : 23 , 457 epoch 74 - iter 120 / 154 - loss 0.08620086 - samples / sec : 24.16 2020 - 07 - 02 19 : 21 : 43 , 214 epoch 74 - iter 135 / 154 - loss 0.08835269 - samples / sec : 24.61 2020 - 07 - 02 19 : 22 : 04 , 287 epoch 74 - iter 150 / 154 - loss 0.08608258 - samples / sec : 22.90 2020 - 07 - 02 19 : 22 : 08 , 952 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 22 : 08 , 954 EPOCH 74 done : loss 0.0853 - lr 0.0005068 2020 - 07 - 02 19 : 22 : 31 , 319 DEV : loss 0.19973701238632202 - score 0.9798 2020 - 07 - 02 19 : 22 : 31 , 390 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 19 : 22 : 31 , 391 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 22 : 53 , 298 epoch 75 - iter 15 / 154 - loss 0.07400509 - samples / sec : 22.44 2020 - 07 - 02 19 : 23 : 11 , 752 epoch 75 - iter 30 / 154 - loss 0.07855947 - samples / sec : 26.18 2020 - 07 - 02 19 : 23 : 32 , 338 epoch 75 - iter 45 / 154 - loss 0.08280147 - samples / sec : 23.45 2020 - 07 - 02 19 : 23 : 51 , 953 epoch 75 - iter 60 / 154 - loss 0.08387791 - samples / sec : 24.81 2020 - 07 - 02 19 : 24 : 11 , 685 epoch 75 - iter 75 / 154 - loss 0.09056895 - samples / sec : 24.47 2020 - 07 - 02 19 : 24 : 31 , 575 epoch 75 - iter 90 / 154 - loss 0.09015049 - samples / sec : 24.28 2020 - 07 - 02 19 : 24 : 51 , 849 epoch 75 - iter 105 / 154 - loss 0.08562251 - samples / sec : 23.97 2020 - 07 - 02 19 : 25 : 11 , 238 epoch 75 - iter 120 / 154 - loss 0.08479583 - samples / sec : 24.89 2020 - 07 - 02 19 : 25 : 31 , 590 epoch 75 - iter 135 / 154 - loss 0.08538203 - samples / sec : 23.87 2020 - 07 - 02 19 : 25 : 50 , 109 epoch 75 - iter 150 / 154 - loss 0.08300129 - samples / sec : 26.08 2020 - 07 - 02 19 : 25 : 54 , 984 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 25 : 54 , 986 EPOCH 75 done : loss 0.0832 - lr 0.0005068 2020 - 07 - 02 19 : 26 : 17 , 461 DEV : loss 0.19750793278217316 - score 0.9804 2020 - 07 - 02 19 : 26 : 17 , 531 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 19 : 26 : 17 , 533 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 26 : 40 , 256 epoch 76 - iter 15 / 154 - loss 0.10106792 - samples / sec : 21.59 2020 - 07 - 02 19 : 27 : 00 , 857 epoch 76 - iter 30 / 154 - loss 0.08253583 - samples / sec : 23.43 2020 - 07 - 02 19 : 27 : 21 , 793 epoch 76 - iter 45 / 154 - loss 0.09450863 - samples / sec : 23.21 2020 - 07 - 02 19 : 27 : 40 , 802 epoch 76 - iter 60 / 154 - loss 0.09367372 - samples / sec : 25.41 2020 - 07 - 02 19 : 28 : 00 , 062 epoch 76 - iter 75 / 154 - loss 0.09405521 - samples / sec : 25.08 2020 - 07 - 02 19 : 28 : 19 , 812 epoch 76 - iter 90 / 154 - loss 0.08686411 - samples / sec : 24.61 2020 - 07 - 02 19 : 28 : 39 , 040 epoch 76 - iter 105 / 154 - loss 0.08190449 - samples / sec : 25.14 2020 - 07 - 02 19 : 28 : 57 , 359 epoch 76 - iter 120 / 154 - loss 0.08770324 - samples / sec : 26.38 2020 - 07 - 02 19 : 29 : 17 , 265 epoch 76 - iter 135 / 154 - loss 0.09068281 - samples / sec : 24.39 2020 - 07 - 02 19 : 29 : 37 , 734 epoch 76 - iter 150 / 154 - loss 0.08872985 - samples / sec : 23.59 2020 - 07 - 02 19 : 29 : 43 , 601 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 29 : 43 , 603 EPOCH 76 done : loss 0.0894 - lr 0.0005068 2020 - 07 - 02 19 : 30 : 06 , 126 DEV : loss 0.1972895711660385 - score 0.9804 2020 - 07 - 02 19 : 30 : 06 , 337 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 19 : 30 : 06 , 338 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 30 : 27 , 741 epoch 77 - iter 15 / 154 - loss 0.10128506 - samples / sec : 22.72 2020 - 07 - 02 19 : 30 : 47 , 730 epoch 77 - iter 30 / 154 - loss 0.10225451 - samples / sec : 24.17 2020 - 07 - 02 19 : 31 : 08 , 978 epoch 77 - iter 45 / 154 - loss 0.09445847 - samples / sec : 22.90 2020 - 07 - 02 19 : 31 : 28 , 184 epoch 77 - iter 60 / 154 - loss 0.09758655 - samples / sec : 25.15 2020 - 07 - 02 19 : 31 : 47 , 413 epoch 77 - iter 75 / 154 - loss 0.09338443 - samples / sec : 25.11 2020 - 07 - 02 19 : 32 : 06 , 982 epoch 77 - iter 90 / 154 - loss 0.08988446 - samples / sec : 24.69 2020 - 07 - 02 19 : 32 : 26 , 392 epoch 77 - iter 105 / 154 - loss 0.08855609 - samples / sec : 24.88 2020 - 07 - 02 19 : 32 : 44 , 943 epoch 77 - iter 120 / 154 - loss 0.09002902 - samples / sec : 26.22 2020 - 07 - 02 19 : 33 : 03 , 714 epoch 77 - iter 135 / 154 - loss 0.08720513 - samples / sec : 25.72 2020 - 07 - 02 19 : 33 : 25 , 042 epoch 77 - iter 150 / 154 - loss 0.08530933 - samples / sec : 22.62 2020 - 07 - 02 19 : 33 : 29 , 991 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 33 : 29 , 992 EPOCH 77 done : loss 0.0887 - lr 0.0005068 2020 - 07 - 02 19 : 33 : 53 , 013 DEV : loss 0.19450919330120087 - score 0.9798 2020 - 07 - 02 19 : 33 : 53 , 083 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 19 : 33 : 53 , 085 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 34 : 14 , 455 epoch 78 - iter 15 / 154 - loss 0.05516994 - samples / sec : 22.75 2020 - 07 - 02 19 : 34 : 34 , 304 epoch 78 - iter 30 / 154 - loss 0.05071913 - samples / sec : 24.59 2020 - 07 - 02 19 : 34 : 54 , 316 epoch 78 - iter 45 / 154 - loss 0.06652978 - samples / sec : 24.12 2020 - 07 - 02 19 : 35 : 13 , 264 epoch 78 - iter 60 / 154 - loss 0.07676350 - samples / sec : 25.50 2020 - 07 - 02 19 : 35 : 32 , 371 epoch 78 - iter 75 / 154 - loss 0.07474116 - samples / sec : 25.28 2020 - 07 - 02 19 : 35 : 52 , 925 epoch 78 - iter 90 / 154 - loss 0.07864122 - samples / sec : 23.65 2020 - 07 - 02 19 : 36 : 12 , 549 epoch 78 - iter 105 / 154 - loss 0.08017938 - samples / sec : 24.62 2020 - 07 - 02 19 : 36 : 33 , 222 epoch 78 - iter 120 / 154 - loss 0.08172559 - samples / sec : 23.40 2020 - 07 - 02 19 : 36 : 54 , 373 epoch 78 - iter 135 / 154 - loss 0.08014307 - samples / sec : 22.96 2020 - 07 - 02 19 : 37 : 14 , 628 epoch 78 - iter 150 / 154 - loss 0.07845808 - samples / sec : 23.83 2020 - 07 - 02 19 : 37 : 19 , 890 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 37 : 19 , 891 EPOCH 78 done : loss 0.0799 - lr 0.0005068 2020 - 07 - 02 19 : 37 : 42 , 493 DEV : loss 0.19961152970790863 - score 0.9786 Epoch 78 : reducing learning rate of group 0 to 2.5341e-04 . 2020 - 07 - 02 19 : 37 : 42 , 593 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 19 : 37 : 42 , 595 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 38 : 03 , 566 epoch 79 - iter 15 / 154 - loss 0.07821662 - samples / sec : 23.50 2020 - 07 - 02 19 : 38 : 22 , 832 epoch 79 - iter 30 / 154 - loss 0.08100213 - samples / sec : 25.07 2020 - 07 - 02 19 : 38 : 42 , 538 epoch 79 - iter 45 / 154 - loss 0.08145519 - samples / sec : 24.51 2020 - 07 - 02 19 : 39 : 02 , 437 epoch 79 - iter 60 / 154 - loss 0.08334038 - samples / sec : 24.25 2020 - 07 - 02 19 : 39 : 22 , 840 epoch 79 - iter 75 / 154 - loss 0.08079175 - samples / sec : 23.67 2020 - 07 - 02 19 : 39 : 44 , 219 epoch 79 - iter 90 / 154 - loss 0.08068648 - samples / sec : 22.59 2020 - 07 - 02 19 : 40 : 04 , 445 epoch 79 - iter 105 / 154 - loss 0.08182918 - samples / sec : 23.88 2020 - 07 - 02 19 : 40 : 23 , 599 epoch 79 - iter 120 / 154 - loss 0.08141914 - samples / sec : 25.42 2020 - 07 - 02 19 : 40 : 43 , 474 epoch 79 - iter 135 / 154 - loss 0.08588830 - samples / sec : 24.30 2020 - 07 - 02 19 : 41 : 02 , 682 epoch 79 - iter 150 / 154 - loss 0.08469767 - samples / sec : 25.15 2020 - 07 - 02 19 : 41 : 07 , 627 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 41 : 07 , 628 EPOCH 79 done : loss 0.0856 - lr 0.0002534 2020 - 07 - 02 19 : 41 : 30 , 538 DEV : loss 0.19733120501041412 - score 0.9798 2020 - 07 - 02 19 : 41 : 30 , 606 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 19 : 41 : 30 , 609 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 41 : 54 , 035 epoch 80 - iter 15 / 154 - loss 0.08911656 - samples / sec : 20.74 2020 - 07 - 02 19 : 42 : 14 , 513 epoch 80 - iter 30 / 154 - loss 0.09176585 - samples / sec : 23.79 2020 - 07 - 02 19 : 42 : 33 , 470 epoch 80 - iter 45 / 154 - loss 0.08643438 - samples / sec : 25.47 2020 - 07 - 02 19 : 42 : 52 , 061 epoch 80 - iter 60 / 154 - loss 0.08399421 - samples / sec : 25.99 2020 - 07 - 02 19 : 43 : 13 , 561 epoch 80 - iter 75 / 154 - loss 0.08570782 - samples / sec : 22.60 2020 - 07 - 02 19 : 43 : 33 , 324 epoch 80 - iter 90 / 154 - loss 0.08288445 - samples / sec : 24.43 2020 - 07 - 02 19 : 43 : 52 , 847 epoch 80 - iter 105 / 154 - loss 0.08118652 - samples / sec : 24.72 2020 - 07 - 02 19 : 44 : 13 , 126 epoch 80 - iter 120 / 154 - loss 0.08139978 - samples / sec : 23.96 2020 - 07 - 02 19 : 44 : 33 , 738 epoch 80 - iter 135 / 154 - loss 0.08077319 - samples / sec : 23.42 2020 - 07 - 02 19 : 44 : 54 , 207 epoch 80 - iter 150 / 154 - loss 0.08154344 - samples / sec : 23.58 2020 - 07 - 02 19 : 44 : 58 , 843 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 44 : 58 , 845 EPOCH 80 done : loss 0.0816 - lr 0.0002534 2020 - 07 - 02 19 : 45 : 21 , 976 DEV : loss 0.19819292426109314 - score 0.9798 2020 - 07 - 02 19 : 45 : 22 , 052 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 19 : 45 : 22 , 055 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 45 : 43 , 473 epoch 81 - iter 15 / 154 - loss 0.11282083 - samples / sec : 22.71 2020 - 07 - 02 19 : 46 : 24 , 008 epoch 81 - iter 45 / 154 - loss 0.08270191 - samples / sec : 23.23 2020 - 07 - 02 19 : 46 : 42 , 832 epoch 81 - iter 60 / 154 - loss 0.08318824 - samples / sec : 25.66 2020 - 07 - 02 19 : 47 : 02 , 499 epoch 81 - iter 75 / 154 - loss 0.08471441 - samples / sec : 24.57 2020 - 07 - 02 19 : 47 : 22 , 738 epoch 81 - iter 90 / 154 - loss 0.08327459 - samples / sec : 23.85 2020 - 07 - 02 19 : 47 : 42 , 972 epoch 81 - iter 105 / 154 - loss 0.07872031 - samples / sec : 23.86 2020 - 07 - 02 19 : 48 : 01 , 928 epoch 81 - iter 120 / 154 - loss 0.07652540 - samples / sec : 25.68 2020 - 07 - 02 19 : 48 : 22 , 614 epoch 81 - iter 135 / 154 - loss 0.07975565 - samples / sec : 23.34 2020 - 07 - 02 19 : 48 : 41 , 787 epoch 81 - iter 150 / 154 - loss 0.07985378 - samples / sec : 25.18 2020 - 07 - 02 19 : 48 : 46 , 539 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 48 : 46 , 541 EPOCH 81 done : loss 0.0798 - lr 0.0002534 2020 - 07 - 02 19 : 49 : 09 , 276 DEV : loss 0.19881562888622284 - score 0.9792 2020 - 07 - 02 19 : 49 : 09 , 344 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 19 : 49 : 09 , 346 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 49 : 33 , 998 epoch 82 - iter 15 / 154 - loss 0.08336314 - samples / sec : 19.69 2020 - 07 - 02 19 : 49 : 53 , 295 epoch 82 - iter 30 / 154 - loss 0.08455563 - samples / sec : 25.27 2020 - 07 - 02 19 : 50 : 12 , 593 epoch 82 - iter 45 / 154 - loss 0.08075597 - samples / sec : 25.02 2020 - 07 - 02 19 : 50 : 31 , 678 epoch 82 - iter 60 / 154 - loss 0.07854668 - samples / sec : 25.30 2020 - 07 - 02 19 : 50 : 52 , 930 epoch 82 - iter 75 / 154 - loss 0.08441896 - samples / sec : 22.72 2020 - 07 - 02 19 : 51 : 12 , 043 epoch 82 - iter 90 / 154 - loss 0.07963300 - samples / sec : 25.26 2020 - 07 - 02 19 : 51 : 29 , 700 epoch 82 - iter 105 / 154 - loss 0.08339172 - samples / sec : 27.55 2020 - 07 - 02 19 : 51 : 49 , 879 epoch 82 - iter 120 / 154 - loss 0.08139258 - samples / sec : 23.93 2020 - 07 - 02 19 : 52 : 09 , 504 epoch 82 - iter 135 / 154 - loss 0.08187230 - samples / sec : 24.59 2020 - 07 - 02 19 : 52 : 29 , 636 epoch 82 - iter 150 / 154 - loss 0.08149730 - samples / sec : 24.17 2020 - 07 - 02 19 : 52 : 34 , 259 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 52 : 34 , 261 EPOCH 82 done : loss 0.0802 - lr 0.0002534 2020 - 07 - 02 19 : 52 : 56 , 633 DEV : loss 0.1967770755290985 - score 0.9798 2020 - 07 - 02 19 : 52 : 56 , 703 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 19 : 52 : 56 , 706 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 53 : 18 , 491 epoch 83 - iter 15 / 154 - loss 0.08312181 - samples / sec : 22.32 2020 - 07 - 02 19 : 53 : 39 , 588 epoch 83 - iter 30 / 154 - loss 0.08574174 - samples / sec : 23.12 2020 - 07 - 02 19 : 53 : 58 , 547 epoch 83 - iter 45 / 154 - loss 0.08475116 - samples / sec : 25.48 2020 - 07 - 02 19 : 54 : 19 , 207 epoch 83 - iter 60 / 154 - loss 0.07566535 - samples / sec : 23.38 2020 - 07 - 02 19 : 54 : 39 , 070 epoch 83 - iter 75 / 154 - loss 0.07675807 - samples / sec : 24.31 2020 - 07 - 02 19 : 54 : 59 , 788 epoch 83 - iter 90 / 154 - loss 0.07665076 - samples / sec : 23.43 2020 - 07 - 02 19 : 55 : 19 , 537 epoch 83 - iter 105 / 154 - loss 0.07946286 - samples / sec : 24.45 2020 - 07 - 02 19 : 55 : 39 , 566 epoch 83 - iter 120 / 154 - loss 0.07992272 - samples / sec : 24.10 2020 - 07 - 02 19 : 55 : 57 , 919 epoch 83 - iter 135 / 154 - loss 0.08126198 - samples / sec : 26.51 2020 - 07 - 02 19 : 56 : 17 , 369 epoch 83 - iter 150 / 154 - loss 0.08188783 - samples / sec : 24.81 2020 - 07 - 02 19 : 56 : 21 , 455 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 56 : 21 , 456 EPOCH 83 done : loss 0.0820 - lr 0.0002534 2020 - 07 - 02 19 : 56 : 44 , 267 DEV : loss 0.19680184125900269 - score 0.9798 2020 - 07 - 02 19 : 56 : 44 , 335 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 19 : 56 : 44 , 337 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 19 : 57 : 06 , 208 epoch 84 - iter 15 / 154 - loss 0.06290983 - samples / sec : 22.22 2020 - 07 - 02 19 : 57 : 24 , 662 epoch 84 - iter 30 / 154 - loss 0.07252258 - samples / sec : 26.19 2020 - 07 - 02 19 : 57 : 43 , 378 epoch 84 - iter 45 / 154 - loss 0.07217186 - samples / sec : 26.06 2020 - 07 - 02 19 : 58 : 03 , 579 epoch 84 - iter 60 / 154 - loss 0.07073096 - samples / sec : 23.90 2020 - 07 - 02 19 : 58 : 23 , 163 epoch 84 - iter 75 / 154 - loss 0.07663974 - samples / sec : 24.65 2020 - 07 - 02 19 : 58 : 42 , 104 epoch 84 - iter 90 / 154 - loss 0.07967136 - samples / sec : 25.69 2020 - 07 - 02 19 : 59 : 01 , 744 epoch 84 - iter 105 / 154 - loss 0.08082978 - samples / sec : 24.59 2020 - 07 - 02 19 : 59 : 23 , 133 epoch 84 - iter 120 / 154 - loss 0.08198013 - samples / sec : 22.57 2020 - 07 - 02 19 : 59 : 43 , 063 epoch 84 - iter 135 / 154 - loss 0.08208106 - samples / sec : 24.39 2020 - 07 - 02 20 : 00 : 04 , 913 epoch 84 - iter 150 / 154 - loss 0.08289737 - samples / sec : 22.07 2020 - 07 - 02 20 : 00 : 09 , 399 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 00 : 09 , 400 EPOCH 84 done : loss 0.0832 - lr 0.0002534 2020 - 07 - 02 20 : 00 : 31 , 750 DEV : loss 0.19807276129722595 - score 0.9798 Epoch 84 : reducing learning rate of group 0 to 1.2670e-04 . 2020 - 07 - 02 20 : 00 : 31 , 974 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 20 : 00 : 31 , 976 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 00 : 55 , 472 epoch 85 - iter 15 / 154 - loss 0.08761623 - samples / sec : 20.67 2020 - 07 - 02 20 : 01 : 14 , 145 epoch 85 - iter 30 / 154 - loss 0.08436980 - samples / sec : 25.88 2020 - 07 - 02 20 : 01 : 34 , 055 epoch 85 - iter 45 / 154 - loss 0.08638632 - samples / sec : 24.25 2020 - 07 - 02 20 : 01 : 53 , 168 epoch 85 - iter 60 / 154 - loss 0.09292158 - samples / sec : 25.49 2020 - 07 - 02 20 : 02 : 12 , 416 epoch 85 - iter 75 / 154 - loss 0.08979836 - samples / sec : 25.09 2020 - 07 - 02 20 : 02 : 32 , 606 epoch 85 - iter 90 / 154 - loss 0.08738348 - samples / sec : 23.91 2020 - 07 - 02 20 : 02 : 53 , 312 epoch 85 - iter 105 / 154 - loss 0.08728358 - samples / sec : 23.47 2020 - 07 - 02 20 : 03 : 13 , 522 epoch 85 - iter 120 / 154 - loss 0.08704443 - samples / sec : 23.89 2020 - 07 - 02 20 : 03 : 32 , 671 epoch 85 - iter 135 / 154 - loss 0.08533383 - samples / sec : 25.23 2020 - 07 - 02 20 : 03 : 53 , 771 epoch 85 - iter 150 / 154 - loss 0.08561399 - samples / sec : 23.03 2020 - 07 - 02 20 : 03 : 58 , 432 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 03 : 58 , 433 EPOCH 85 done : loss 0.0846 - lr 0.0001267 2020 - 07 - 02 20 : 04 : 20 , 935 DEV : loss 0.19647707045078278 - score 0.9798 2020 - 07 - 02 20 : 04 : 21 , 004 BAD EPOCHS ( no improvement ): 1 2020 - 07 - 02 20 : 04 : 21 , 006 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 04 : 44 , 697 epoch 86 - iter 15 / 154 - loss 0.08141229 - samples / sec : 20.51 2020 - 07 - 02 20 : 05 : 05 , 999 epoch 86 - iter 30 / 154 - loss 0.08228939 - samples / sec : 22.67 2020 - 07 - 02 20 : 05 : 26 , 309 epoch 86 - iter 45 / 154 - loss 0.07569061 - samples / sec : 23.76 2020 - 07 - 02 20 : 05 : 45 , 276 epoch 86 - iter 60 / 154 - loss 0.07859332 - samples / sec : 25.46 2020 - 07 - 02 20 : 06 : 05 , 770 epoch 86 - iter 75 / 154 - loss 0.08294603 - samples / sec : 23.69 2020 - 07 - 02 20 : 06 : 25 , 472 epoch 86 - iter 90 / 154 - loss 0.08264545 - samples / sec : 24.51 2020 - 07 - 02 20 : 06 : 45 , 127 epoch 86 - iter 105 / 154 - loss 0.08138939 - samples / sec : 24.56 2020 - 07 - 02 20 : 07 : 03 , 601 epoch 86 - iter 120 / 154 - loss 0.08116858 - samples / sec : 26.35 2020 - 07 - 02 20 : 07 : 22 , 317 epoch 86 - iter 135 / 154 - loss 0.08432639 - samples / sec : 25.80 2020 - 07 - 02 20 : 07 : 40 , 946 epoch 86 - iter 150 / 154 - loss 0.08361728 - samples / sec : 25.93 2020 - 07 - 02 20 : 07 : 46 , 002 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 07 : 46 , 003 EPOCH 86 done : loss 0.0821 - lr 0.0001267 2020 - 07 - 02 20 : 08 : 08 , 553 DEV : loss 0.19691519439220428 - score 0.9798 2020 - 07 - 02 20 : 08 : 08 , 622 BAD EPOCHS ( no improvement ): 2 2020 - 07 - 02 20 : 08 : 08 , 623 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 08 : 32 , 228 epoch 87 - iter 15 / 154 - loss 0.07756042 - samples / sec : 20.58 2020 - 07 - 02 20 : 08 : 51 , 084 epoch 87 - iter 30 / 154 - loss 0.08246949 - samples / sec : 25.92 2020 - 07 - 02 20 : 09 : 10 , 734 epoch 87 - iter 45 / 154 - loss 0.08848242 - samples / sec : 24.57 2020 - 07 - 02 20 : 09 : 30 , 149 epoch 87 - iter 60 / 154 - loss 0.09197148 - samples / sec : 24.87 2020 - 07 - 02 20 : 09 : 52 , 124 epoch 87 - iter 75 / 154 - loss 0.09220921 - samples / sec : 22.09 2020 - 07 - 02 20 : 10 : 12 , 434 epoch 87 - iter 90 / 154 - loss 0.09423689 - samples / sec : 23.77 2020 - 07 - 02 20 : 10 : 31 , 990 epoch 87 - iter 105 / 154 - loss 0.09056646 - samples / sec : 24.69 2020 - 07 - 02 20 : 10 : 51 , 277 epoch 87 - iter 120 / 154 - loss 0.08802644 - samples / sec : 25.23 2020 - 07 - 02 20 : 11 : 11 , 081 epoch 87 - iter 135 / 154 - loss 0.08470457 - samples / sec : 24.38 2020 - 07 - 02 20 : 11 : 30 , 365 epoch 87 - iter 150 / 154 - loss 0.08198533 - samples / sec : 25.05 2020 - 07 - 02 20 : 11 : 34 , 515 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 11 : 34 , 517 EPOCH 87 done : loss 0.0823 - lr 0.0001267 2020 - 07 - 02 20 : 11 : 57 , 259 DEV : loss 0.19768422842025757 - score 0.9798 2020 - 07 - 02 20 : 11 : 57 , 329 BAD EPOCHS ( no improvement ): 3 2020 - 07 - 02 20 : 11 : 57 , 330 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 12 : 20 , 421 epoch 88 - iter 15 / 154 - loss 0.10425307 - samples / sec : 21.03 2020 - 07 - 02 20 : 12 : 40 , 927 epoch 88 - iter 30 / 154 - loss 0.08032743 - samples / sec : 23.56 2020 - 07 - 02 20 : 13 : 01 , 191 epoch 88 - iter 45 / 154 - loss 0.08593136 - samples / sec : 24.04 2020 - 07 - 02 20 : 13 : 19 , 940 epoch 88 - iter 60 / 154 - loss 0.08089487 - samples / sec : 25.77 2020 - 07 - 02 20 : 13 : 39 , 682 epoch 88 - iter 75 / 154 - loss 0.07908025 - samples / sec : 24.45 2020 - 07 - 02 20 : 13 : 59 , 018 epoch 88 - iter 90 / 154 - loss 0.08750883 - samples / sec : 25.15 2020 - 07 - 02 20 : 14 : 20 , 234 epoch 88 - iter 105 / 154 - loss 0.08899166 - samples / sec : 22.75 2020 - 07 - 02 20 : 14 : 39 , 067 epoch 88 - iter 120 / 154 - loss 0.08754672 - samples / sec : 25.65 2020 - 07 - 02 20 : 14 : 59 , 203 epoch 88 - iter 135 / 154 - loss 0.08932739 - samples / sec : 24.17 2020 - 07 - 02 20 : 15 : 17 , 676 epoch 88 - iter 150 / 154 - loss 0.08721906 - samples / sec : 26.15 2020 - 07 - 02 20 : 15 : 22 , 756 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 15 : 22 , 757 EPOCH 88 done : loss 0.0892 - lr 0.0001267 2020 - 07 - 02 20 : 15 : 45 , 517 DEV : loss 0.1971229910850525 - score 0.9798 2020 - 07 - 02 20 : 15 : 45 , 584 BAD EPOCHS ( no improvement ): 4 2020 - 07 - 02 20 : 15 : 45 , 586 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 16 : 06 , 439 epoch 89 - iter 15 / 154 - loss 0.07559101 - samples / sec : 23.33 2020 - 07 - 02 20 : 16 : 27 , 847 epoch 89 - iter 30 / 154 - loss 0.07773159 - samples / sec : 22.54 2020 - 07 - 02 20 : 16 : 47 , 195 epoch 89 - iter 45 / 154 - loss 0.09153157 - samples / sec : 25.20 2020 - 07 - 02 20 : 17 : 09 , 002 epoch 89 - iter 60 / 154 - loss 0.08735939 - samples / sec : 22.14 2020 - 07 - 02 20 : 17 : 29 , 072 epoch 89 - iter 75 / 154 - loss 0.08408470 - samples / sec : 24.06 2020 - 07 - 02 20 : 17 : 49 , 062 epoch 89 - iter 90 / 154 - loss 0.08779174 - samples / sec : 24.17 2020 - 07 - 02 20 : 18 : 08 , 900 epoch 89 - iter 105 / 154 - loss 0.08387496 - samples / sec : 24.35 2020 - 07 - 02 20 : 18 : 28 , 500 epoch 89 - iter 120 / 154 - loss 0.08260459 - samples / sec : 24.82 2020 - 07 - 02 20 : 18 : 48 , 390 epoch 89 - iter 135 / 154 - loss 0.08210002 - samples / sec : 24.34 2020 - 07 - 02 20 : 19 : 08 , 537 epoch 89 - iter 150 / 154 - loss 0.08263975 - samples / sec : 23.97 2020 - 07 - 02 20 : 19 : 13 , 248 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 19 : 13 , 250 EPOCH 89 done : loss 0.0817 - lr 0.0001267 2020 - 07 - 02 20 : 19 : 35 , 671 DEV : loss 0.19783227145671844 - score 0.9792 2020 - 07 - 02 20 : 19 : 35 , 900 BAD EPOCHS ( no improvement ): 5 2020 - 07 - 02 20 : 19 : 35 , 902 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 19 : 58 , 748 epoch 90 - iter 15 / 154 - loss 0.08422335 - samples / sec : 21.28 2020 - 07 - 02 20 : 20 : 19 , 410 epoch 90 - iter 30 / 154 - loss 0.07745877 - samples / sec : 23.37 2020 - 07 - 02 20 : 20 : 39 , 526 epoch 90 - iter 45 / 154 - loss 0.07794650 - samples / sec : 24.20 2020 - 07 - 02 20 : 20 : 58 , 368 epoch 90 - iter 60 / 154 - loss 0.07353494 - samples / sec : 25.64 2020 - 07 - 02 20 : 21 : 18 , 494 epoch 90 - iter 75 / 154 - loss 0.07113823 - samples / sec : 23.98 2020 - 07 - 02 20 : 21 : 40 , 280 epoch 90 - iter 90 / 154 - loss 0.07130840 - samples / sec : 22.29 2020 - 07 - 02 20 : 21 : 59 , 776 epoch 90 - iter 105 / 154 - loss 0.07188892 - samples / sec : 24.76 2020 - 07 - 02 20 : 22 : 17 , 909 epoch 90 - iter 120 / 154 - loss 0.07002038 - samples / sec : 26.84 2020 - 07 - 02 20 : 22 : 38 , 265 epoch 90 - iter 135 / 154 - loss 0.07641873 - samples / sec : 23.71 2020 - 07 - 02 20 : 22 : 58 , 198 epoch 90 - iter 150 / 154 - loss 0.07757820 - samples / sec : 24.24 2020 - 07 - 02 20 : 23 : 02 , 778 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 23 : 02 , 779 EPOCH 90 done : loss 0.0785 - lr 0.0001267 2020 - 07 - 02 20 : 23 : 25 , 603 DEV : loss 0.19686871767044067 - score 0.9798 Epoch 90 : reducing learning rate of group 0 to 6.3352e-05 . 2020 - 07 - 02 20 : 23 : 25 , 672 BAD EPOCHS ( no improvement ): 6 2020 - 07 - 02 20 : 23 : 25 , 674 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 23 : 25 , 675 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 23 : 25 , 675 learning rate too small - quitting training ! 2020 - 07 - 02 20 : 23 : 25 , 676 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 23 : 26 , 199 ---------------------------------------------------------------------------------------------------- 2020 - 07 - 02 20 : 23 : 26 , 201 Testing using best model ... 2020 - 07 - 02 20 : 23 : 26 , 204 loading file Path / to / model / output / directory / best - model . pt 2020 - 07 - 02 20 : 23 : 40 , 895 0.974 0.974 0.974 2020 - 07 - 02 20 : 23 : 40 , 897 MICRO_AVG : acc 0.9913333333333333 - f1 - score 0.974 MACRO_AVG : acc 0.9913333333333334 - f1 - score 0.9776831958334483 ABBR tp : 9 - fp : 0 - fn : 0 - tn : 491 - precision : 1.0000 - recall : 1.0000 - accuracy : 1.0000 - f1 - score : 1.0000 DESC tp : 136 - fp : 5 - fn : 2 - tn : 357 - precision : 0.9645 - recall : 0.9855 - accuracy : 0.9860 - f1 - score : 0.9749 ENTY tp : 86 - fp : 3 - fn : 8 - tn : 403 - precision : 0.9663 - recall : 0.9149 - accuracy : 0.9780 - f1 - score : 0.9399 HUM tp : 63 - fp : 1 - fn : 2 - tn : 434 - precision : 0.9844 - recall : 0.9692 - accuracy : 0.9940 - f1 - score : 0.9767 LOC tp : 80 - fp : 1 - fn : 1 - tn : 418 - precision : 0.9877 - recall : 0.9877 - accuracy : 0.9960 - f1 - score : 0.9877 NUM tp : 113 - fp : 3 - fn : 0 - tn : 384 - precision : 0.9741 - recall : 1.0000 - accuracy : 0.9940 - f1 - score : 0.9869 2020 - 07 - 02 20 : 23 : 40 , 898 ---------------------------------------------------------------------------------------------------- The model was saved in the directory OUTPUT_DIR . We can load the sequence classifier into our EasySequenceClassifier instance and start running inference. from adaptnlp import EasySequenceClassifier # Set example text and instantiate tagger instance example_text = '''Where was the Queen's wedding held? ''' classifier = EasySequenceClassifier () sentences = classifier . tag_text ( example_text , model_name_or_path = OUTPUT_DIR ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels ) Output [ LOC ( 0.9990556836128235 )]","title":"Getting Started with SequenceClassifierTrainer"},{"location":"tutorial/translation.html","text":"Translation is the task of producing the input text in another language. Below, we'll walk through how we can use AdaptNLP's EasyTranslator module to translate text with state-of-the-art models. Getting Started with EasyTranslator \u00b6 We'll first get started by importing the EasyTranslator class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the translator. from adaptnlp import EasyTranslator text = [ \"Machine learning will take over the world very soon.\" , \"Machines can speak in many languages.\" ,] translator = EasyTranslator () Translating with translate(text: str, model_name_or_path: str, mini_batch_size: int, num_beams:int, min_length: int, max_length: int, early_stopping: bool **kwargs) \u00b6 Now that we have the translator instantiated, we are ready to load in a model and translate the text with the built-in translate() method. This method takes in parameters: text , model_name_or_path , and mini_batch_size as well as optional keyword arguments from the Transformers.PreTrainedModel.generate() method. Note You can set model_name_or_path to any of Transformers pretrained Translation Models with Language Model heads. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxWithLMHead model. The method returns a list of Strings. Here is an example using the T5-small model: # Translate translations = translator . translate ( text = text , t5_prefix = \"translate English to German\" , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Translations: \\n \" ) for t in translations : print ( t , \" \\n \" ) Output Das Maschinelle Lernen wird die Welt in K\u00fcrze \u00fcbernehmen. Maschinen k\u00f6nnen in vielen Sprachen sprechen. Below are some examples of Hugging Face's Pre-Trained Translation models that you can use (These do not include models hosted in Hugging Face's model repo): Model ID T5 't5-small' 't5-base' 't5-large' 't5-3B' 't5-11B'","title":"Translation"},{"location":"tutorial/translation.html#getting-started-with-easytranslator","text":"We'll first get started by importing the EasyTranslator class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the translator. from adaptnlp import EasyTranslator text = [ \"Machine learning will take over the world very soon.\" , \"Machines can speak in many languages.\" ,] translator = EasyTranslator ()","title":"Getting Started with EasyTranslator"},{"location":"tutorial/translation.html#translating-with-translatetext-str-model_name_or_path-str-mini_batch_size-int-num_beamsint-min_length-int-max_length-int-early_stopping-bool-kwargs","text":"Now that we have the translator instantiated, we are ready to load in a model and translate the text with the built-in translate() method. This method takes in parameters: text , model_name_or_path , and mini_batch_size as well as optional keyword arguments from the Transformers.PreTrainedModel.generate() method. Note You can set model_name_or_path to any of Transformers pretrained Translation Models with Language Model heads. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxWithLMHead model. The method returns a list of Strings. Here is an example using the T5-small model: # Translate translations = translator . translate ( text = text , t5_prefix = \"translate English to German\" , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Translations: \\n \" ) for t in translations : print ( t , \" \\n \" ) Output Das Maschinelle Lernen wird die Welt in K\u00fcrze \u00fcbernehmen. Maschinen k\u00f6nnen in vielen Sprachen sprechen. Below are some examples of Hugging Face's Pre-Trained Translation models that you can use (These do not include models hosted in Hugging Face's model repo): Model ID T5 't5-small' 't5-base' 't5-large' 't5-3B' 't5-11B'","title":"Translating with translate(text: str, model_name_or_path: str, mini_batch_size: int, num_beams:int, min_length: int, max_length: int, early_stopping: bool **kwargs)"}]}